{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IBU cluster documentation Introduction The IBU cluster is the main compute of cluster of the Interfaculty Bioinformatics Unit at the University of Bern . Working on the IBU cluster & new users We assume all users have a good knowledge of UNIX command line, basic knowledge on HPC computing and job submission using SLURM . If you are doubting whether you possess the right skills, you can: Follow the UNIX e-learning module at SIB . Follow our tutorials on SSH and SLURM","title":"Home"},{"location":"#ibu-cluster-documentation","text":"","title":"IBU cluster documentation"},{"location":"#introduction","text":"The IBU cluster is the main compute of cluster of the Interfaculty Bioinformatics Unit at the University of Bern .","title":"Introduction"},{"location":"#working-on-the-ibu-cluster-new-users","text":"We assume all users have a good knowledge of UNIX command line, basic knowledge on HPC computing and job submission using SLURM . If you are doubting whether you possess the right skills, you can: Follow the UNIX e-learning module at SIB . Follow our tutorials on SSH and SLURM","title":"Working on the IBU cluster &amp; new users"},{"location":"checklists/checklist_courses_cluster/","text":"If you are organizing a course that requires cluster resources, please contact it@bioinformatics.unibe.ch and provide the following information as soon as possible. We need to know at least two months in advance the dates of the course in order to reserve the resources and make sure there is no overlap with maintenance. We need to know about each course, even it is recurring every year. When is the course happening? (From-To) do the students need an IBU Account by their name or will you use students* accounts ? how many students* accounts are needed and by which date ? who are the teachers: provide an excel or tabulation-separated list with : IBU username if existing First Name Last Name Email address PI (username or email address) who are the students ? Provide an excel or tabulation-separated list with : IBU username if existing First Name Last Name Email address PI (username or email address) where should data be stored ? path on /data/courses or /home or /data/users or ? how much data in GB where who should have read-only access to which data who should have read-and-write access to which data How many CPUs do you need reserved for the course ? are 128 CPUS enough ? How much RAM needed ? Is 512 GB RAM enough for the reserved partition ? How long should the course data be retained ? how long should the students accounts remain valid ? do the students need to access the HPC resources (storage, slurm partition) after the end of the course and if yes until when ? please ask the students to test their access to HPC and to the storage folder before the course starts","title":"Prepare a course checklist"},{"location":"coding_best_practices/best_practices/","text":"In progress This document describes the best practices for code writing at IBU. The main objective is to write clean code that is readable. Global Rules Do not exceed line width of 80 characters. Add a line to your editor of choice. DRY (Don\u2019t repeat yourself). If you write the same code more than two times, consider writing a function. Wrtie modular code which is organised in different files. Explicit code is better than implicit code. R We refer to the style guide from tidyverse for a detailed explanation of accepted styles. Some of the most important style recommendations are listed here: Objects Naming objects As a general rule, we use _ for separating words in object names. Object names should be nouns. Good: mean_sepal_length <- ... Bad: mean.sepal.lengths <- ... The reason why a . can be a confusing delimiter for variable names is because it resembles the notation of S3 objects. Functions Naming functions Function names should be meaningful and give an understanding of what the functions does. Use verbs for functions Good: calculate_mean() permute() Bad: mean_calculator() permuter() Long functions If functions have many (longish) arguments, indent the second line where the definition starts: calculate_mean <- function(x = \"First long argument\", y = \"second long argument\", z = \"third long argument\"){ } The curly braces The closing curly braces belong to a new line. Explaining Functions If the functions are rather complicated (which is most often the case), write down what the function does, explain what arguments are needed and what the function returns. calculate_nt_identity <- function(sequence_1, sequence_2, method){ # Function aligns two sequences and calculates the nucleotide identity # score. # Args: # sequence_1: A DNA sequence of class DnaSeq. # sequence_2: A DNA sequence of class DnaSeq. # method: alignment method (\"method1\", \"method2\") # Returns: A number between 0 and 1. <code that does what the function claims> } Pipes The pipe operator %>% should be preceeded by a space followed by a new line. Good: iris %>% group_by(Species) %>% summarize_if(is.numeric, mean) %>% ungroup() %>% gather(measure, value, -Species) %>% arrange(value) Bad: iris %>% group_by(Species)%>% summarize_if(is.numeric, mean) %>% ungroup() %>%gather(measure, value, -Species) %>% arrange(value) Rmarkdown Rmarkdown is a great tool for presenting/documenting research while including code. However, Rmarkdown files can quickly become very long and hard-to-read if text is interrupted by large code chunks. Therefore, it is recommended to source code from external files which contain functions. Sourcing .R files ```{r, include=FALSE} source(\"path/to/script.R\", local= knitr::knit_global()) \\``` Sourcing chunks Python in progress","title":"Coding Best Practices"},{"location":"coding_best_practices/best_practices/#global-rules","text":"Do not exceed line width of 80 characters. Add a line to your editor of choice. DRY (Don\u2019t repeat yourself). If you write the same code more than two times, consider writing a function. Wrtie modular code which is organised in different files. Explicit code is better than implicit code.","title":"Global Rules"},{"location":"coding_best_practices/best_practices/#r","text":"We refer to the style guide from tidyverse for a detailed explanation of accepted styles. Some of the most important style recommendations are listed here:","title":"R"},{"location":"coding_best_practices/best_practices/#objects","text":"","title":"Objects"},{"location":"coding_best_practices/best_practices/#naming-objects","text":"As a general rule, we use _ for separating words in object names. Object names should be nouns. Good: mean_sepal_length <- ... Bad: mean.sepal.lengths <- ... The reason why a . can be a confusing delimiter for variable names is because it resembles the notation of S3 objects.","title":"Naming objects"},{"location":"coding_best_practices/best_practices/#functions","text":"","title":"Functions"},{"location":"coding_best_practices/best_practices/#naming-functions","text":"Function names should be meaningful and give an understanding of what the functions does. Use verbs for functions Good: calculate_mean() permute() Bad: mean_calculator() permuter()","title":"Naming functions"},{"location":"coding_best_practices/best_practices/#long-functions","text":"If functions have many (longish) arguments, indent the second line where the definition starts: calculate_mean <- function(x = \"First long argument\", y = \"second long argument\", z = \"third long argument\"){ }","title":"Long functions"},{"location":"coding_best_practices/best_practices/#the-curly-braces","text":"The closing curly braces belong to a new line.","title":"The curly braces"},{"location":"coding_best_practices/best_practices/#explaining-functions","text":"If the functions are rather complicated (which is most often the case), write down what the function does, explain what arguments are needed and what the function returns. calculate_nt_identity <- function(sequence_1, sequence_2, method){ # Function aligns two sequences and calculates the nucleotide identity # score. # Args: # sequence_1: A DNA sequence of class DnaSeq. # sequence_2: A DNA sequence of class DnaSeq. # method: alignment method (\"method1\", \"method2\") # Returns: A number between 0 and 1. <code that does what the function claims> }","title":"Explaining Functions"},{"location":"coding_best_practices/best_practices/#pipes","text":"The pipe operator %>% should be preceeded by a space followed by a new line. Good: iris %>% group_by(Species) %>% summarize_if(is.numeric, mean) %>% ungroup() %>% gather(measure, value, -Species) %>% arrange(value) Bad: iris %>% group_by(Species)%>% summarize_if(is.numeric, mean) %>% ungroup() %>%gather(measure, value, -Species) %>% arrange(value)","title":"Pipes"},{"location":"coding_best_practices/best_practices/#rmarkdown","text":"Rmarkdown is a great tool for presenting/documenting research while including code. However, Rmarkdown files can quickly become very long and hard-to-read if text is interrupted by large code chunks. Therefore, it is recommended to source code from external files which contain functions.","title":"Rmarkdown"},{"location":"coding_best_practices/best_practices/#sourcing-r-files","text":"```{r, include=FALSE} source(\"path/to/script.R\", local= knitr::knit_global()) \\```","title":"Sourcing .R files"},{"location":"coding_best_practices/best_practices/#sourcing-chunks","text":"","title":"Sourcing chunks"},{"location":"coding_best_practices/best_practices/#python","text":"in progress","title":"Python"},{"location":"documentation/apptainer/","text":"Apptainer is container software that can be used to run computations inside a container. In order to get introduced to containers and apptainer have a look at this container tutorial . For the impatient Apptainer is only available on compute nodes (not on binfservms01 ) By default only ~ is mounted. Use --bind to bind other directories (like /data ) Set the variable APPTAINER_TMPDIR to a directory with enough disk, e.g. export APPTAINER_TMPDIR=\"$SCRATCH\" Running apptainer on the IBU cluster On the IBU cluster, apptainer is only installed on the compute nodes (not on the head node). In order to test whether apptainer works for you, start an interactive session with SLURM: srun --cpus-per-task = 2 --mem = 8000 --time = 01 :00:00 --pty bash With this interactive session, you will have shell available on a compute node, so we can use apptainer : apptainer --help Pulling images For pulling (large) images from e.g. dockerhub it is important to set the variable $APPTAINER_TMPDIR . Otherwise it will write large files to /tmp , and this disk will quickly fill up. Your container build will run faster if it is set to the scratch directory associated with your (interactive) job: export APPTAINER_TMPDIR = \" $SCRATCH \" Do this from within a job The variable $SCRATCH only exists inside an (interactive) job. Check it with: echo \" $SCRATCH \" This should return something like: /scratch/8946748 If it doesn\u2019t return anything, you\u2019re not in an interactive job. In order to pull an image from dockerhub you can run the following: apptainer pull docker://ubuntu:latest This will result in a local image called ubuntu_latest.sif . Add $APPTAINER_TMPDIR to your .bashrc Every time you run an interactive job, you will have to set $APPTAINER_TMPDIR to $SCRATCH . However, you might forget that at some point. Therefore, it\u2019s best to take the precaution to set the variable to a writable directory on /data in your .bashrc . In this way, you will never write to /tmp by accident: ~/.bashrc # apptainer variables export APPTAINER_TMPDIR = /data/users/yourusername/ It will always make sense to run export APPTAINER_TMPDIR=\"$SCRATCH\" in an interactive job, as the container building will likely be faster. Mounting directories By default, only your home directory will be mounted to a container. Luckily, you can mount other directories (e.g. /data ) with the option --bind : apptainer exec --bind \"/data\" ubuntu_latest.sif ls /data If you find yourself often writing --bind /data while running apptainer containers, you can use the variable APPTAINER_BIND : export APPTAINER_BIND=/data Add the $APPTAINER_BIND to your .bashrc For example: ~/.bashrc # apptainer variables export APPTAINER_TMPDIR = /data/users/yourusername/ export APPTAINER_BIND = /data When using snakemake If you are using apptainer containers with the pipeline software snakemake you will have to specify this extra argument as well. Do this with the option --apptainer-args , e.g.: snakemake \\ --use-apptainer \\ --apptainer-args \"--bind /data\" \\ --cores 2","title":"Apptainer"},{"location":"documentation/apptainer/#for-the-impatient","text":"Apptainer is only available on compute nodes (not on binfservms01 ) By default only ~ is mounted. Use --bind to bind other directories (like /data ) Set the variable APPTAINER_TMPDIR to a directory with enough disk, e.g. export APPTAINER_TMPDIR=\"$SCRATCH\"","title":"For the impatient"},{"location":"documentation/apptainer/#running-apptainer-on-the-ibu-cluster","text":"On the IBU cluster, apptainer is only installed on the compute nodes (not on the head node). In order to test whether apptainer works for you, start an interactive session with SLURM: srun --cpus-per-task = 2 --mem = 8000 --time = 01 :00:00 --pty bash With this interactive session, you will have shell available on a compute node, so we can use apptainer : apptainer --help","title":"Running apptainer on the IBU cluster"},{"location":"documentation/apptainer/#pulling-images","text":"For pulling (large) images from e.g. dockerhub it is important to set the variable $APPTAINER_TMPDIR . Otherwise it will write large files to /tmp , and this disk will quickly fill up. Your container build will run faster if it is set to the scratch directory associated with your (interactive) job: export APPTAINER_TMPDIR = \" $SCRATCH \" Do this from within a job The variable $SCRATCH only exists inside an (interactive) job. Check it with: echo \" $SCRATCH \" This should return something like: /scratch/8946748 If it doesn\u2019t return anything, you\u2019re not in an interactive job. In order to pull an image from dockerhub you can run the following: apptainer pull docker://ubuntu:latest This will result in a local image called ubuntu_latest.sif . Add $APPTAINER_TMPDIR to your .bashrc Every time you run an interactive job, you will have to set $APPTAINER_TMPDIR to $SCRATCH . However, you might forget that at some point. Therefore, it\u2019s best to take the precaution to set the variable to a writable directory on /data in your .bashrc . In this way, you will never write to /tmp by accident: ~/.bashrc # apptainer variables export APPTAINER_TMPDIR = /data/users/yourusername/ It will always make sense to run export APPTAINER_TMPDIR=\"$SCRATCH\" in an interactive job, as the container building will likely be faster.","title":"Pulling images"},{"location":"documentation/apptainer/#mounting-directories","text":"By default, only your home directory will be mounted to a container. Luckily, you can mount other directories (e.g. /data ) with the option --bind : apptainer exec --bind \"/data\" ubuntu_latest.sif ls /data If you find yourself often writing --bind /data while running apptainer containers, you can use the variable APPTAINER_BIND : export APPTAINER_BIND=/data Add the $APPTAINER_BIND to your .bashrc For example: ~/.bashrc # apptainer variables export APPTAINER_TMPDIR = /data/users/yourusername/ export APPTAINER_BIND = /data When using snakemake If you are using apptainer containers with the pipeline software snakemake you will have to specify this extra argument as well. Do this with the option --apptainer-args , e.g.: snakemake \\ --use-apptainer \\ --apptainer-args \"--bind /data\" \\ --cores 2","title":"Mounting directories"},{"location":"documentation/storage/","text":"User storage Each user has storage capacity at /home/<user>/ and at /data/users/<user> . Storage at /home/ is limited to 20 GB per user, while /data/users/ has a limit of 4 TB per user. If you don\u2019t work with projects (at /data/projects ), it is therefore recommended to use /data/users for storing large files, while using /home/ only if necessary. You can get your diskquota with: lsquota This will give you: Disk quotas for user berthier (uid 622): Filesystem Used S-Quota H-Quota Grace #Files Quota Limit Grace /home 4G 20G 21G none 9k 0k 0k none /data/users 102G* 100G 105G 7d 3k 0k 0k none Where S-Quota indicates soft quota, and H-Quota indicates hard quota. If you exceed the soft quota, (indicated by *), you have 7 days to clean up. Otherwise, you will not be able to write to these folders. Manual backup solution Everything will be backupped that is in a folder called /data/projects/pXXX*/BACKUP . This folder must contain files or hard links to files (no soft links). You can use the /data/scripts/hardLinkScripts.sh script to automatically parse your projects/user folder and hard link files from specific folders and/or files with specific extensions to the BACKUP folder. Make a copy of the script and edit the first couple of lines to indicate the projects, folders or files with specific extensions that you want to include in your backup. You will have to remember to add new projects to the script as you open them. To execute the script once per day: open crontab: crontab -e and add the following line 55 23 * * * <pathToScript> . The script will now be executed every day at 23:55 To execute the script every time you login to the cluster: If you execute the script within your .bashrc (within your home directory), the script is automatically executed everytime you login to the cluster Data storage options The IBU cluster cannot be used for long term archiving of data . To archive a project, we recommend you delete all files that are no longer needed, then create a tar ball using the command below. Copy the tar ball to locations of your choice and delete the project from the cluster. tar -zcvf tar-archive-name.tar.gz source-folder-name Some useful resources on data management and storage: for DBMR members UniBe info on research data management European Nucleotide Archive: We have a tutorial on how to upload data here .","title":"Storage"},{"location":"documentation/storage/#user-storage","text":"Each user has storage capacity at /home/<user>/ and at /data/users/<user> . Storage at /home/ is limited to 20 GB per user, while /data/users/ has a limit of 4 TB per user. If you don\u2019t work with projects (at /data/projects ), it is therefore recommended to use /data/users for storing large files, while using /home/ only if necessary. You can get your diskquota with: lsquota This will give you: Disk quotas for user berthier (uid 622): Filesystem Used S-Quota H-Quota Grace #Files Quota Limit Grace /home 4G 20G 21G none 9k 0k 0k none /data/users 102G* 100G 105G 7d 3k 0k 0k none Where S-Quota indicates soft quota, and H-Quota indicates hard quota. If you exceed the soft quota, (indicated by *), you have 7 days to clean up. Otherwise, you will not be able to write to these folders.","title":"User storage"},{"location":"documentation/storage/#manual-backup-solution","text":"Everything will be backupped that is in a folder called /data/projects/pXXX*/BACKUP . This folder must contain files or hard links to files (no soft links). You can use the /data/scripts/hardLinkScripts.sh script to automatically parse your projects/user folder and hard link files from specific folders and/or files with specific extensions to the BACKUP folder. Make a copy of the script and edit the first couple of lines to indicate the projects, folders or files with specific extensions that you want to include in your backup. You will have to remember to add new projects to the script as you open them. To execute the script once per day: open crontab: crontab -e and add the following line 55 23 * * * <pathToScript> . The script will now be executed every day at 23:55 To execute the script every time you login to the cluster: If you execute the script within your .bashrc (within your home directory), the script is automatically executed everytime you login to the cluster","title":"Manual backup solution"},{"location":"documentation/storage/#data-storage-options","text":"The IBU cluster cannot be used for long term archiving of data . To archive a project, we recommend you delete all files that are no longer needed, then create a tar ball using the command below. Copy the tar ball to locations of your choice and delete the project from the cluster. tar -zcvf tar-archive-name.tar.gz source-folder-name Some useful resources on data management and storage: for DBMR members UniBe info on research data management European Nucleotide Archive: We have a tutorial on how to upload data here .","title":"Data storage options"},{"location":"documentation/SLURM_usage/allocation_options/","text":"Allocation options Most options can be used for both sbatch and srun . Full documentation on sbatch can be found here , and for srun here . Option Description Example Default Value --mail-user User e-mail address. Used for e-mails on e.g. job failure --mail-user=user@students.unibe.ch --mail-type When to notify a job owner: none, all, begin, end, fail, requeue, array_tasks --mail-type=end,fail --job-name Specify a job name --job-name=\"Simple Matlab \u201c --time Expected runtime of the job. Format: dd-hh:mm:ss --time=12:00:00 --time=2-06:00:00 Partition-specific, see scontrol show partition <partname> --mem-per-cpu Memory required per allocated CPU in megabytes. Different units can be specified using the suffix [K|M|G] --mem-per-cpu=2G 2048 MB --tmp Specify the amount of disk space that must be available on the compute node(s). The local scratch space for the job is referenced by the variable SCRATCH . Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. --tmp=8G --tmp=2048 --ntasks Number of tasks (processes). Used for MPI jobs and job steps that may run distributed on multiple compute nodes --ntasks=4 1 or to match --nodes , --tasks-per-node if specified --nodes Request a certain number of nodes --nodes=2 1 or to match --ntasks , --tasks-per-node if specified --ntasks-per-node Specifies how many tasks will run on each allocated node. Meant to be used with --nodes . If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. --ntasks-per-node=2 --cpus-per-task Number of CPUs per taks (threads). Used for shared memory jobs that run locally on a single compute node --cpus-per-task=4 1 --array Submit an array job. Use % to specify the max number of tasks allowed to run concurrently. Only works in combination with sbatch --array=1,4,16-32:4 - -array=1-100%20 --output Redirect standard output. All directories specified in the path must exist before the job starts! By default stderr and stdout are connected to the same file slurm-%j.out , where %j is replaced with the job allocation number. --error Redirect standard error. All directories specified in the path must exist before the job starts! By default stderr and stdout are connected to the same file slurm-%j.out , where %j is replaced with the job allocation number. --partition The partition to use. --partition=pall --partition=pshort --partition=phighmem Default partition: pall --immediate Only submit the job if all requested resources are immediately available --exclusive Use the compute node(s) (with sbatch ) or CPU(s) (with srun ) exclusively. CAUTION: use in combination with sbatch only if you know what your are doing --test-only Validate the batch script and return the estimated start time considering the current cluster state --exclude Explicitly exclude certain nodes from the resources granted to the job --exclude=binfservas01 --nodelist Request a specific list of nodes. The job will contain all of these hosts and possibly additional hosts as needed to satisfy resource requirements. --nodelist=binfservas01","title":"Allocation options"},{"location":"documentation/SLURM_usage/allocation_options/#allocation-options","text":"Most options can be used for both sbatch and srun . Full documentation on sbatch can be found here , and for srun here . Option Description Example Default Value --mail-user User e-mail address. Used for e-mails on e.g. job failure --mail-user=user@students.unibe.ch --mail-type When to notify a job owner: none, all, begin, end, fail, requeue, array_tasks --mail-type=end,fail --job-name Specify a job name --job-name=\"Simple Matlab \u201c --time Expected runtime of the job. Format: dd-hh:mm:ss --time=12:00:00 --time=2-06:00:00 Partition-specific, see scontrol show partition <partname> --mem-per-cpu Memory required per allocated CPU in megabytes. Different units can be specified using the suffix [K|M|G] --mem-per-cpu=2G 2048 MB --tmp Specify the amount of disk space that must be available on the compute node(s). The local scratch space for the job is referenced by the variable SCRATCH . Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. --tmp=8G --tmp=2048 --ntasks Number of tasks (processes). Used for MPI jobs and job steps that may run distributed on multiple compute nodes --ntasks=4 1 or to match --nodes , --tasks-per-node if specified --nodes Request a certain number of nodes --nodes=2 1 or to match --ntasks , --tasks-per-node if specified --ntasks-per-node Specifies how many tasks will run on each allocated node. Meant to be used with --nodes . If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. --ntasks-per-node=2 --cpus-per-task Number of CPUs per taks (threads). Used for shared memory jobs that run locally on a single compute node --cpus-per-task=4 1 --array Submit an array job. Use % to specify the max number of tasks allowed to run concurrently. Only works in combination with sbatch --array=1,4,16-32:4 - -array=1-100%20 --output Redirect standard output. All directories specified in the path must exist before the job starts! By default stderr and stdout are connected to the same file slurm-%j.out , where %j is replaced with the job allocation number. --error Redirect standard error. All directories specified in the path must exist before the job starts! By default stderr and stdout are connected to the same file slurm-%j.out , where %j is replaced with the job allocation number. --partition The partition to use. --partition=pall --partition=pshort --partition=phighmem Default partition: pall --immediate Only submit the job if all requested resources are immediately available --exclusive Use the compute node(s) (with sbatch ) or CPU(s) (with srun ) exclusively. CAUTION: use in combination with sbatch only if you know what your are doing --test-only Validate the batch script and return the estimated start time considering the current cluster state --exclude Explicitly exclude certain nodes from the resources granted to the job --exclude=binfservas01 --nodelist Request a specific list of nodes. The job will contain all of these hosts and possibly additional hosts as needed to satisfy resource requirements. --nodelist=binfservas01","title":"Allocation options"},{"location":"documentation/SLURM_usage/partitions/","text":"The IBU has different partitions to which you can submit, the default is pall . Each with their own limits and assigned nodes. To see to which partitions you can submit run: lsaccounts.sh For an overview of all partitions, their time limits, assigned nodes and max memory: sinfo --format = \"%.P %.10l %.6D %.N %.m\" Returning: PARTITION TIMELIMIT NODES NODELIST MEMORY pall* 28 -00:00:0 30 binfservas [ 07 -17,19-37 ] 250000 + ptest 7 -00:00:00 1 binfservas99 1800 pshort 3 :00:00 3 binfservas [ 03 ,06,18 ] 112385 + phighmem 110 -00:00: 1 binfservas03 2000000 pcmpg infinite 7 cmpgnode [ 01 -07 ] 32768 +","title":"Partitions"},{"location":"documentation/SLURM_usage/slurm_general/","text":"Introduction A cluster has many resources and many users. Often the demand for resources is higher than supply. A job scheduler can schedule the jobs submitted by the different users, based on priority, required resources and time. The IBU cluster uses SLURM as job scheduler. After submitting a job to the cluster, SLURM will try to fulfill the job\u2019s resource request by allocating resources to the job. Some useful links (outside this documentation): SLURM quickstart","title":"General"},{"location":"documentation/SLURM_usage/slurm_general/#introduction","text":"A cluster has many resources and many users. Often the demand for resources is higher than supply. A job scheduler can schedule the jobs submitted by the different users, based on priority, required resources and time. The IBU cluster uses SLURM as job scheduler. After submitting a job to the cluster, SLURM will try to fulfill the job\u2019s resource request by allocating resources to the job. Some useful links (outside this documentation): SLURM quickstart","title":"Introduction"},{"location":"documentation/SLURM_usage/slurm_variables/","text":"All variables can be found here . Below, the most frequently used variables: Variable Meaning SLURM_ARRAY_TASK_COUNT Total number of tasks in a job array. SLURM_ARRAY_TASK_ID Job array ID (index) number. SLURM_ARRAY_TASK_MAX Job array\u2019s maximum ID (index) number. SLURM_ARRAY_TASK_MIN Job array\u2019s minimum ID (index) number. SLURM_ARRAY_TASK_STEP Job array\u2019s index step size. SLURM_ARRAY_JOB_ID Job array\u2019s master job ID number. SLURM_CLUSTER_NAME Name of the cluster on which the job is executing. SLURM_CPUS_ON_NODE Number of CPUS on the allocated node. SLURM_CPUS_PER_TASK Number of cpus requested per task. Only set if the \u2013cpus-per-task option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation. SLURM_JOB_ID The ID of the job allocation. SLURM_JOB_DEPENDENCY Set to value of the \u2013dependency option. SLURM_JOB_NAME Name of the job. SLURM_JOB_NODELIST List of nodes allocated to the job. SLURM_JOB_NUM_NODES Total number of nodes in the job\u2019s resource allocation. SLURM_JOB_PARTITION Name of the partition in which the job is running. SLURM_MEM_PER_CPU Same as \u2013mem-per-cpu SLURM_NODEID ID of the nodes allocated. SLURM_NTASKS Same as -n, \u2013ntasks SLURM_SUBMIT_DIR The directory from which sbatch was invoked or, if applicable, the directory specified by the -D, \u2013chdir option. SLURM_SUBMIT_HOST The hostname of the computer from which sbatch was invoked. SLURM_TASKS_PER_NODE Number of tasks to be initiated on each node. SLURM_TASK_PID The process ID of the task being started. SLURMD_NODENAME Name of the node running the job script.","title":"Special variables"},{"location":"documentation/SLURM_usage/status_monitoring/","text":"General status of nodes For a detailed overview of the state of the cluster use: clusterstate.sh This returns the full list of nodes: NODELIST CPUS(A/I/O/T) CPU_LOAD FREEMEMORY STATE binfservas03 24/56/0/80 7.83 1969280 mixed binfservas06 0/16/0/16 0.10 250000 idle binfservas07 16/0/0/16 4.22 11984 allocated binfservas08 11/5/0/16 10.98 81552 mixed binfservas09 7/9/0/16 4.26 234144 mixed binfservas10 4/12/0/16 5.56 218000 mixed binfservas11 24/0/0/24 16.48 108630 allocated ... the meaning of A/I/O/T A: allocated I: idle O: other T: total If you are interested in the specifications of a certain node, e.g. one of the nodes in pall , you can use: scontrol show node binfservas07 Which will result in: NodeName=binfservas07 Arch=x86_64 CoresPerSocket=8 CPUAlloc=16 CPUTot=16 CPULoad=16.15 AvailableFeatures=(null) ActiveFeatures=(null) Gres=(null) NodeAddr=binfservas07 NodeHostName=binfservas07 Version=20.11.8 OS=Linux 3.10.0-1160.45.1.el7.x86_64 #1 SMP Wed Oct 13 17:20:51 UTC 2021 RealMemory=250000 AllocMem=120424 FreeMem=103877 Sockets=2 Boards=1 State=ALLOCATED ThreadsPerCore=1 TmpDisk=6900000 Weight=1 Owner=N/A MCS_label=N/A Partitions=pall BootTime=2021-10-25T15:48:07 SlurmdStartTime=2021-10-25T17:44:25 CfgTRES=cpu=16,mem=250000M,billing=16 AllocTRES=cpu=16,mem=120424M CapWatts=n/a CurrentWatts=0 AveWatts=0 ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s Comment=(null) This gives information on e.g. number of CPUs ( CPUTot=16 ) and memory ( RealMemory=250000 ), so 16 CPUs with 250 Gb of memory. From this overview you can also see how much memory and CPU are allocated on this node: AllocMem=120424 shows that 120.4 Gb is allocated and CPUAlloc=16 show that all 16 CPUs are allocated. Status of a job To get a simple overview of a job status, you can find it out based on the job ID: squeue -j 7377455 Returning: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 7377455 pall interact gvangees R 0:13 1 binfservas10 Which gives you the partition, the status, time it has been running and the node. You can also get a list all jobs submitted by you (or another user) with: squeue -A gvangeest Returning: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 7377456 pall interact gvangees R 0:06 1 binfservas10 7377455 pall interact gvangees R 2:00 1 binfservas10 To get more information on submitted jobs that have not yet completed (i.e. status is RUNNING/PENDING): scontrol show job 7377455 After job completion, you migth be interested in how much CPU, time and memory your job actually took. For this, you could use: seff 7317012 Returning: Job ID: 7317012 Array Job ID: 7317010_6 Cluster: ibu_main User/Group: gvangeest/gvangeest State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 16 CPU Utilized: 16:08:54 CPU Efficiency: 4.32% of 15-13:31:28 core-walltime Job Wall-clock time: 23:20:43 Memory Utilized: 32.29 GB Memory Efficiency: 50.45% of 64.00 GB Which gives you a quick overview on how efficient your job was on CPU and memory usage. In order to get some more specific characteristics of your job, you can use sacct , e.g.: sacct -j 7317012 --format = \"NodeList,AllocCPUS,TotalCPU,ReqMem,MaxRSS,Timelimit,Elapsed\" Returning: NodeList AllocCPUS TotalCPU ReqMem MaxRSS Timelimit Elapsed --------------- ---------- ---------- ---------- ---------- ---------- ---------- binfservas16 16 16:08:53 4Gc 3-00:00:00 23:20:43 binfservas16 16 16:08:53 4Gc 33855268K 23:20:43 Here, the first line represents the allocation, whereas the second line represents the actual job step. If you want to see other parameters that sacct can provide, run sacct -e . You can conveniently pipe this to grep . E.g. to see all time related parameters, you can type: sacct -e | grep Time Returning: Constraints ConsumedEnergy ConsumedEnergyRaw CPUTime CPUTimeRAW DBIndex DerivedExitCode Elapsed SystemCPU SystemComment Timelimit TimelimitRaw For your convenience, you can summarize these statistics based on a pattern in the job name. Here\u2019s an example on CPU time (here we\u2019re looking for pattern p121 ): # To see the total CPU time spent on a particular project (in seconds): # CPUTimeRaw = Elapsed time * CPU sacct \\ --starttime 2018 -01-01 \\ -o CPUTimeRaw,JobName \\ | grep \"p121\" \\ | awk '{ sum += $1 } END { print sum }'","title":"Status monitoring"},{"location":"documentation/SLURM_usage/status_monitoring/#general-status-of-nodes","text":"For a detailed overview of the state of the cluster use: clusterstate.sh This returns the full list of nodes: NODELIST CPUS(A/I/O/T) CPU_LOAD FREEMEMORY STATE binfservas03 24/56/0/80 7.83 1969280 mixed binfservas06 0/16/0/16 0.10 250000 idle binfservas07 16/0/0/16 4.22 11984 allocated binfservas08 11/5/0/16 10.98 81552 mixed binfservas09 7/9/0/16 4.26 234144 mixed binfservas10 4/12/0/16 5.56 218000 mixed binfservas11 24/0/0/24 16.48 108630 allocated ... the meaning of A/I/O/T A: allocated I: idle O: other T: total If you are interested in the specifications of a certain node, e.g. one of the nodes in pall , you can use: scontrol show node binfservas07 Which will result in: NodeName=binfservas07 Arch=x86_64 CoresPerSocket=8 CPUAlloc=16 CPUTot=16 CPULoad=16.15 AvailableFeatures=(null) ActiveFeatures=(null) Gres=(null) NodeAddr=binfservas07 NodeHostName=binfservas07 Version=20.11.8 OS=Linux 3.10.0-1160.45.1.el7.x86_64 #1 SMP Wed Oct 13 17:20:51 UTC 2021 RealMemory=250000 AllocMem=120424 FreeMem=103877 Sockets=2 Boards=1 State=ALLOCATED ThreadsPerCore=1 TmpDisk=6900000 Weight=1 Owner=N/A MCS_label=N/A Partitions=pall BootTime=2021-10-25T15:48:07 SlurmdStartTime=2021-10-25T17:44:25 CfgTRES=cpu=16,mem=250000M,billing=16 AllocTRES=cpu=16,mem=120424M CapWatts=n/a CurrentWatts=0 AveWatts=0 ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s Comment=(null) This gives information on e.g. number of CPUs ( CPUTot=16 ) and memory ( RealMemory=250000 ), so 16 CPUs with 250 Gb of memory. From this overview you can also see how much memory and CPU are allocated on this node: AllocMem=120424 shows that 120.4 Gb is allocated and CPUAlloc=16 show that all 16 CPUs are allocated.","title":"General status of nodes"},{"location":"documentation/SLURM_usage/status_monitoring/#status-of-a-job","text":"To get a simple overview of a job status, you can find it out based on the job ID: squeue -j 7377455 Returning: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 7377455 pall interact gvangees R 0:13 1 binfservas10 Which gives you the partition, the status, time it has been running and the node. You can also get a list all jobs submitted by you (or another user) with: squeue -A gvangeest Returning: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 7377456 pall interact gvangees R 0:06 1 binfservas10 7377455 pall interact gvangees R 2:00 1 binfservas10 To get more information on submitted jobs that have not yet completed (i.e. status is RUNNING/PENDING): scontrol show job 7377455 After job completion, you migth be interested in how much CPU, time and memory your job actually took. For this, you could use: seff 7317012 Returning: Job ID: 7317012 Array Job ID: 7317010_6 Cluster: ibu_main User/Group: gvangeest/gvangeest State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 16 CPU Utilized: 16:08:54 CPU Efficiency: 4.32% of 15-13:31:28 core-walltime Job Wall-clock time: 23:20:43 Memory Utilized: 32.29 GB Memory Efficiency: 50.45% of 64.00 GB Which gives you a quick overview on how efficient your job was on CPU and memory usage. In order to get some more specific characteristics of your job, you can use sacct , e.g.: sacct -j 7317012 --format = \"NodeList,AllocCPUS,TotalCPU,ReqMem,MaxRSS,Timelimit,Elapsed\" Returning: NodeList AllocCPUS TotalCPU ReqMem MaxRSS Timelimit Elapsed --------------- ---------- ---------- ---------- ---------- ---------- ---------- binfservas16 16 16:08:53 4Gc 3-00:00:00 23:20:43 binfservas16 16 16:08:53 4Gc 33855268K 23:20:43 Here, the first line represents the allocation, whereas the second line represents the actual job step. If you want to see other parameters that sacct can provide, run sacct -e . You can conveniently pipe this to grep . E.g. to see all time related parameters, you can type: sacct -e | grep Time Returning: Constraints ConsumedEnergy ConsumedEnergyRaw CPUTime CPUTimeRAW DBIndex DerivedExitCode Elapsed SystemCPU SystemComment Timelimit TimelimitRaw For your convenience, you can summarize these statistics based on a pattern in the job name. Here\u2019s an example on CPU time (here we\u2019re looking for pattern p121 ): # To see the total CPU time spent on a particular project (in seconds): # CPUTimeRaw = Elapsed time * CPU sacct \\ --starttime 2018 -01-01 \\ -o CPUTimeRaw,JobName \\ | grep \"p121\" \\ | awk '{ sum += $1 } END { print sum }'","title":"Status of a job"},{"location":"documentation/SLURM_usage/submit_jobs/","text":"Looking for the SLURM tutorial? Find the SLURM tutorial here Resource Allocation SLURM can allocate the resources for a job (e.g. nodes, cores and memory). Allocations are created by most users with the sbatch and srun commands. Most users will use sbatch with or without srun . sbatch The sbatch command is used to submit a job script for later execution. SLURM options can be part of the submission command, but they are usually in the job script and prefixed by #SBATCH . Basic usage The first part of a job script contains the options for SLURM prefixed with #SBATCH . These are used to manage the resources (e.g. memory) and configure the job environment. All the options for job allocation can be found at Allocation options . The SLURM options are followed by the actual job script. Here\u2019s an example, first_job.sh : #!/bin/bash #SBATCH --job-name=\"first_job\" #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=4G ./calc_mat.sh Submit the job script: [ username@binfservms01 ~ ] $ sbatch first_job.sh Submitted batch job 34534 Jobs get a job ID that is printed to stdout after job submission. In this case it is 34534. We can use this number to track back our job, e.g. for status monitoring . Parallelization with arrays In bioinformatics, you will often need to run a similar command with a range of parameters, e.g. running an alignment of multiple files with reads. SLURM makes this easy with the --array option. The array option generates a special variable for SLURM , namely $SLURM_ARRAY_TASK_ID . This variable is replaced for each number given to the array. Basic usage: #!/bin/bash #SBATCH --job-name=test_emb_arr #SBATCH --output=res_emb_arr.txt #SBATCH --ntasks=1 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --array=1-8 my_program $SLURM_ARRAY_TASK_ID This will do a parameter sweep for my_program with the parameters 1 till 8. It generates separate jobs and allocations for each item in the array. Each job will inherit the options given to sbatch by the #SBATCH prefix. This concept can become very useful in the combination with UNIX arrays (yes, same name, different thing..). Let\u2019s say you have a directory with input files (e.g. sequence reads), and you want run a program on them (e.g. quality control). By generating a UNIX array of the file names and index them based on $SLURM_ARRAY_TASK_ID , you can run it in parallel with only a few lines of code: #!/bin/bash # #SBATCH --job-name=test_emb_arr #SBATCH --output=res_emb_arr.txt #SBATCH --ntasks=1 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --array=0-7 ## make a unix array of all files in a directory FILES =( /path/to/input_data/* ) ## run my_program on the ith item in the UNIX array my_program ${ FILES [ $SLURM_ARRAY_TASK_ID ] } UNIX uses zero indexing Like python and many other languages UNIX uses zero-based indexing, meaning the first item in a list is indicated by a 0, the second by 1 etc. So, in the example above the SLURM array should range from 0-7 if there are 8 files in your input directory. Or, if you want to \u2018loop\u2019 over lines in a file: #!/bin/bash # #SBATCH --job-name=test_emb_arr #SBATCH --output=res_emb_arr.txt #SBATCH --ntasks=1 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --array=1-7 ## Each line in the list contains a file name/path FOFN = \"list_of_file_names.txt\" LINE = ` sed \" ${ SLURM_ARRAY_TASK_ID } q;d\" \" $FOFN \" ` ## run my_program on the ith line in the file my_program \" $LINE \" srun The command srun submits a job for execution in real time, and takes most of the same options as sbatch described at Allocation options . You need srun if you want to make use of job steps or parallelization with a Message Passing Interface (MPI). Because it runs in real-time, the terminal will be blocked after calling srun untill the job is finished. Usually srun is used within a script submitted with sbatch to generate job steps and/or parallelization. In the case srun is used within a sbatch script, the sbatch options are inherited by srun . A detailed discussion on the difference between sbatch and srun can be found at stackoverflow . Generating job steps Within a script submitted with sbatch , srun creates job steps. The script below, jobsteps.sh , generates two job steps with srun : #!/bin/bash #SBATCH --time=00:10:00 #SBATCH --partition=pall #SBATCH --job-name=test_slurm srun hostname srun sleep 60 With default values of sacct (more on this at status monitoring ) you can get basic information on job steps. Here you can see which job steps have run, together with their allocated CPUs, status and exit code: [ username@binfservms01 ~ ] $ sbatch jobsteps.sh Submitted batch job 6200897 [ username@binfservms01 ~ ] $ sacct -j 6200897 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 6200897 test_slurm pall username 1 COMPLETED 0 :0 6200897 .bat+ batch username 1 COMPLETED 0 :0 6200897 .0 hostname username 1 COMPLETED 0 :0 6200897 .1 sleep username 1 COMPLETED 0 :0 Parallelization of job steps The command srun submits a job in real-time. Because the terminal is blocked, a following step in a job script will have to wait before the previous job step is finished. In the previous example, the sleep command could only start after the hostname command had finished. It is therefore sequential, just like a regular shell script. However, UNIX comes with functionality to run jobs in the background, which relieves terminal blocking. In this way, you don\u2019t have to wait before the previous command has finished. Running jobs in the background is simply done with ending the command with & . By putting the first job step in the background, the terminal is unblocked and the next job step can start. Here\u2019s an example: #!/bin/bash #SBATCH --time=00:10:00 #SBATCH --partition=pall #SBATCH --job-name=test_slurm srun sleep 60 & srun hostname & wait In this example, the hostname command will start at the same time as the sleep command, and they run in parallel. The resources they can use are defined by the options supplied to sbatch . The total usage of the parallel job steps will never exceed that. The wait command The command sbatch closes the job if the terminal is unblocked. But that\u2019s exactly what we\u2019re doing with & ! Therefore we use wait . This blocks the terminal until all background processes are finished. Otherwise sbatch will close the job before it is finished. As you remember, sbatch options are inherited by srun . Therefore without extra options to srun overriding it, a job step has access to every CPU allocated to the job created by sbatch . All job steps share therefore all job resources. When running job steps in parallel, this is probably not what you want. How multiple job steps divide the allocated resources can be arranged with the option --ntasks and --exclusive . Using --ntasks for sbatch defines how many tasks can be allocated at the same time for that job, for srun this means the number of tasks per job step. With the option --exclusive to srun you make sure each task can make full use of the allocated CPU and not share it with other job steps. Don\u2019t use --exclusive in combination with sbatch Using --exclusive with sbatch leads to exclusive usage of an entire node. This is almost never necessary, and leads to very high usage of the available resources. In the below example we generate a job with 4 tasks ( #SBATCH --ntasks=4 ). However, the code generates three job steps with different demands, that add up to 7 tasks. By defining the options --ntasks=n and --exclusive we make sure only four tasks can run at the same time with full usage of the required CPU. The remaining jobs will wait untill the other jobs are finised. #!/bin/bash #SBATCH --time=00:10:00 #SBATCH --ntasks=4 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=1 #SBATCH --partition=pall #SBATCH --job-name=test_slurm srun --ntasks = 1 --exclusive 1cpu_jobstep.sh & srun --ntasks = 2 --exclusive 2cpu_jobstep.sh & srun --ntasks = 4 --exclusive 4cpu_jobstep.sh & wait When not to use parallel job steps The above works if you have many relatively similar heterogeneous jobs steps. If you have for example one job that is resource intensive but short, and the other is resource limited but takes long, you can probably better submit them as separate jobs using a sbatch command for both of them. This is because a single job will keep the same resources allocated for the entire duration. So, the allocated resources will be the resources needed for the resource intensive job step, and will remain allocated during the long non-intensive job step. Interactive jobs Performing heavy calculations on the home node is not a good practice. However, for debugging, or small/simple steps, you might want to perform relatively heavy jobs interactively. With srun you can allocate a job and use bash to interactively call commands within the allocation. With the script below we allocate resources with 4 CPUs, 4G memory per CPU restricted to a time of 4 hours. With the option --pty we say that we want to perform the task in \u2018pseudo-terminal mode\u2019, and the terminal of our choice is bash. [ username@binfservms01 ~ ] $ srun --cpus-per-task = 4 --mem-per-cpu = 4000 --time = 04 :00:00 --pty bash srun: job 6200830 queued and waiting for resources srun: job 6200830 has been allocated resources [ username@binfservas32 ~ ] $ Different node Note that the new shell is not at the home node binfservms01 but at a compute node assigned by SLURM: binfservas32","title":"Submit jobs"},{"location":"documentation/SLURM_usage/submit_jobs/#resource-allocation","text":"SLURM can allocate the resources for a job (e.g. nodes, cores and memory). Allocations are created by most users with the sbatch and srun commands. Most users will use sbatch with or without srun .","title":"Resource Allocation"},{"location":"documentation/SLURM_usage/submit_jobs/#sbatch","text":"The sbatch command is used to submit a job script for later execution. SLURM options can be part of the submission command, but they are usually in the job script and prefixed by #SBATCH .","title":"sbatch"},{"location":"documentation/SLURM_usage/submit_jobs/#basic-usage","text":"The first part of a job script contains the options for SLURM prefixed with #SBATCH . These are used to manage the resources (e.g. memory) and configure the job environment. All the options for job allocation can be found at Allocation options . The SLURM options are followed by the actual job script. Here\u2019s an example, first_job.sh : #!/bin/bash #SBATCH --job-name=\"first_job\" #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=4G ./calc_mat.sh Submit the job script: [ username@binfservms01 ~ ] $ sbatch first_job.sh Submitted batch job 34534 Jobs get a job ID that is printed to stdout after job submission. In this case it is 34534. We can use this number to track back our job, e.g. for status monitoring .","title":"Basic usage"},{"location":"documentation/SLURM_usage/submit_jobs/#parallelization-with-arrays","text":"In bioinformatics, you will often need to run a similar command with a range of parameters, e.g. running an alignment of multiple files with reads. SLURM makes this easy with the --array option. The array option generates a special variable for SLURM , namely $SLURM_ARRAY_TASK_ID . This variable is replaced for each number given to the array. Basic usage: #!/bin/bash #SBATCH --job-name=test_emb_arr #SBATCH --output=res_emb_arr.txt #SBATCH --ntasks=1 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --array=1-8 my_program $SLURM_ARRAY_TASK_ID This will do a parameter sweep for my_program with the parameters 1 till 8. It generates separate jobs and allocations for each item in the array. Each job will inherit the options given to sbatch by the #SBATCH prefix. This concept can become very useful in the combination with UNIX arrays (yes, same name, different thing..). Let\u2019s say you have a directory with input files (e.g. sequence reads), and you want run a program on them (e.g. quality control). By generating a UNIX array of the file names and index them based on $SLURM_ARRAY_TASK_ID , you can run it in parallel with only a few lines of code: #!/bin/bash # #SBATCH --job-name=test_emb_arr #SBATCH --output=res_emb_arr.txt #SBATCH --ntasks=1 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --array=0-7 ## make a unix array of all files in a directory FILES =( /path/to/input_data/* ) ## run my_program on the ith item in the UNIX array my_program ${ FILES [ $SLURM_ARRAY_TASK_ID ] } UNIX uses zero indexing Like python and many other languages UNIX uses zero-based indexing, meaning the first item in a list is indicated by a 0, the second by 1 etc. So, in the example above the SLURM array should range from 0-7 if there are 8 files in your input directory. Or, if you want to \u2018loop\u2019 over lines in a file: #!/bin/bash # #SBATCH --job-name=test_emb_arr #SBATCH --output=res_emb_arr.txt #SBATCH --ntasks=1 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --array=1-7 ## Each line in the list contains a file name/path FOFN = \"list_of_file_names.txt\" LINE = ` sed \" ${ SLURM_ARRAY_TASK_ID } q;d\" \" $FOFN \" ` ## run my_program on the ith line in the file my_program \" $LINE \"","title":"Parallelization with arrays"},{"location":"documentation/SLURM_usage/submit_jobs/#srun","text":"The command srun submits a job for execution in real time, and takes most of the same options as sbatch described at Allocation options . You need srun if you want to make use of job steps or parallelization with a Message Passing Interface (MPI). Because it runs in real-time, the terminal will be blocked after calling srun untill the job is finished. Usually srun is used within a script submitted with sbatch to generate job steps and/or parallelization. In the case srun is used within a sbatch script, the sbatch options are inherited by srun . A detailed discussion on the difference between sbatch and srun can be found at stackoverflow .","title":"srun"},{"location":"documentation/SLURM_usage/submit_jobs/#generating-job-steps","text":"Within a script submitted with sbatch , srun creates job steps. The script below, jobsteps.sh , generates two job steps with srun : #!/bin/bash #SBATCH --time=00:10:00 #SBATCH --partition=pall #SBATCH --job-name=test_slurm srun hostname srun sleep 60 With default values of sacct (more on this at status monitoring ) you can get basic information on job steps. Here you can see which job steps have run, together with their allocated CPUs, status and exit code: [ username@binfservms01 ~ ] $ sbatch jobsteps.sh Submitted batch job 6200897 [ username@binfservms01 ~ ] $ sacct -j 6200897 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 6200897 test_slurm pall username 1 COMPLETED 0 :0 6200897 .bat+ batch username 1 COMPLETED 0 :0 6200897 .0 hostname username 1 COMPLETED 0 :0 6200897 .1 sleep username 1 COMPLETED 0 :0","title":"Generating job steps"},{"location":"documentation/SLURM_usage/submit_jobs/#parallelization-of-job-steps","text":"The command srun submits a job in real-time. Because the terminal is blocked, a following step in a job script will have to wait before the previous job step is finished. In the previous example, the sleep command could only start after the hostname command had finished. It is therefore sequential, just like a regular shell script. However, UNIX comes with functionality to run jobs in the background, which relieves terminal blocking. In this way, you don\u2019t have to wait before the previous command has finished. Running jobs in the background is simply done with ending the command with & . By putting the first job step in the background, the terminal is unblocked and the next job step can start. Here\u2019s an example: #!/bin/bash #SBATCH --time=00:10:00 #SBATCH --partition=pall #SBATCH --job-name=test_slurm srun sleep 60 & srun hostname & wait In this example, the hostname command will start at the same time as the sleep command, and they run in parallel. The resources they can use are defined by the options supplied to sbatch . The total usage of the parallel job steps will never exceed that. The wait command The command sbatch closes the job if the terminal is unblocked. But that\u2019s exactly what we\u2019re doing with & ! Therefore we use wait . This blocks the terminal until all background processes are finished. Otherwise sbatch will close the job before it is finished. As you remember, sbatch options are inherited by srun . Therefore without extra options to srun overriding it, a job step has access to every CPU allocated to the job created by sbatch . All job steps share therefore all job resources. When running job steps in parallel, this is probably not what you want. How multiple job steps divide the allocated resources can be arranged with the option --ntasks and --exclusive . Using --ntasks for sbatch defines how many tasks can be allocated at the same time for that job, for srun this means the number of tasks per job step. With the option --exclusive to srun you make sure each task can make full use of the allocated CPU and not share it with other job steps. Don\u2019t use --exclusive in combination with sbatch Using --exclusive with sbatch leads to exclusive usage of an entire node. This is almost never necessary, and leads to very high usage of the available resources. In the below example we generate a job with 4 tasks ( #SBATCH --ntasks=4 ). However, the code generates three job steps with different demands, that add up to 7 tasks. By defining the options --ntasks=n and --exclusive we make sure only four tasks can run at the same time with full usage of the required CPU. The remaining jobs will wait untill the other jobs are finised. #!/bin/bash #SBATCH --time=00:10:00 #SBATCH --ntasks=4 #SBATCH --mem-per-cpu=4G #SBATCH --cpus-per-task=1 #SBATCH --partition=pall #SBATCH --job-name=test_slurm srun --ntasks = 1 --exclusive 1cpu_jobstep.sh & srun --ntasks = 2 --exclusive 2cpu_jobstep.sh & srun --ntasks = 4 --exclusive 4cpu_jobstep.sh & wait When not to use parallel job steps The above works if you have many relatively similar heterogeneous jobs steps. If you have for example one job that is resource intensive but short, and the other is resource limited but takes long, you can probably better submit them as separate jobs using a sbatch command for both of them. This is because a single job will keep the same resources allocated for the entire duration. So, the allocated resources will be the resources needed for the resource intensive job step, and will remain allocated during the long non-intensive job step.","title":"Parallelization of job steps"},{"location":"documentation/SLURM_usage/submit_jobs/#interactive-jobs","text":"Performing heavy calculations on the home node is not a good practice. However, for debugging, or small/simple steps, you might want to perform relatively heavy jobs interactively. With srun you can allocate a job and use bash to interactively call commands within the allocation. With the script below we allocate resources with 4 CPUs, 4G memory per CPU restricted to a time of 4 hours. With the option --pty we say that we want to perform the task in \u2018pseudo-terminal mode\u2019, and the terminal of our choice is bash. [ username@binfservms01 ~ ] $ srun --cpus-per-task = 4 --mem-per-cpu = 4000 --time = 04 :00:00 --pty bash srun: job 6200830 queued and waiting for resources srun: job 6200830 has been allocated resources [ username@binfservas32 ~ ] $ Different node Note that the new shell is not at the home node binfservms01 but at a compute node assigned by SLURM: binfservas32","title":"Interactive jobs"},{"location":"documentation/resources/cloud_services/","text":"Campus Cloud The is a university-wide cloud instance. Command line upload The bash script share_file_through_campus_cloud.sh on binfservms01 allows you to upload a file or folder directly from the cluster to your campus cloud and share it with external parties, even outside the university. You can the script and more documentation in a dedicated github repository . Simply upload a file/folder without sharing it: /mnt/apps/centos7/bin/share_file_through_campus_cloud.sh -l <location> Upload a file and create a public link to it: ( Only works for files, link will be accessible by anyone.) /mnt/apps/centos7/bin/share_file_through_campus_cloud.sh \\ -l <location> \\ --public-link \\ -d <days> Upload and then share the file/folder: /mnt/apps/centos7/bin/share_file_through_campus_cloud.sh \\ -l <location> \\ -e <email ( s ) > \\ -d <days> Add the script to your .bashrc If you want to use the script without having to remember the path to it, add this line at the end of your ~/.bashrc file: alias share_file_through_campus_cloud.sh = '/mnt/apps/centos7/bin/share_file_through_campus_cloud.sh' Explanation of the parameters -l, \u2014location location of file/folder to be shared. -e, \u2014email email of recipient(s): If multiple, they must be separated by comma; e.g., \u2018-e a@x.com,b@x.com\u2019 (Optional.) -p, \u2014public-link return a public link. Takes no arguments. Only works for files. (Optional.) -d, \u2014days days until access expires. 0 means share doesn\u2019t expire. (Optional. Default = 10 days.) -h, \u2014help display this help and exit Next, you will be prompted to enter your campus username and password, then for a name of the new folder on your Campus Cloud. Then, the upload will begin. Command line download The opposite direction is also possible and it works as follows: Open CampusCloud , locate the file you want to download. Create a freely accessible link to the file. Download using wget: wget <link-to-file> Notes To save space on the Cluster, consider compression. (E.g.: zip foo.zip file1.txt file2.txt or zip -r foo.zip /folder ) The files will be uploaded to your personal campus account. Access to the file wills end for the external user after 10 days or a specified number of days ( -d parameter), but the file will then not be deleted on your Campus Cloud. If you want to share the same file later-on with more people, you don\u2019t have to upload it again: Simply go to campuscloud.unibe.ch and change the sharing parameters of the file. If you want to revoke somebody\u2019s right to see a file, go to campuscloud.unibe.ch and edit the relevant sharing permission. External users will have to set up a guest account the first time a file/folder is shared with their particular email address. They will have to know the login info for the next time something is shared with them. Can I use it on my own machine? Yes, if your machine supports bash and jq (a JSON parser) is installed. Get the script from the github page .","title":"Cloud services"},{"location":"documentation/resources/cloud_services/#campus-cloud","text":"The is a university-wide cloud instance.","title":"Campus Cloud"},{"location":"documentation/resources/cloud_services/#command-line-upload","text":"The bash script share_file_through_campus_cloud.sh on binfservms01 allows you to upload a file or folder directly from the cluster to your campus cloud and share it with external parties, even outside the university. You can the script and more documentation in a dedicated github repository . Simply upload a file/folder without sharing it: /mnt/apps/centos7/bin/share_file_through_campus_cloud.sh -l <location> Upload a file and create a public link to it: ( Only works for files, link will be accessible by anyone.) /mnt/apps/centos7/bin/share_file_through_campus_cloud.sh \\ -l <location> \\ --public-link \\ -d <days> Upload and then share the file/folder: /mnt/apps/centos7/bin/share_file_through_campus_cloud.sh \\ -l <location> \\ -e <email ( s ) > \\ -d <days> Add the script to your .bashrc If you want to use the script without having to remember the path to it, add this line at the end of your ~/.bashrc file: alias share_file_through_campus_cloud.sh = '/mnt/apps/centos7/bin/share_file_through_campus_cloud.sh' Explanation of the parameters -l, \u2014location location of file/folder to be shared. -e, \u2014email email of recipient(s): If multiple, they must be separated by comma; e.g., \u2018-e a@x.com,b@x.com\u2019 (Optional.) -p, \u2014public-link return a public link. Takes no arguments. Only works for files. (Optional.) -d, \u2014days days until access expires. 0 means share doesn\u2019t expire. (Optional. Default = 10 days.) -h, \u2014help display this help and exit Next, you will be prompted to enter your campus username and password, then for a name of the new folder on your Campus Cloud. Then, the upload will begin.","title":"Command line upload"},{"location":"documentation/resources/cloud_services/#command-line-download","text":"The opposite direction is also possible and it works as follows: Open CampusCloud , locate the file you want to download. Create a freely accessible link to the file. Download using wget: wget <link-to-file> Notes To save space on the Cluster, consider compression. (E.g.: zip foo.zip file1.txt file2.txt or zip -r foo.zip /folder ) The files will be uploaded to your personal campus account. Access to the file wills end for the external user after 10 days or a specified number of days ( -d parameter), but the file will then not be deleted on your Campus Cloud. If you want to share the same file later-on with more people, you don\u2019t have to upload it again: Simply go to campuscloud.unibe.ch and change the sharing parameters of the file. If you want to revoke somebody\u2019s right to see a file, go to campuscloud.unibe.ch and edit the relevant sharing permission. External users will have to set up a guest account the first time a file/folder is shared with their particular email address. They will have to know the login info for the next time something is shared with them.","title":"Command line download"},{"location":"documentation/resources/cloud_services/#can-i-use-it-on-my-own-machine","text":"Yes, if your machine supports bash and jq (a JSON parser) is installed. Get the script from the github page .","title":"Can I use it on my own machine?"},{"location":"documentation/resources/reference_genomes/","text":"Reference genomes for commonly used taxa are available here: /data/references Important : All cluster users have write permission in this folder. You are welcome to add new files or new species as long as you stick to this structure: Each species-specific folder must contain a README with information on (i) when a file was downloaded, (ii) from where it was downloaded and (iii) how it was processed (e.g. how a particular index was built). We will regularly check data/references and delete without warning any folders/files that do not follow the mandatory structure or are not properly documented. Currently available species include: - Bos_taurus - Canis_lupus_familiaris - Capra_hircus - Danio_rerio - Drosophila_melanogaster - Equus_caballus - Escherichia_virus_phiX174 - Gasterosteus_aculeatus - Homo_sapiens - Mus_musculus - Neogobius_melanostomus - Ovis_aries - Pundamilia_nyererei - Rattus_norvegicus - Salmo_trutta - sDMDMm2_flora - Sus_scrofa - Xiphorus_maculatus","title":"Reference genomes"},{"location":"documentation/resources/rstudio_server/","text":"RStudio Server This will create a user-specific session and you will need to install all non-default packages (note: packages are installed in user libraries) https://rstudio.bioinformatics.unibe.ch/users/$USER_NAME/ (Do not forget the \u201c/\u201d at the end) To link RStudio server to the filesystem: create an RSTUDIO directory in the project root (e.g. /data/projects/$PROJECT_ID/ ) or at the user level ( /data/users/$USERNAME/) the folder will appear in your RStudio home directory (check the files pane). Note: this may take up to 10' (worst case scenario)","title":"Rstudio server"},{"location":"documentation/resources/rstudio_server/#rstudio-server","text":"This will create a user-specific session and you will need to install all non-default packages (note: packages are installed in user libraries) https://rstudio.bioinformatics.unibe.ch/users/$USER_NAME/ (Do not forget the \u201c/\u201d at the end) To link RStudio server to the filesystem: create an RSTUDIO directory in the project root (e.g. /data/projects/$PROJECT_ID/ ) or at the user level ( /data/users/$USERNAME/) the folder will appear in your RStudio home directory (check the files pane). Note: this may take up to 10' (worst case scenario)","title":"RStudio Server"},{"location":"documentation/resources/shared_datasets/","text":"The directory /data/datasets is for depositing datasets that can be used by users of the IBU cluster. The data sharing has two main objectives: To save space on the cluster: Several people might work on the same data with different objectives, using different analysis pipelines. To have a centralized location for storing small datasets for pipeline testing or tutorials. In order to keep this shared folder tidy, a few guidelines are explained here. Folder Structure There is no set folder structure that has to be followed. Please consider the following: Each dataset should have a meaningful folder name. Inside the data folder, the README has to be placed and filled out (the template is provided at data/README_TEMPLATE). An example folder structure could look like the following: /data/datasets | GUIDELINES | README\\_TEMPLATE | \u2514\u2500\u2500\u2500 [dataset name] | | \u2502 \u2502 README \u2502 \u2514\u2500\u2500\u2500 Raw \u2502 \u2514\u2500\u2500\u2500 p100 \u2502 | \u2514\u2500\u2500\u2500 sample1\\_L1\\_R1\\_001.fastq.gz \u2502 | \u2514\u2500\u2500\u2500 sample1\\_L1\\_R2\\_001.fastq.gz \u2502 | \u2514\u2500\u2500\u2500 sample2\\_L1\\_R1\\_001.fastq.gz \u2502 | \u2514\u2500\u2500\u2500 ... \u2502 | \u2502 \u2514\u2500\u2500\u2500 p1 \u2502 \u2514\u2500\u2500\u2500 sample1\\_L1\\_R1\\_001.fastq.gz \u2502 \u2514\u2500\u2500\u2500 sample1\\_L1\\_R2\\_001.fastq.gz \u2502 \u2514\u2500\u2500\u2500 sample2\\_L1\\_R1\\_001.fastq.gz \u2502 \u2514\u2500\u2500\u2500 ... | \u2514\u2500\u2500\u2500 other\\_meaningful\\_dataset\\_name | | | | README | \u2514\u2500\u2500\u2500 ... | \u2514\u2500\u2500\u2500 ... \u2514\u2500\u2500\u2500 ... In this example unprocessed raw data is stored in a folder called Raw . Inside the Raw folder, there is a folder which indicates the percentage of original data stored. For example, p100 means that the files inside this folder contains all original reads. Similarily, p1 means that the original data has been subsampled to 1%. These datasets are helpful for pipeline testing or tutorials etc. There might be several different use cases which may include the storage of some processed data files (e.g. .bam files). Please chose appropriate names for subfolders and make a README file inside the subfolder so that anyone can understand where the data came from and how it was processed.","title":"Shared datasets"},{"location":"documentation/resources/shared_datasets/#folder-structure","text":"There is no set folder structure that has to be followed. Please consider the following: Each dataset should have a meaningful folder name. Inside the data folder, the README has to be placed and filled out (the template is provided at data/README_TEMPLATE). An example folder structure could look like the following: /data/datasets | GUIDELINES | README\\_TEMPLATE | \u2514\u2500\u2500\u2500 [dataset name] | | \u2502 \u2502 README \u2502 \u2514\u2500\u2500\u2500 Raw \u2502 \u2514\u2500\u2500\u2500 p100 \u2502 | \u2514\u2500\u2500\u2500 sample1\\_L1\\_R1\\_001.fastq.gz \u2502 | \u2514\u2500\u2500\u2500 sample1\\_L1\\_R2\\_001.fastq.gz \u2502 | \u2514\u2500\u2500\u2500 sample2\\_L1\\_R1\\_001.fastq.gz \u2502 | \u2514\u2500\u2500\u2500 ... \u2502 | \u2502 \u2514\u2500\u2500\u2500 p1 \u2502 \u2514\u2500\u2500\u2500 sample1\\_L1\\_R1\\_001.fastq.gz \u2502 \u2514\u2500\u2500\u2500 sample1\\_L1\\_R2\\_001.fastq.gz \u2502 \u2514\u2500\u2500\u2500 sample2\\_L1\\_R1\\_001.fastq.gz \u2502 \u2514\u2500\u2500\u2500 ... | \u2514\u2500\u2500\u2500 other\\_meaningful\\_dataset\\_name | | | | README | \u2514\u2500\u2500\u2500 ... | \u2514\u2500\u2500\u2500 ... \u2514\u2500\u2500\u2500 ... In this example unprocessed raw data is stored in a folder called Raw . Inside the Raw folder, there is a folder which indicates the percentage of original data stored. For example, p100 means that the files inside this folder contains all original reads. Similarily, p1 means that the original data has been subsampled to 1%. These datasets are helpful for pipeline testing or tutorials etc. There might be several different use cases which may include the storage of some processed data files (e.g. .bam files). Please chose appropriate names for subfolders and make a README file inside the subfolder so that anyone can understand where the data came from and how it was processed.","title":"Folder Structure"},{"location":"documentation/resources/software_modules/","text":"Vital-IT A lot of software is pre-installed as modules on the server. In the SLURM tutorial it is explained how to use these modules. All modules can be found here: http://www.vital-it.ch/services/software Vital-IT singularity containers Singularity is installed on all nodes, but not on ms01. Containers can be run via bash-scripts submitted to slurm or in an interactive slurm session. In order to test or debug a singularity container, start an interactive slurm session: srun --pty --mem = 20G /bin/bash Usually, you can run the software after the singularity executable like you\u2019re used to via the command line. E.g. running the tool import from QIIME would look like this: singularity exec /software/singularity/containers/qiime2-2018.8-1.debian9.simg qiime tools import --param1 someParam --param2 someParam You can find SIB course material on how to use docker and singularity here . Locally installed software This needs to be elaborated /mnt/apps/ Spack The following has to be run before using spack on the IBU cluster: source /mnt/cluster/software/bin/ibuspack.sh Easiest is to put this line in your bashrc file. To see all spack modules currently installed on our cluster module avail To see all tools that are available as spack modules and could be installed: spack list","title":"Software modules"},{"location":"documentation/resources/software_modules/#vital-it","text":"A lot of software is pre-installed as modules on the server. In the SLURM tutorial it is explained how to use these modules. All modules can be found here: http://www.vital-it.ch/services/software","title":"Vital-IT"},{"location":"documentation/resources/software_modules/#vital-it-singularity-containers","text":"Singularity is installed on all nodes, but not on ms01. Containers can be run via bash-scripts submitted to slurm or in an interactive slurm session. In order to test or debug a singularity container, start an interactive slurm session: srun --pty --mem = 20G /bin/bash Usually, you can run the software after the singularity executable like you\u2019re used to via the command line. E.g. running the tool import from QIIME would look like this: singularity exec /software/singularity/containers/qiime2-2018.8-1.debian9.simg qiime tools import --param1 someParam --param2 someParam You can find SIB course material on how to use docker and singularity here .","title":"Vital-IT singularity containers"},{"location":"documentation/resources/software_modules/#locally-installed-software","text":"This needs to be elaborated /mnt/apps/","title":"Locally installed software"},{"location":"documentation/resources/software_modules/#spack","text":"The following has to be run before using spack on the IBU cluster: source /mnt/cluster/software/bin/ibuspack.sh Easiest is to put this line in your bashrc file. To see all spack modules currently installed on our cluster module avail To see all tools that are available as spack modules and could be installed: spack list","title":"Spack"},{"location":"tutorials/SLURM_tutorial/","text":"Experienced users Are you a more experienced user and looking for some specific SLURM documentation? Have a look at the SLURM usage page How we work: For each chapter (2 till 5) we\u2019ll have a few slides of presentation and exercises. We\u2019ll discuss the exercises and continue with the next chapter when most of you have finished the exercises of the current chapter. You\u2019re doing this to learn, so don\u2019t be tempted to look at the answers to early. If the given answer doesn\u2019t correspond to your answer, try to understand why. If you\u2019re stuck, ask your colleagues first. Someone who has just learned, is often better able to explain it than a teacher. Material Download the presentation 1. Setting up VSCode to work on the IBU cluster To work conveniently on the cluster we will set up VScode. VScode is a code editor that can be used to edit files and run commands locally, but also on a remote server. In this subchapter we will set up VScode to work on the IBU cluster. Required installations For this exercise you will need to have installed VScode . In addition you would need to have followed the instructions to set up remote-ssh: OpenSSH compatible client . This is usually pre-installed on your OS. You can check whether the command ssh exists. The Remote-SSH extension. To install, open VSCode and click on the extensions icon (four squares) on the left side of the window. Search for Remote-SSH and click on Install . Open VScode and click on the green or blue button in the bottom left corner. Select Connect to Host... , and then on Configure SSH Host... . Specify a the location for the config file. Use the same directory as where your keys are stored (so ~/.ssh ). A skeleton config file will be provided. Edit it, so it looks like this: Windows MacOS/Linux Host ibucluster User username HostName binfservms01.unibe.ch IdentityFile ~\\.ssh\\id_rsa Host ibucluster User username HostName binfservms01.unibe.ch IdentityFile ~/.ssh/id_rsa Save and close the config file. Now click again the green or blue button in the bottom left corner. Select Connect to Host... , and then on ibucluster . You will be asked which operating system is used on the remote. Specify \u2018Linux\u2019. If your private key is password protected, and you don\u2019t have the ssh agent running (this will be true for most Linux/Mac OS users), you will be asked for your private key password. Now that you have connected to the cluster, you will end up in your home directory on the head node ( /home/yourusername ). However, we want to work in your personal directory on /data . This directory is at /data/users/yourusername . First, we will create a working directory called slurm_tutorial in your personal directory. Write in the terminal: mkdir -p /data/users/yourusername/slurm_tutorial After that, open this directory as a working directory in VSCode by clicking the blue button \u2018Open Folder\u2019 or by going to \u2018File\u2019 > \u2018Open Folder\u2019. Type the path to the directory slurm_tutorial in your personal directory ( /data/users/yourusername/slurm_tutorial ). You will now be able to edit files in this directory. 2. SLURM introduction Note From now on you will work in VSCode. Make sure you have set /data/users/yourusername/slurm_tutorial as your working directory. Exercise 2A: Create a script named sleep.sh (File > New Text File). Write a command in it that lets the system sleep for 120 seconds (use the command sleep ). Answer Your script should look like this: sleep.sh #!/usr/bin/env bash sleep 120 You could submit this script as a job to the cluster with the following command: sbatch \\ --cpus-per-task = 1 \\ --mem-per-cpu = 100M \\ --time = 00 :05:00 \\ sleep.sh Exercise 2B: Rewrite the script sleep.sh in a way that you integrate the SLURM options in the script (hint: use #SBATCH ). Answers Your script should look like this: sleep.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=100M #SBATCH --time=00:05:00 sleep 120 Exercise 2C: Submit your script with integrated sbatch options to the cluster. Check out the status of the job (use squeue ). Answer Sumbit it like this: sbatch sleep.sh Check out the status: squeue --job [JOBID] or squeue -A [USERNAME] 3. sbatch options 3.1 Required resources Exercise 3.1A: The command sleep requires none or very little computational resources. Modify the options --time and --mem-per-cpu to minimize the allocation. Submit it to the cluster with sbatch to see if it still works. Answer Your script could look like this: sleep.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1M #SBATCH --time=00:02:10 sleep 120 3.2 User specific options Exercise 3.2A: Add options to sleep.sh that: gives it a name provide your e-mail address e-mails you when it begins and ends Submit it to the cluster, and answer the following questions based on the information in the e-mails: How long was your job queued? On which node did it run? How long did the job take? Answer Your script should look like this: sleep.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1M #SBATCH --time=00:02:10 #SBATCH --job-name=go_to_sleep #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end sleep 120 The queued time is in the subject of the first e-mail. The node you can find in the second e-mail at NodeList The time the job took is also in the second e-mail at Elapsed 3.3 Output and error Exercise 3.3A: Create a new script called output_and_error.sh that: writes a message to stdout (hint: use echo ). generates an error by using ls on a file that doesn\u2019t exist. Run it at the login node. Where do stdin and stdout end up? Answer Your script should look like this: output_and_error.sh #!/usr/bin/env bash echo \"I am output!\" ls my_nonexisting_file.txt The output and error end up both at the console. Exercise 3.3B: Add sbatch options to output_and_error.sh that we have learned before (job name, email, cpu, memory and time) with sensible parameters. Now also include --output and --error . Use %j to associate the files with the job ID. Submit it with sbatch. Were your expected files created? Where did stdout and stderr end up? Answer Your script should look like this: output_and_error.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=100M #SBATCH --time=00:01:00 #SBATCH --job-name=output_and_error #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_%j.e echo \"I am output!\" ls my_nonexisting_file.txt Stdout should be in /data/users/[USER]/slurm_tutorial/output_[JOBID].o and stderr in /data/users/[USER]/slurm_tutorial/error_[JOBID].e 4. Interactive jobs Exercise 4A: Create interactive job with srun . Use the options --cpus-per-task , --mem-per-cpu and --time . Allocate maximum 1 CPU, 500 MB memory for 5 minutes. What is your job ID? To which node is your job submitted? Answer Your command should look like this: srun --cpus-per-task = 1 --mem-per-cpu = 500M --time = 00 :05:00 --pty bash You can find the job ID and node with squeue . However the node you\u2019re on also appears in the shell, e.g.: [ gvangeest@binfservms01 ~ ] $ becomes something different, e.g.: [ gvangeest@binfservas07 ~ ] $ 5. Modules and containers Exercise 5A: Load the module for the latest version of minimap2 (look it up here: https://www.vital-it.ch/services ), and check out the help documentation with minimap2 --help . Answer The command to load minimap2 is: module add UHTS/Analysis/minimap2/2.17 Exercise 5B: Start an interactive job with srun , and allocate little resources. Is minimap2 still available in your interactive job? Answer Yes, it should still be available. If a job is submitted, the current environment of the user is used for that job to run in. However, in a script, it is good practice to always include the module add commands, to make your script re-usable and sharable. We\u2019ll now be working with some actual biological data. Go to /data/courses/HPC_tutorial/ecoli/ and see what\u2019s in there. Exercise 5C: Submit a job to run minimap2 to align sequence reads of sample 1 to the reference ecoli-strK12-MG1655.fasta . Call the script run_minimap2.sh . Use the sbatch options for cpu, memory, time, job name, e-mail, output and error. This job requires: 3 CPU 200M of memory per cpu 1 minute You can find the commands to run minimap2 below. Add the sbatch options and module add command to this script and submit to the cluster. Warning Do not run this on the head node! run_minimap2.sh REFERENCE_DIR = /data/courses/HPC_tutorial/ecoli/reference READS_DIR = /data/courses/HPC_tutorial/ecoli/reads ALIGN_DIR = /data/users/ [ USER ] /slurm_tutorial/ecoli/alignment mkdir -p $ALIGN_DIR cd $READS_DIR minimap2 \\ -a \\ -x sr \\ $REFERENCE_DIR /ecoli-strK12-MG1655.fasta \\ ecoli_sample1.fastq \\ > $ALIGN_DIR /ecoli_sample1.fastq.sam Answer Your script should look like this: run_minimap2.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=3 #SBATCH --mem-per-cpu=200M #SBATCH --time=00:01:00 #SBATCH --job-name=align_reads #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_alignment_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_alignment_%j.e module add UHTS/Analysis/minimap2/2.17 REFERENCE_DIR = /data/courses/HPC_tutorial/ecoli/reference READS_DIR = /data/courses/HPC_tutorial/ecoli/reads ALIGN_DIR = /data/users/ [ USER ] /slurm_tutorial/ecoli/alignment mkdir -p $ALIGN_DIR cd $READS_DIR minimap2 \\ -a \\ -x sr \\ $REFERENCE_DIR /ecoli-strK12-MG1655.fasta \\ ecoli_sample1.fastq \\ > $ALIGN_DIR /ecoli_sample1.fastq.sam As you have learned in the apptainer exercises , you can run applications in a container. We have created a container with minimap2 in it. You can find it at /data/courses/HPC_tutorial/containers/minimap2.sif . We\u2019ll use this container to run minimap2 in the next exercise. Exercise 5D: Do the same job submission as in exercise 4C, but now run the command in a container with apptainer. Make sure to add the option --bind /data/courses to the apptainer exec command, because by default the container has no access to the /data/courses directory. Hint A command to run an application in a container looks like this: apptainer exec --bind /data/courses /path/to/container.sif command Answer The script looks the same as in exercise 4C, but now with the apptainer exec command: run_minimap2.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=3 #SBATCH --mem-per-cpu=200M #SBATCH --time=00:01:00 #SBATCH --job-name=align_reads #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_alignment_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_alignment_%j.e module add UHTS/Analysis/minimap2/2.17 REFERENCE_DIR = /data/courses/HPC_tutorial/ecoli/reference READS_DIR = /data/courses/HPC_tutorial/ecoli/reads ALIGN_DIR = /data/users/ [ USER ] /slurm_tutorial/ecoli/alignment mkdir -p $ALIGN_DIR cd $READS_DIR apptainer exec \\ --bind /data/courses \\ /data/courses/HPC_tutorial/containers/minimap2.sif \\ minimap2 \\ -a \\ -x sr \\ $REFERENCE_DIR /ecoli-strK12-MG1655.fasta \\ ecoli_sample1.fastq \\ > $ALIGN_DIR /ecoli_sample1.fastq.sam 6 Job arrays 6.1 Jobs in parallel Exercise 6.1A: Generate a script array.sh with the sbatch options for cpu, memory, time, job name, e-mail, output and error. Now also include the option to initiate an array counting from 10 till 15. In the script, let each element of the array create a file with the $SLURM_ARRAY_TASK_ID in it\u2019s name in your home directory. In order to check out squeue , let the system sleep for 60 seconds after generating the file. Hint Your script (without sbatch options) should look something like this: array.sh touch /data/users/ [ USER ] /slurm_tutorial/file_ $SLURM_ARRAY_TASK_ID .txt sleep 60 How many jobs were created? How many error and output files were created? Answer Your script should like like this: array.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1M #SBATCH --time=00:02:00 #SBATCH --job-name=arrays #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_%j.e #SBATCH --array=10-15 touch /data/users/ [ USER ] /slurm_tutorial/file_ $SLURM_ARRAY_TASK_ID .txt sleep 60 Six jobs were created Six output and six error files are created 6.2 Using UNIX arrays Exercise 6.2A: Modify the script in exercise 4C to align the reads of all three samples in separate jobs. Use the sbatch --array option in combination with a UNIX array, and submit it to the cluster. Check out the output files. Hint Modify the script in exercise 4C with the following steps: Add the option --array to the sbatch options (remember: there are three files and UNIX starts counting at 0) Make the directory with reads the current directory, and generate variable with a UNIX array containing the read files. (Use () and * , e.g. FILES=(*.fastq) ) Replace the read file name that is used as input for minimap2 with something like ${FILES[$SLURM_ARRAY_TASK_ID]} Add .sam to the variable file name to modify the output file name. Answer Your script should look like this: run_minimap2.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=3 #SBATCH --mem-per-cpu=200M #SBATCH --time=00:01:00 #SBATCH --job-name=align_reads #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_alignment_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_alignment_%j.e #SBATCH --array=0-2 module add UHTS/Analysis/minimap2/2.17 REFERENCE_DIR = /data/courses/HPC_tutorial/ecoli/reference READS_DIR = /data/courses/HPC_tutorial/ecoli/reads ALIGN_DIR = /data/users/ [ USER ] /slurm_tutorial/ecoli/alignment mkdir -p $ALIGN_DIR cd $READS_DIR FILES =( *.fastq ) apptainer exec \\ --bind /data/courses \\ /data/courses/HPC_tutorial/containers/minimap2.sif \\ minimap2 \\ -a \\ -x sr \\ $REFERENCE_DIR /ecoli-strK12-MG1655.fasta \\ ${ FILES [ $SLURM_ARRAY_TASK_ID ] } \\ > $ALIGN_DIR / ${ FILES [ $SLURM_ARRAY_TASK_ID ] } .sam","title":"SLURM tutorial"},{"location":"tutorials/SLURM_tutorial/#how-we-work","text":"For each chapter (2 till 5) we\u2019ll have a few slides of presentation and exercises. We\u2019ll discuss the exercises and continue with the next chapter when most of you have finished the exercises of the current chapter. You\u2019re doing this to learn, so don\u2019t be tempted to look at the answers to early. If the given answer doesn\u2019t correspond to your answer, try to understand why. If you\u2019re stuck, ask your colleagues first. Someone who has just learned, is often better able to explain it than a teacher.","title":"How we work:"},{"location":"tutorials/SLURM_tutorial/#material","text":"Download the presentation","title":"Material"},{"location":"tutorials/SLURM_tutorial/#1-setting-up-vscode-to-work-on-the-ibu-cluster","text":"To work conveniently on the cluster we will set up VScode. VScode is a code editor that can be used to edit files and run commands locally, but also on a remote server. In this subchapter we will set up VScode to work on the IBU cluster. Required installations For this exercise you will need to have installed VScode . In addition you would need to have followed the instructions to set up remote-ssh: OpenSSH compatible client . This is usually pre-installed on your OS. You can check whether the command ssh exists. The Remote-SSH extension. To install, open VSCode and click on the extensions icon (four squares) on the left side of the window. Search for Remote-SSH and click on Install . Open VScode and click on the green or blue button in the bottom left corner. Select Connect to Host... , and then on Configure SSH Host... . Specify a the location for the config file. Use the same directory as where your keys are stored (so ~/.ssh ). A skeleton config file will be provided. Edit it, so it looks like this: Windows MacOS/Linux Host ibucluster User username HostName binfservms01.unibe.ch IdentityFile ~\\.ssh\\id_rsa Host ibucluster User username HostName binfservms01.unibe.ch IdentityFile ~/.ssh/id_rsa Save and close the config file. Now click again the green or blue button in the bottom left corner. Select Connect to Host... , and then on ibucluster . You will be asked which operating system is used on the remote. Specify \u2018Linux\u2019. If your private key is password protected, and you don\u2019t have the ssh agent running (this will be true for most Linux/Mac OS users), you will be asked for your private key password. Now that you have connected to the cluster, you will end up in your home directory on the head node ( /home/yourusername ). However, we want to work in your personal directory on /data . This directory is at /data/users/yourusername . First, we will create a working directory called slurm_tutorial in your personal directory. Write in the terminal: mkdir -p /data/users/yourusername/slurm_tutorial After that, open this directory as a working directory in VSCode by clicking the blue button \u2018Open Folder\u2019 or by going to \u2018File\u2019 > \u2018Open Folder\u2019. Type the path to the directory slurm_tutorial in your personal directory ( /data/users/yourusername/slurm_tutorial ). You will now be able to edit files in this directory.","title":"1. Setting up VSCode to work on the IBU cluster"},{"location":"tutorials/SLURM_tutorial/#2-slurm-introduction","text":"Note From now on you will work in VSCode. Make sure you have set /data/users/yourusername/slurm_tutorial as your working directory. Exercise 2A: Create a script named sleep.sh (File > New Text File). Write a command in it that lets the system sleep for 120 seconds (use the command sleep ). Answer Your script should look like this: sleep.sh #!/usr/bin/env bash sleep 120 You could submit this script as a job to the cluster with the following command: sbatch \\ --cpus-per-task = 1 \\ --mem-per-cpu = 100M \\ --time = 00 :05:00 \\ sleep.sh Exercise 2B: Rewrite the script sleep.sh in a way that you integrate the SLURM options in the script (hint: use #SBATCH ). Answers Your script should look like this: sleep.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=100M #SBATCH --time=00:05:00 sleep 120 Exercise 2C: Submit your script with integrated sbatch options to the cluster. Check out the status of the job (use squeue ). Answer Sumbit it like this: sbatch sleep.sh Check out the status: squeue --job [JOBID] or squeue -A [USERNAME]","title":"2. SLURM introduction"},{"location":"tutorials/SLURM_tutorial/#3-sbatch-options","text":"","title":"3. sbatch options"},{"location":"tutorials/SLURM_tutorial/#31-required-resources","text":"Exercise 3.1A: The command sleep requires none or very little computational resources. Modify the options --time and --mem-per-cpu to minimize the allocation. Submit it to the cluster with sbatch to see if it still works. Answer Your script could look like this: sleep.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1M #SBATCH --time=00:02:10 sleep 120","title":"3.1 Required resources"},{"location":"tutorials/SLURM_tutorial/#32-user-specific-options","text":"Exercise 3.2A: Add options to sleep.sh that: gives it a name provide your e-mail address e-mails you when it begins and ends Submit it to the cluster, and answer the following questions based on the information in the e-mails: How long was your job queued? On which node did it run? How long did the job take? Answer Your script should look like this: sleep.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1M #SBATCH --time=00:02:10 #SBATCH --job-name=go_to_sleep #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end sleep 120 The queued time is in the subject of the first e-mail. The node you can find in the second e-mail at NodeList The time the job took is also in the second e-mail at Elapsed","title":"3.2 User specific options"},{"location":"tutorials/SLURM_tutorial/#33-output-and-error","text":"Exercise 3.3A: Create a new script called output_and_error.sh that: writes a message to stdout (hint: use echo ). generates an error by using ls on a file that doesn\u2019t exist. Run it at the login node. Where do stdin and stdout end up? Answer Your script should look like this: output_and_error.sh #!/usr/bin/env bash echo \"I am output!\" ls my_nonexisting_file.txt The output and error end up both at the console. Exercise 3.3B: Add sbatch options to output_and_error.sh that we have learned before (job name, email, cpu, memory and time) with sensible parameters. Now also include --output and --error . Use %j to associate the files with the job ID. Submit it with sbatch. Were your expected files created? Where did stdout and stderr end up? Answer Your script should look like this: output_and_error.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=100M #SBATCH --time=00:01:00 #SBATCH --job-name=output_and_error #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_%j.e echo \"I am output!\" ls my_nonexisting_file.txt Stdout should be in /data/users/[USER]/slurm_tutorial/output_[JOBID].o and stderr in /data/users/[USER]/slurm_tutorial/error_[JOBID].e","title":"3.3 Output and error"},{"location":"tutorials/SLURM_tutorial/#4-interactive-jobs","text":"Exercise 4A: Create interactive job with srun . Use the options --cpus-per-task , --mem-per-cpu and --time . Allocate maximum 1 CPU, 500 MB memory for 5 minutes. What is your job ID? To which node is your job submitted? Answer Your command should look like this: srun --cpus-per-task = 1 --mem-per-cpu = 500M --time = 00 :05:00 --pty bash You can find the job ID and node with squeue . However the node you\u2019re on also appears in the shell, e.g.: [ gvangeest@binfservms01 ~ ] $ becomes something different, e.g.: [ gvangeest@binfservas07 ~ ] $","title":"4. Interactive jobs"},{"location":"tutorials/SLURM_tutorial/#5-modules-and-containers","text":"Exercise 5A: Load the module for the latest version of minimap2 (look it up here: https://www.vital-it.ch/services ), and check out the help documentation with minimap2 --help . Answer The command to load minimap2 is: module add UHTS/Analysis/minimap2/2.17 Exercise 5B: Start an interactive job with srun , and allocate little resources. Is minimap2 still available in your interactive job? Answer Yes, it should still be available. If a job is submitted, the current environment of the user is used for that job to run in. However, in a script, it is good practice to always include the module add commands, to make your script re-usable and sharable. We\u2019ll now be working with some actual biological data. Go to /data/courses/HPC_tutorial/ecoli/ and see what\u2019s in there. Exercise 5C: Submit a job to run minimap2 to align sequence reads of sample 1 to the reference ecoli-strK12-MG1655.fasta . Call the script run_minimap2.sh . Use the sbatch options for cpu, memory, time, job name, e-mail, output and error. This job requires: 3 CPU 200M of memory per cpu 1 minute You can find the commands to run minimap2 below. Add the sbatch options and module add command to this script and submit to the cluster. Warning Do not run this on the head node! run_minimap2.sh REFERENCE_DIR = /data/courses/HPC_tutorial/ecoli/reference READS_DIR = /data/courses/HPC_tutorial/ecoli/reads ALIGN_DIR = /data/users/ [ USER ] /slurm_tutorial/ecoli/alignment mkdir -p $ALIGN_DIR cd $READS_DIR minimap2 \\ -a \\ -x sr \\ $REFERENCE_DIR /ecoli-strK12-MG1655.fasta \\ ecoli_sample1.fastq \\ > $ALIGN_DIR /ecoli_sample1.fastq.sam Answer Your script should look like this: run_minimap2.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=3 #SBATCH --mem-per-cpu=200M #SBATCH --time=00:01:00 #SBATCH --job-name=align_reads #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_alignment_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_alignment_%j.e module add UHTS/Analysis/minimap2/2.17 REFERENCE_DIR = /data/courses/HPC_tutorial/ecoli/reference READS_DIR = /data/courses/HPC_tutorial/ecoli/reads ALIGN_DIR = /data/users/ [ USER ] /slurm_tutorial/ecoli/alignment mkdir -p $ALIGN_DIR cd $READS_DIR minimap2 \\ -a \\ -x sr \\ $REFERENCE_DIR /ecoli-strK12-MG1655.fasta \\ ecoli_sample1.fastq \\ > $ALIGN_DIR /ecoli_sample1.fastq.sam As you have learned in the apptainer exercises , you can run applications in a container. We have created a container with minimap2 in it. You can find it at /data/courses/HPC_tutorial/containers/minimap2.sif . We\u2019ll use this container to run minimap2 in the next exercise. Exercise 5D: Do the same job submission as in exercise 4C, but now run the command in a container with apptainer. Make sure to add the option --bind /data/courses to the apptainer exec command, because by default the container has no access to the /data/courses directory. Hint A command to run an application in a container looks like this: apptainer exec --bind /data/courses /path/to/container.sif command Answer The script looks the same as in exercise 4C, but now with the apptainer exec command: run_minimap2.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=3 #SBATCH --mem-per-cpu=200M #SBATCH --time=00:01:00 #SBATCH --job-name=align_reads #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_alignment_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_alignment_%j.e module add UHTS/Analysis/minimap2/2.17 REFERENCE_DIR = /data/courses/HPC_tutorial/ecoli/reference READS_DIR = /data/courses/HPC_tutorial/ecoli/reads ALIGN_DIR = /data/users/ [ USER ] /slurm_tutorial/ecoli/alignment mkdir -p $ALIGN_DIR cd $READS_DIR apptainer exec \\ --bind /data/courses \\ /data/courses/HPC_tutorial/containers/minimap2.sif \\ minimap2 \\ -a \\ -x sr \\ $REFERENCE_DIR /ecoli-strK12-MG1655.fasta \\ ecoli_sample1.fastq \\ > $ALIGN_DIR /ecoli_sample1.fastq.sam","title":"5. Modules and containers"},{"location":"tutorials/SLURM_tutorial/#6-job-arrays","text":"","title":"6 Job arrays"},{"location":"tutorials/SLURM_tutorial/#61-jobs-in-parallel","text":"Exercise 6.1A: Generate a script array.sh with the sbatch options for cpu, memory, time, job name, e-mail, output and error. Now also include the option to initiate an array counting from 10 till 15. In the script, let each element of the array create a file with the $SLURM_ARRAY_TASK_ID in it\u2019s name in your home directory. In order to check out squeue , let the system sleep for 60 seconds after generating the file. Hint Your script (without sbatch options) should look something like this: array.sh touch /data/users/ [ USER ] /slurm_tutorial/file_ $SLURM_ARRAY_TASK_ID .txt sleep 60 How many jobs were created? How many error and output files were created? Answer Your script should like like this: array.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1M #SBATCH --time=00:02:00 #SBATCH --job-name=arrays #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_%j.e #SBATCH --array=10-15 touch /data/users/ [ USER ] /slurm_tutorial/file_ $SLURM_ARRAY_TASK_ID .txt sleep 60 Six jobs were created Six output and six error files are created","title":"6.1 Jobs in parallel"},{"location":"tutorials/SLURM_tutorial/#62-using-unix-arrays","text":"Exercise 6.2A: Modify the script in exercise 4C to align the reads of all three samples in separate jobs. Use the sbatch --array option in combination with a UNIX array, and submit it to the cluster. Check out the output files. Hint Modify the script in exercise 4C with the following steps: Add the option --array to the sbatch options (remember: there are three files and UNIX starts counting at 0) Make the directory with reads the current directory, and generate variable with a UNIX array containing the read files. (Use () and * , e.g. FILES=(*.fastq) ) Replace the read file name that is used as input for minimap2 with something like ${FILES[$SLURM_ARRAY_TASK_ID]} Add .sam to the variable file name to modify the output file name. Answer Your script should look like this: run_minimap2.sh #!/usr/bin/env bash #SBATCH --cpus-per-task=3 #SBATCH --mem-per-cpu=200M #SBATCH --time=00:01:00 #SBATCH --job-name=align_reads #SBATCH --mail-user=user@students.unibe.ch #SBATCH --mail-type=begin,end #SBATCH --output=/data/users/[USER]/slurm_tutorial/output_alignment_%j.o #SBATCH --error=/data/users/[USER]/slurm_tutorial/error_alignment_%j.e #SBATCH --array=0-2 module add UHTS/Analysis/minimap2/2.17 REFERENCE_DIR = /data/courses/HPC_tutorial/ecoli/reference READS_DIR = /data/courses/HPC_tutorial/ecoli/reads ALIGN_DIR = /data/users/ [ USER ] /slurm_tutorial/ecoli/alignment mkdir -p $ALIGN_DIR cd $READS_DIR FILES =( *.fastq ) apptainer exec \\ --bind /data/courses \\ /data/courses/HPC_tutorial/containers/minimap2.sif \\ minimap2 \\ -a \\ -x sr \\ $REFERENCE_DIR /ecoli-strK12-MG1655.fasta \\ ${ FILES [ $SLURM_ARRAY_TASK_ID ] } \\ > $ALIGN_DIR / ${ FILES [ $SLURM_ARRAY_TASK_ID ] } .sam","title":"6.2 Using UNIX arrays"},{"location":"tutorials/SSH_tutorial/","text":"Public keys best practices Material Download the presentation Before you start his tutorial, you will need to: be on Unibe or UniFR Network (maybe start VPN ) have a \u201cIBU cluster\u201d account (have you received the \u201cWelcome to IBU Cluster\u201d email, looking like this: Welcome to IBU Cluster email Dear John we have created an account for you on the IBU computing cluster. Login Name: jdoe Password will be sent in a separate email. \u2026 Note In this following tutorial, code snippets witht the title \u201clocal\u201d are on your local computer; snippets with the tile \u201cremote\u201d are on the remote server. 1. Verify host public key Exercise 1A: check for existing host key Open the default terminal and make sure you are in your home directory. Then execute: For windows users If you are on windows use PowerShell for these exercises. So not WSL2 or MobaXterm. The reason for this is that it complicates working with VSCode in later exercises otherwise. Powershell mac OS/Linux local ssh-keygen -F binfservms01 . unibe . ch -l -f .\\. ssh \\ known_hosts local ssh-keygen -F binfservms01.unibe.ch -l -f ~/.ssh/known_hosts Result If you have been connecting to this server before, the output looks like this: # Host binfservms01.unibe.ch found: line 1 binfservms01.unibe.ch ECDSA SHA256:Yz6JYkqIEHYni+EJgEwQIPqlz0IEUBQLHEQVU8nEwSY $ Otherwise your output will be empty, or if you\u2019ve never used ssh before it will look like this: do_known_hosts: hostkeys_foreach failed: No such file or directory Exercise 1B: remove existing host key Because in this tutorial we will learn to verify host keys, we will first remove stored host keys if there are any. You can remove host keys with the following command: Powershell mac OS/Linux local ssh-keygen -R binfservms01 . unibe . ch -f .\\. ssh \\ known_hosts local ssh-keygen -R binfservms01.unibe.ch -f ~/.ssh/known_hosts Result # Host binfservms01.unibe.ch found: line 1 /home/user/.ssh/known_hosts updated. Original contents retained as /home/user/.ssh/known_hosts.old $ If you have never used ssh before you might get an error like this (can be safely ignored): mkstemp: No such file or directory Exercise 1C: check host key Warning Replace <hpcuser> with your Login Name on the server. First, let\u2019s try to login to the server with ssh : $ ssh <hpcuser>@binfservms01.unibe.ch Result The authenticity of host 'binfservms01.unibe.ch (130.92.199.95)' can't be established. ED25519 key fingerprint is SHA256:lvjBlWxYtrwFe509ktewvRFXCX7KfAZiLMlMh2mmbeY. Are you sure you want to continue connecting (yes/no/[fingerprint])? Note It is always wise to check that the fingerprint is correct. You should have got it from another source of information (eg email, website). Verify that the fingerprint is correct, then type yes . This will result in the following output: Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'binfservms01.unibe.ch' (ECDSA) to the list of known hosts. ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| <hpcuser>@binfservms01.unibe.ch's password: Now you can enter your password. Depending on your type of terminal, you might not be able to see the password being typed. Also, copy-pasting might be different compared to what you are used to (e.g. pasting by right-click in PowerShell). Result <hpcuser>@binfservms01.unibe.ch's password: ******* *ENTER* Last login: Wed Oct 21 16:57:39 2020 from dhcp-100-237.vpn.unibe.ch [<hpcuser>@binfservms01 ~]$ Exercise 1D: check content of .ssh directory on the server: We will add the public key to the server at a later stage. Now, check whether you already have a key in the .ssh directory at the server: remote ls .ssh Result If you don\u2019t have a public key at the server (which will be the case for new users), you\u2019ll get a message the .ssh directory does not exist (but we will create one): ls: cannot access .ssh: No such file or directory [<hpcuser>@binfservms01 ~]$ Exercise 1E: create .ssh directory and authorized_keys file on the server: We will create the .ssh directory and its authorized_keys file at the server and ensure it has proper permissions: remote mkdir .ssh touch .ssh/authorized_keys chmod -R go-rwx .ssh ls -la .ssh Result the .ssh directory and authorized_keys file have been created with permissions that only allow you to read and write: total 8 drwx------. 2 <hpcuser> <hpcuser> 29 29 sep 11:14 . drwx------. 8 <hpcuser> <hpcuser> 4096 29 sep 11:14 .. -rw-------. 1 <hpcuser> <hpcuser> 0 29 sep 11:16 authorized_keys [<hpcuser>@binfservms01 ~]$ Warning Logout before moving on to the next exercise. remote exit 2. Generate user keys pair Exercise 2A: use ssh-keygen command to generate a key and encrypt the private key with a passphrase Warning Below should be executed on your local computer Now, we can generate a public and private key on your local computer. Do this with the following command: local ssh-keygen This will prompt to with some questions: Generating public/private rsa key pair. Enter file in which to save the key (/home/<localuser>/.ssh/id_rsa): Usually, the default directory is where you want to store your keys, so hit ENTER . This is followed by a asking for a passphrase. This passphrase will be used to protect your keys locally. It will prevent anyone to access your keys from your local computer: Enter passphrase (empty for no passphrase): After completed this successfully, you will get a message like this: Enter passphrase (empty for no passphrase): *********** *ENTER* Enter same passphrase again: *********** *ENTER* Your identification has been saved in /home/<localuser>/.ssh/id_rsa Your public key has been saved in /home/<localuser>/.ssh/id_rsa.pub The key fingerprint is: SHA256:erkOXJp0loytm7+cbfk7rs9dDzaEtvP6GCmR9Bt7lvg user@laptop The key's randomart image is: +---[RSA 3072]----+ | | | | | . | | + o o . | | o S o = . | | o O . o O . | | B o ..O B .| | * +oo.@ +.| | ooBoo=O=E .| +----[SHA256]-----+ $ Exercise 2B: look at the generated files Now, key information has been stored in your local .ssh directory. Check it by typing: local ls -l .ssh Result Which should result in: -rw------- 1 <localuser> <localuser> 2.6K Okt 21 17:01 id_rsa -rw-r--r-- 1 <localuser> <localuser> 574 Okt 21 17:01 id_rsa.pub -rw-r--r-- 1 <localuser> <localuser> 444 Okt 22 13:56 known_hosts $ Check out the hash of your public key with: Powershell mac OS/Linux local cat .\\. ssh \\ id_rsa . pub local cat .ssh/id_rsa.pub Result ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQD38aCe4ICZZ6kPrxlAYruBNhguvHv5YQ2OPj L5Bvs2lzAtV1JPu+QQV9F5SUE3AJc7jh9yn/Agkrg4pMC9EDObWKTl5lg6ritcGtzQMfXYszEe vMNRv8ukV6nCt6WGfyjK4l61nXiuXxTv1RvGzJxAefdUGYMvMkkZOdkMKGKTxwE/xmyXJVYUPc EJEqGt4TSD3nC2Wg8GSp1L+MDpI5626UEVVafEzuOIbbHBmQMPhB+0MevP+ZsXzD0Dz1sWWI0w GlnU9W9a1gZ+QNiIeWCvKtNuxXFKB98338W3YQqE+dk/YwwSB1jeUHIRTEVSyKaIcr42s2Hg9E 2TEVZhmZM4vFJb8nozL8Hu3ZKAHqG1JR3FE1mqJ8kOHnWZiGNf3pQwUe3cgN7c5bsZPEl8VJGw uDArQSAFik+nmrNgQlcodIHYnzY6DtbOMnZUpWuVO1zfQQkPGBbGfdDuNT2cvxAkM1RkWtnCT5 JdOSn//4njp6aCfg38SopbDn3tfJcJTM= <localuser>@laptop $ And your private key with: Powershell mac OS/Linux local cat .\\. ssh \\ id_rsa local cat . ssh / id_rsa Result -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAFGhqvzt qUP/ybuCOapCXgAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQD38aCe4ICZ Z6kPrxlAYruBNhguvHv5YQ2OPjL5Bvs2lzAtV1JPu+QQV9F5SUE3AJc7jh9yn/Agkrg4pM ... /mOQjFg57Pn4XdswU+/gX3mbMWbZXJxIdUO1OIlCKolSe2dJA2CfYUv0XpCIWLe36Iiczn NqTpG7AzfMNH/Ok9Ojr2pqrQnSI= -----END OPENSSH PRIVATE KEY----- $ 3. Copy Public Key to server Now that we have generated both the public and private key files, we will need to copy the public key to the server, so it can be used for authentication. Exercise 3A: use ssh-copy-id to copy your public key to the server Copy the public key by using the following command: Powershell mac OS/Linux local type $env:USERPROFILE \\. ssh \\ id_rsa . pub | ssh < hpcuser > @binfservms01 . unibe . ch \"cat >> .ssh/authorized_keys\" local ssh-copy-id <hpcuser>@binfservms01.unibe.ch Result /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/home/<localuser>/.ssh/id_rsa.pub\" /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| <hpcuser>@binfservms01.unibe.ch's password: This requires authentication. Therefore type your password: Powershell mac OS/Linux <hpcuser>@binfservms01.unibe.ch's password: *********** *ENTER* <hpcuser>@binfservms01.unibe.ch's password: *********** *ENTER* Number of key(s) added: 1 Now try logging into the machine, with: \"ssh '<hpcuser>@binfservms01.unibe.ch'\" and check to make sure that only the key(s) you wanted were added. Exercise 3B: login to the server without password Now you can login on to the server without having to type the password associated with your server login. However, because you protected your keys with a passphrase, you will need to type the passphrase to access your local keys: local ssh <hpcuser>@binfservms01.unibe.ch Result ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| Enter passphrase for key '/home/<localuser>/.ssh/id_rsa': *********** *ENTER* Last login: Wed Oct 21 18:47:46 2020 from dhcp-99-231.vpn.unibe.ch [<hpcuser>@binfservms01 ~]$ Exercise 3C: check content of .ssh on the server We copied the public key to the server (with the ssh-copy-id command), so it should be on there. It is stored in the ssh folder. Check whether it\u2019s there with: remote ls -l .ssh Now the folder exists and contains a file with authorized keys: -rw-------. 1 <hpcuser> <hpcuser> 988 Oct 21 18:48 authorized_keys [<hpcuser>@binfservms01 ~]$ Check what\u2019s in there: remote cat .ssh/authorized_keys Result It\u2019s your public key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQD38aCe4ICZZ6kPrxlAYruBNhguvHv5YQ2OPj L5Bvs2lzAtV1JPu+QQV9F5SUE3AJc7jh9yn/Agkrg4pMC9EDObWKTl5lg6ritcGtzQMfXYszEe vMNRv8ukV6nCt6WGfyjK4l61nXiuXxTv1RvGzJxAefdUGYMvMkkZOdkMKGKTxwE/xmyXJVYUPc EJEqGt4TSD3nC2Wg8GSp1L+MDpI5626UEVVafEzuOIbbHBmQMPhB+0MevP+ZsXzD0Dz1sWWI0w GlnU9W9a1gZ+QNiIeWCvKtNuxXFKB98338W3YQqE+dk/YwwSB1jeUHIRTEVSyKaIcr42s2Hg9E 2TEVZhmZM4vFJb8nozL8Hu3ZKAHqG1JR3FE1mqJ8kOHnWZiGNf3pQwUe3cgN7c5bsZPEl8VJGw uDArQSAFik+nmrNgQlcodIHYnzY6DtbOMnZUpWuVO1zfQQkPGBbGfdDuNT2cvxAkM1RkWtnCT5 JdOSn//4njp6aCfg38SopbDn3tfJcJTM= <localuser>@laptop [<hpcuser>@binfservms01 ~]$ 4. SSH-Agent You probably don\u2019t want to re-type your passphrase every time you are connecting to the server. We will use ssh-agent to enable you to access your keys without having to type your passphrase every time you want to use them. Note This part is platform dependent. Choose here therefore either PowerShell or mac OS/Linux . PowerShell mac OS/Linux Exercise 4A: start ssh-agent Open PowerShell in admin mode (right-click on the PowerShell icon and choose \u201cRun as administrator\u201d). Then type: Get-Service ssh-agent | Set-Service -StartupType Automatic -PassThru | Start-Service To check that the ssh-agent is running: Get-Service -Name ssh-agent | Select-Object Status Result Status ------ Running Exercise 4A: start ssh-agent Write the output of ssh-agent to a file: $ ssh-agent > sshagent.txt $ cat sshagent.txt Result SSH_AUTH_SOCK=/tmp/ssh-JGZOTKftGNqM/agent.23764; export SSH_AUTH_SOCK; SSH_AGENT_PID=23765; export SSH_AGENT_PID; echo Agent pid 23765; $ Exercise 4B: connect to ssh-agent Now, you can always connect to ssh-agent by typing: $ . sshagent.txt Result Agent pid 23765 Exercise 4C: List identities We have not added any identities to ssh-agent yet: $ ssh-add -l The agent has no identities. $ Exercise 4D: add a key to ssh-agent So let\u2019s add our private key: Powershell mac OS/Linux local ssh-add .\\. ssh \\ id_rsa local $ ssh-add .ssh/id_rsa This will require your passphrase: Enter passphrase for .ssh/id_rsa: ************* *ENTER* Identity added: .ssh/id_rsa (<user>@laptop) $ Exercise 4E: List identities Now check whether the identity has been added. $ ssh-add -l Your key hash should appear: 3072 SHA256:uw6W41Pg5MMLKwASHK1G3JhRe+UOUQyt4NHNp2grVTo <user>@laptop (RSA) Exercise 4F: use ssh-agent Since ssh-agent is running and has your private key added to it, you don\u2019t have to re-type your password if you want to acces your keys: $ ssh <hpcuser>@binfservms01.unibe.ch Will result in: ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| Last login: Wed Oct 21 18:47:46 2020 from dhcp-99-231.vpn.unibe.ch [<hpcuser>@binfservms01 ~]$ Exercise 4G: reuse a running ssh-agent from a new terminal Open a new terminal, and check which identities are known by ssh-agent : $ ssh-add -l Powershell Mac OS/Linux Your key hash should appear: 3072 SHA256:uw6W41Pg5MMLKwASHK1G3JhRe+UOUQyt4NHNp2grVTo <user>@laptop (RSA) This means that ssh agent will always run in the background, even if you close your terminal, or restart your comptuer. The added identities are gone: Could not open a connection to your authentication agent. $ However, we have stored the information on the ssh-agent. So we can refer back to it: $ . sshagent.txt Result Agent pid 23765 Exercise 4H: use ssh-agent Now, that we have access to the ssh-agent , we can login without the requirement of a passphrase: $ ssh <hpcuser>@binfservms01.unibe.ch Result ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| Last login: Wed Oct 21 18:47:46 2020 from dhcp-99-231.vpn.unibe.ch [<hpcuser>@binfservms01 ~]$","title":"SSH Tutorial"},{"location":"tutorials/SSH_tutorial/#public-keys-best-practices","text":"","title":"Public keys best practices"},{"location":"tutorials/SSH_tutorial/#material","text":"Download the presentation Before you start his tutorial, you will need to: be on Unibe or UniFR Network (maybe start VPN ) have a \u201cIBU cluster\u201d account (have you received the \u201cWelcome to IBU Cluster\u201d email, looking like this: Welcome to IBU Cluster email Dear John we have created an account for you on the IBU computing cluster. Login Name: jdoe Password will be sent in a separate email. \u2026 Note In this following tutorial, code snippets witht the title \u201clocal\u201d are on your local computer; snippets with the tile \u201cremote\u201d are on the remote server.","title":"Material"},{"location":"tutorials/SSH_tutorial/#1-verify-host-public-key","text":"Exercise 1A: check for existing host key Open the default terminal and make sure you are in your home directory. Then execute: For windows users If you are on windows use PowerShell for these exercises. So not WSL2 or MobaXterm. The reason for this is that it complicates working with VSCode in later exercises otherwise. Powershell mac OS/Linux local ssh-keygen -F binfservms01 . unibe . ch -l -f .\\. ssh \\ known_hosts local ssh-keygen -F binfservms01.unibe.ch -l -f ~/.ssh/known_hosts Result If you have been connecting to this server before, the output looks like this: # Host binfservms01.unibe.ch found: line 1 binfservms01.unibe.ch ECDSA SHA256:Yz6JYkqIEHYni+EJgEwQIPqlz0IEUBQLHEQVU8nEwSY $ Otherwise your output will be empty, or if you\u2019ve never used ssh before it will look like this: do_known_hosts: hostkeys_foreach failed: No such file or directory Exercise 1B: remove existing host key Because in this tutorial we will learn to verify host keys, we will first remove stored host keys if there are any. You can remove host keys with the following command: Powershell mac OS/Linux local ssh-keygen -R binfservms01 . unibe . ch -f .\\. ssh \\ known_hosts local ssh-keygen -R binfservms01.unibe.ch -f ~/.ssh/known_hosts Result # Host binfservms01.unibe.ch found: line 1 /home/user/.ssh/known_hosts updated. Original contents retained as /home/user/.ssh/known_hosts.old $ If you have never used ssh before you might get an error like this (can be safely ignored): mkstemp: No such file or directory Exercise 1C: check host key Warning Replace <hpcuser> with your Login Name on the server. First, let\u2019s try to login to the server with ssh : $ ssh <hpcuser>@binfservms01.unibe.ch Result The authenticity of host 'binfservms01.unibe.ch (130.92.199.95)' can't be established. ED25519 key fingerprint is SHA256:lvjBlWxYtrwFe509ktewvRFXCX7KfAZiLMlMh2mmbeY. Are you sure you want to continue connecting (yes/no/[fingerprint])? Note It is always wise to check that the fingerprint is correct. You should have got it from another source of information (eg email, website). Verify that the fingerprint is correct, then type yes . This will result in the following output: Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'binfservms01.unibe.ch' (ECDSA) to the list of known hosts. ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| <hpcuser>@binfservms01.unibe.ch's password: Now you can enter your password. Depending on your type of terminal, you might not be able to see the password being typed. Also, copy-pasting might be different compared to what you are used to (e.g. pasting by right-click in PowerShell). Result <hpcuser>@binfservms01.unibe.ch's password: ******* *ENTER* Last login: Wed Oct 21 16:57:39 2020 from dhcp-100-237.vpn.unibe.ch [<hpcuser>@binfservms01 ~]$ Exercise 1D: check content of .ssh directory on the server: We will add the public key to the server at a later stage. Now, check whether you already have a key in the .ssh directory at the server: remote ls .ssh Result If you don\u2019t have a public key at the server (which will be the case for new users), you\u2019ll get a message the .ssh directory does not exist (but we will create one): ls: cannot access .ssh: No such file or directory [<hpcuser>@binfservms01 ~]$ Exercise 1E: create .ssh directory and authorized_keys file on the server: We will create the .ssh directory and its authorized_keys file at the server and ensure it has proper permissions: remote mkdir .ssh touch .ssh/authorized_keys chmod -R go-rwx .ssh ls -la .ssh Result the .ssh directory and authorized_keys file have been created with permissions that only allow you to read and write: total 8 drwx------. 2 <hpcuser> <hpcuser> 29 29 sep 11:14 . drwx------. 8 <hpcuser> <hpcuser> 4096 29 sep 11:14 .. -rw-------. 1 <hpcuser> <hpcuser> 0 29 sep 11:16 authorized_keys [<hpcuser>@binfservms01 ~]$ Warning Logout before moving on to the next exercise. remote exit","title":"1. Verify host public key"},{"location":"tutorials/SSH_tutorial/#2-generate-user-keys-pair","text":"Exercise 2A: use ssh-keygen command to generate a key and encrypt the private key with a passphrase Warning Below should be executed on your local computer Now, we can generate a public and private key on your local computer. Do this with the following command: local ssh-keygen This will prompt to with some questions: Generating public/private rsa key pair. Enter file in which to save the key (/home/<localuser>/.ssh/id_rsa): Usually, the default directory is where you want to store your keys, so hit ENTER . This is followed by a asking for a passphrase. This passphrase will be used to protect your keys locally. It will prevent anyone to access your keys from your local computer: Enter passphrase (empty for no passphrase): After completed this successfully, you will get a message like this: Enter passphrase (empty for no passphrase): *********** *ENTER* Enter same passphrase again: *********** *ENTER* Your identification has been saved in /home/<localuser>/.ssh/id_rsa Your public key has been saved in /home/<localuser>/.ssh/id_rsa.pub The key fingerprint is: SHA256:erkOXJp0loytm7+cbfk7rs9dDzaEtvP6GCmR9Bt7lvg user@laptop The key's randomart image is: +---[RSA 3072]----+ | | | | | . | | + o o . | | o S o = . | | o O . o O . | | B o ..O B .| | * +oo.@ +.| | ooBoo=O=E .| +----[SHA256]-----+ $ Exercise 2B: look at the generated files Now, key information has been stored in your local .ssh directory. Check it by typing: local ls -l .ssh Result Which should result in: -rw------- 1 <localuser> <localuser> 2.6K Okt 21 17:01 id_rsa -rw-r--r-- 1 <localuser> <localuser> 574 Okt 21 17:01 id_rsa.pub -rw-r--r-- 1 <localuser> <localuser> 444 Okt 22 13:56 known_hosts $ Check out the hash of your public key with: Powershell mac OS/Linux local cat .\\. ssh \\ id_rsa . pub local cat .ssh/id_rsa.pub Result ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQD38aCe4ICZZ6kPrxlAYruBNhguvHv5YQ2OPj L5Bvs2lzAtV1JPu+QQV9F5SUE3AJc7jh9yn/Agkrg4pMC9EDObWKTl5lg6ritcGtzQMfXYszEe vMNRv8ukV6nCt6WGfyjK4l61nXiuXxTv1RvGzJxAefdUGYMvMkkZOdkMKGKTxwE/xmyXJVYUPc EJEqGt4TSD3nC2Wg8GSp1L+MDpI5626UEVVafEzuOIbbHBmQMPhB+0MevP+ZsXzD0Dz1sWWI0w GlnU9W9a1gZ+QNiIeWCvKtNuxXFKB98338W3YQqE+dk/YwwSB1jeUHIRTEVSyKaIcr42s2Hg9E 2TEVZhmZM4vFJb8nozL8Hu3ZKAHqG1JR3FE1mqJ8kOHnWZiGNf3pQwUe3cgN7c5bsZPEl8VJGw uDArQSAFik+nmrNgQlcodIHYnzY6DtbOMnZUpWuVO1zfQQkPGBbGfdDuNT2cvxAkM1RkWtnCT5 JdOSn//4njp6aCfg38SopbDn3tfJcJTM= <localuser>@laptop $ And your private key with: Powershell mac OS/Linux local cat .\\. ssh \\ id_rsa local cat . ssh / id_rsa Result -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAFGhqvzt qUP/ybuCOapCXgAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQD38aCe4ICZ Z6kPrxlAYruBNhguvHv5YQ2OPjL5Bvs2lzAtV1JPu+QQV9F5SUE3AJc7jh9yn/Agkrg4pM ... /mOQjFg57Pn4XdswU+/gX3mbMWbZXJxIdUO1OIlCKolSe2dJA2CfYUv0XpCIWLe36Iiczn NqTpG7AzfMNH/Ok9Ojr2pqrQnSI= -----END OPENSSH PRIVATE KEY----- $","title":"2. Generate user keys pair"},{"location":"tutorials/SSH_tutorial/#3-copy-public-key-to-server","text":"Now that we have generated both the public and private key files, we will need to copy the public key to the server, so it can be used for authentication. Exercise 3A: use ssh-copy-id to copy your public key to the server Copy the public key by using the following command: Powershell mac OS/Linux local type $env:USERPROFILE \\. ssh \\ id_rsa . pub | ssh < hpcuser > @binfservms01 . unibe . ch \"cat >> .ssh/authorized_keys\" local ssh-copy-id <hpcuser>@binfservms01.unibe.ch Result /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/home/<localuser>/.ssh/id_rsa.pub\" /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| <hpcuser>@binfservms01.unibe.ch's password: This requires authentication. Therefore type your password: Powershell mac OS/Linux <hpcuser>@binfservms01.unibe.ch's password: *********** *ENTER* <hpcuser>@binfservms01.unibe.ch's password: *********** *ENTER* Number of key(s) added: 1 Now try logging into the machine, with: \"ssh '<hpcuser>@binfservms01.unibe.ch'\" and check to make sure that only the key(s) you wanted were added. Exercise 3B: login to the server without password Now you can login on to the server without having to type the password associated with your server login. However, because you protected your keys with a passphrase, you will need to type the passphrase to access your local keys: local ssh <hpcuser>@binfservms01.unibe.ch Result ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| Enter passphrase for key '/home/<localuser>/.ssh/id_rsa': *********** *ENTER* Last login: Wed Oct 21 18:47:46 2020 from dhcp-99-231.vpn.unibe.ch [<hpcuser>@binfservms01 ~]$ Exercise 3C: check content of .ssh on the server We copied the public key to the server (with the ssh-copy-id command), so it should be on there. It is stored in the ssh folder. Check whether it\u2019s there with: remote ls -l .ssh Now the folder exists and contains a file with authorized keys: -rw-------. 1 <hpcuser> <hpcuser> 988 Oct 21 18:48 authorized_keys [<hpcuser>@binfservms01 ~]$ Check what\u2019s in there: remote cat .ssh/authorized_keys Result It\u2019s your public key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQD38aCe4ICZZ6kPrxlAYruBNhguvHv5YQ2OPj L5Bvs2lzAtV1JPu+QQV9F5SUE3AJc7jh9yn/Agkrg4pMC9EDObWKTl5lg6ritcGtzQMfXYszEe vMNRv8ukV6nCt6WGfyjK4l61nXiuXxTv1RvGzJxAefdUGYMvMkkZOdkMKGKTxwE/xmyXJVYUPc EJEqGt4TSD3nC2Wg8GSp1L+MDpI5626UEVVafEzuOIbbHBmQMPhB+0MevP+ZsXzD0Dz1sWWI0w GlnU9W9a1gZ+QNiIeWCvKtNuxXFKB98338W3YQqE+dk/YwwSB1jeUHIRTEVSyKaIcr42s2Hg9E 2TEVZhmZM4vFJb8nozL8Hu3ZKAHqG1JR3FE1mqJ8kOHnWZiGNf3pQwUe3cgN7c5bsZPEl8VJGw uDArQSAFik+nmrNgQlcodIHYnzY6DtbOMnZUpWuVO1zfQQkPGBbGfdDuNT2cvxAkM1RkWtnCT5 JdOSn//4njp6aCfg38SopbDn3tfJcJTM= <localuser>@laptop [<hpcuser>@binfservms01 ~]$","title":"3. Copy Public Key to server"},{"location":"tutorials/SSH_tutorial/#4-ssh-agent","text":"You probably don\u2019t want to re-type your passphrase every time you are connecting to the server. We will use ssh-agent to enable you to access your keys without having to type your passphrase every time you want to use them. Note This part is platform dependent. Choose here therefore either PowerShell or mac OS/Linux . PowerShell mac OS/Linux Exercise 4A: start ssh-agent Open PowerShell in admin mode (right-click on the PowerShell icon and choose \u201cRun as administrator\u201d). Then type: Get-Service ssh-agent | Set-Service -StartupType Automatic -PassThru | Start-Service To check that the ssh-agent is running: Get-Service -Name ssh-agent | Select-Object Status Result Status ------ Running Exercise 4A: start ssh-agent Write the output of ssh-agent to a file: $ ssh-agent > sshagent.txt $ cat sshagent.txt Result SSH_AUTH_SOCK=/tmp/ssh-JGZOTKftGNqM/agent.23764; export SSH_AUTH_SOCK; SSH_AGENT_PID=23765; export SSH_AGENT_PID; echo Agent pid 23765; $ Exercise 4B: connect to ssh-agent Now, you can always connect to ssh-agent by typing: $ . sshagent.txt Result Agent pid 23765 Exercise 4C: List identities We have not added any identities to ssh-agent yet: $ ssh-add -l The agent has no identities. $ Exercise 4D: add a key to ssh-agent So let\u2019s add our private key: Powershell mac OS/Linux local ssh-add .\\. ssh \\ id_rsa local $ ssh-add .ssh/id_rsa This will require your passphrase: Enter passphrase for .ssh/id_rsa: ************* *ENTER* Identity added: .ssh/id_rsa (<user>@laptop) $ Exercise 4E: List identities Now check whether the identity has been added. $ ssh-add -l Your key hash should appear: 3072 SHA256:uw6W41Pg5MMLKwASHK1G3JhRe+UOUQyt4NHNp2grVTo <user>@laptop (RSA) Exercise 4F: use ssh-agent Since ssh-agent is running and has your private key added to it, you don\u2019t have to re-type your password if you want to acces your keys: $ ssh <hpcuser>@binfservms01.unibe.ch Will result in: ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| Last login: Wed Oct 21 18:47:46 2020 from dhcp-99-231.vpn.unibe.ch [<hpcuser>@binfservms01 ~]$ Exercise 4G: reuse a running ssh-agent from a new terminal Open a new terminal, and check which identities are known by ssh-agent : $ ssh-add -l Powershell Mac OS/Linux Your key hash should appear: 3072 SHA256:uw6W41Pg5MMLKwASHK1G3JhRe+UOUQyt4NHNp2grVTo <user>@laptop (RSA) This means that ssh agent will always run in the background, even if you close your terminal, or restart your comptuer. The added identities are gone: Could not open a connection to your authentication agent. $ However, we have stored the information on the ssh-agent. So we can refer back to it: $ . sshagent.txt Result Agent pid 23765 Exercise 4H: use ssh-agent Now, that we have access to the ssh-agent , we can login without the requirement of a passphrase: $ ssh <hpcuser>@binfservms01.unibe.ch Result ___ ____ _ _ _ _ _ _ |_ _| __ )| | | | | | (_)_ __ _ ___ __ ___| |_ _ ___| |_ ___ _ __ | || _ \\| | | | | | | | '_ \\| | | \\ \\/ / / __| | | | / __| __/ _ \\ '__| | || |_) | |_| | | |___| | | | | |_| |> < | (__| | |_| \\__ \\ || __/ | |___|____/ \\___/ |_____|_|_| |_|\\__,_/_/\\_\\ \\___|_|\\__,_|___/\\__\\___|_| Last login: Wed Oct 21 18:47:46 2020 from dhcp-99-231.vpn.unibe.ch [<hpcuser>@binfservms01 ~]$","title":"4. SSH-Agent"},{"location":"tutorials/getting_started/","text":"To work on the IBU cluster you need some basic skills in the command line, using the secure shell (ssh) and job submission with SLURM. In this section there are some tutorials to get you started, but also some short tutorials on topics. Most of the tutorials are part of the MSc course \u2018HPC and cloud computing\u2019. Click to directly go to: Introduction to containers SSH tutorial SLURM tutorial","title":"Getting started"},{"location":"tutorials/containers/apptainer/","text":"Learning outcomes After having completed this chapter you will be able to: Login to a remote machine with ssh Use apptainer pull to convert an image from dockerhub to the \u2018apptainer image format\u2019 ( .sif ) Execute a apptainer container Explain the difference in default mounting behaviour between docker and apptainer Use apptainer shell to generate an interactive shell inside a .sif image Search and use images with both docker and apptainer from bioconda Material Download the presentation An article on Docker vs Apptainer Using conda and containers with snakemake Exercises Login to remote and start an interactive session Login to the IBU cluster with ssh : ssh [ USERNAME ] @binfservms01.unibe.ch Now you are on the head node, which is not meant for running jobs. Therefore, we need to start an interactive session on a compute node. We do this with srun : srun --time 1 :0:0 --pty bash Apptainer only availabe on compute nodes Note that apptainer is not available on the head node, so you would always need to start an interactive session on a compute node or submit a job to the cluster. More about submitting jobs here . Pulling an image Apptainer can take several image formats (e.g. a docker image), and convert them into it\u2019s own .sif format. Unlike docker this image doesn\u2019t live in a local image cache, but it\u2019s stored as an actual file. Exercise: On the remote server, pull the docker image that has the adjusted default CMD that we have pushed to dockerhub in this exercise ( ubuntu-figlet-df:v3 ) with apptainer pull . The syntax is: apptainer pull docker:// [ USER NAME ] / [ IMAGE NAME ] : [ TAG ] Answer apptainer pull docker:// [ USER NAME ] /ubuntu-figlet:v3 This will result in a file called ubuntu-figlet_v3.sif Note If you weren\u2019t able to push the image in the previous exercises to your docker hub, you can use geertvangeest as username to pull the image. Executing an image These .sif files can be run as standalone executables: ./ubuntu-figlet_v3.sif Note This is shorthand for: apptainer run ubuntu-figlet_v3.sif And you can overwrite the default command like this: apptainer run [ IMAGE NAME ] .sif [ COMMAND ] Note In this case, you can also use ./ [ IMAGE NAME ] .sif [ COMMAND ] However, most applications require apptainer run . Especially if you want to provide options like --bind (for mounting directories). Exercise: Run the .sif file without a command, and with a command that runs figlet . Do you get expected output? Do the same for the python or R image you\u2019ve created in the previous chapter. Entrypoint and apptainer The daterange image has an entrypoint set, and apptainer run does not overwrite it. In order to ignore both the entrypoint and cmd use apptainer exec . Answer Running it without a command ( ./ubuntu-figlet_v3.sif ) should give: __ __ _ _ _ | \\/ |_ _ (_)_ __ ___ __ _ __ _ ___ __ _____ _ __| | _____| | | |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| | | | | | |_| | | | | | | | | (_| | (_| | __/ \\ V V / (_) | | | <\\__ \\_| |_| |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___| \\_/\\_/ \\___/|_| |_|\\_\\___(_) |___/ |___/ Which is the default command that we changed in the Dockerfile . Running with a another figlet command: ./ubuntu-figlet_v3.sif figlet 'Something else' Should give: ____ _ _ _ _ / ___| ___ _ __ ___ ___| |_| |__ (_)_ __ __ _ ___| |___ ___ \\___ \\ / _ \\| '_ ` _ \\ / _ \\ __| '_ \\| | '_ \\ / _` | / _ \\ / __|/ _ \\ ___) | (_) | | | | | | __/ |_| | | | | | | | (_| | | __/ \\__ \\ __/ |____/ \\___/|_| |_| |_|\\___|\\__|_| |_|_|_| |_|\\__, | \\___|_|___/\\___| |___/ R Python Pulling the search_biomart_datasets image: apptainer pull docker:// [ USER NAME ] /search_biomart_datasets:v1 Running it without command: ./search_biomart_datasets_v1.sif Running with a command: ./search_biomart_datasets_v1.sif --pattern sapiens To overwrite both entrypoint and the command: apptainer exec search_biomart_datasets_v1.sif search_biomart_datasets.R --pattern \"(R|r)at\" Pulling the daterange.py image: apptainer pull docker:// [ USER NAME ] /daterange:v1 Running it without command: ./daterange_v1.sif Running with a command: ./daterange_v1.sif --date 20221005 To overwrite both entrypoint and the command: apptainer exec daterange_v1.sif daterange.py --date 20221005 Mounting with Apptainer Apptainer is also different from Docker in the way it handles mounting. By default, Apptainer binds your home directory and a number of paths in the root directory to the container. This results in behaviour that is almost like if you are working on the directory structure of the host. If your directory is not mounted by default It depends on the apptainer settings whether most directories are mounted by default to the container. If your directory is not mounted, you can do that with the --bind option of apptainer exec : apptainer exec --bind /my/dir/to/mount/ [ IMAGE NAME ] .sif [ COMMAND ] Running the command pwd (full name of current working directory) will therefore result in a path on the host machine: ./ubuntu-figlet_v3.sif pwd Exercise: Run the above command. What is the output? How would the output look like if you would run a similar command with Docker? Hint A similar Docker command would look like (run this on your local computer): docker run --rm ubuntu-figlet:v3 pwd Answer The output of ./ubuntu-figlet_v3.sif pwd is the current directory on the host: i.e. /home/username if you have it in your home directory. The output of docker run --rm ubuntu-figlet:v3 pwd (on the local host) would be / , which is the default workdir (root directory) of the container. As we did not mount any host directory, this directory exists only within the container (i.e. separated from the host). Interactive shell If you want to debug or inspect an image, it can be helpful to have a shell inside the container. You can do that with apptainer shell : apptainer shell ubuntu-figlet_v3.sif Note To exit the shell type exit . Exercise: Can you run figlet inside this shell? Answer Yes: Apptainer> figlet test _ _ | |_ ___ ___| |_ | __/ _ \\/ __| __| | || __/\\__ \\ |_ \\__\\___||___/\\__| During the lecture you have learned that apptainer takes over the user privileges of the user on the host. You can get user information with command like whoami , id , groups etc. Exercise: Run the figlet container interactively. Do you have the same user privileges as if you were on the host? How is that with docker ? Answer A command like whoami will result in your username printed at stdout: Apptainer> whoami myusername Apptainer> id uid=1030(myusername) gid=1031(myusername) groups=1031(myusername),1001(condausers) Apptainer> groups myusername condausers With apptainer, you have the same privileges inside the apptainer container as on the host. If you do this in the docker container (based on the same image), you\u2019ll get output like this: root@a3d6e59dc19d:/# whoami root root@a3d6e59dc19d:/# groups root root@a3d6e59dc19d:/# id uid=0(root) gid=0(root) groups=0(root) A bioinformatics example (extra) All bioconda packages also have a pre-built container. Have a look at the bioconda website , and search for fastqc . In the search results, click on the appropriate record (i.e. package \u2018fastqc\u2019). Now, scroll down and find the namespace and tag for the latest fastqc image. Now we can pull it with apptainer like this: apptainer pull docker://quay.io/biocontainers/fastqc:0.11.9--hdfd78af_1 Let\u2019s test the image. Download some sample reads first: mkdir reads cd reads wget https://introduction-containers.s3.eu-central-1.amazonaws.com/ecoli_reads.tar.gz tar -xzvf ecoli_reads.tar.gz rm ecoli_reads.tar.gz Now you can simply run the image as an executable preceding the commands you would like to run within the container. E.g. running fastqc would look like: cd ./fastqc_0.11.9--hdfd78af_1.sif fastqc ./reads/ecoli_*.fastq.gz This will result in html files in the directory ./reads . These are quality reports for the sequence reads. If you\u2019d like to view them, you can download them with scp or e.g. FileZilla , and view them with your local browser.","title":"Apptainer"},{"location":"tutorials/containers/apptainer/#learning-outcomes","text":"After having completed this chapter you will be able to: Login to a remote machine with ssh Use apptainer pull to convert an image from dockerhub to the \u2018apptainer image format\u2019 ( .sif ) Execute a apptainer container Explain the difference in default mounting behaviour between docker and apptainer Use apptainer shell to generate an interactive shell inside a .sif image Search and use images with both docker and apptainer from bioconda","title":"Learning outcomes"},{"location":"tutorials/containers/apptainer/#material","text":"Download the presentation An article on Docker vs Apptainer Using conda and containers with snakemake","title":"Material"},{"location":"tutorials/containers/apptainer/#exercises","text":"","title":"Exercises"},{"location":"tutorials/containers/apptainer/#login-to-remote-and-start-an-interactive-session","text":"Login to the IBU cluster with ssh : ssh [ USERNAME ] @binfservms01.unibe.ch Now you are on the head node, which is not meant for running jobs. Therefore, we need to start an interactive session on a compute node. We do this with srun : srun --time 1 :0:0 --pty bash Apptainer only availabe on compute nodes Note that apptainer is not available on the head node, so you would always need to start an interactive session on a compute node or submit a job to the cluster. More about submitting jobs here .","title":"Login to remote and start an interactive session"},{"location":"tutorials/containers/apptainer/#pulling-an-image","text":"Apptainer can take several image formats (e.g. a docker image), and convert them into it\u2019s own .sif format. Unlike docker this image doesn\u2019t live in a local image cache, but it\u2019s stored as an actual file. Exercise: On the remote server, pull the docker image that has the adjusted default CMD that we have pushed to dockerhub in this exercise ( ubuntu-figlet-df:v3 ) with apptainer pull . The syntax is: apptainer pull docker:// [ USER NAME ] / [ IMAGE NAME ] : [ TAG ] Answer apptainer pull docker:// [ USER NAME ] /ubuntu-figlet:v3 This will result in a file called ubuntu-figlet_v3.sif Note If you weren\u2019t able to push the image in the previous exercises to your docker hub, you can use geertvangeest as username to pull the image.","title":"Pulling an image"},{"location":"tutorials/containers/apptainer/#executing-an-image","text":"These .sif files can be run as standalone executables: ./ubuntu-figlet_v3.sif Note This is shorthand for: apptainer run ubuntu-figlet_v3.sif And you can overwrite the default command like this: apptainer run [ IMAGE NAME ] .sif [ COMMAND ] Note In this case, you can also use ./ [ IMAGE NAME ] .sif [ COMMAND ] However, most applications require apptainer run . Especially if you want to provide options like --bind (for mounting directories). Exercise: Run the .sif file without a command, and with a command that runs figlet . Do you get expected output? Do the same for the python or R image you\u2019ve created in the previous chapter. Entrypoint and apptainer The daterange image has an entrypoint set, and apptainer run does not overwrite it. In order to ignore both the entrypoint and cmd use apptainer exec . Answer Running it without a command ( ./ubuntu-figlet_v3.sif ) should give: __ __ _ _ _ | \\/ |_ _ (_)_ __ ___ __ _ __ _ ___ __ _____ _ __| | _____| | | |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| | | | | | |_| | | | | | | | | (_| | (_| | __/ \\ V V / (_) | | | <\\__ \\_| |_| |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___| \\_/\\_/ \\___/|_| |_|\\_\\___(_) |___/ |___/ Which is the default command that we changed in the Dockerfile . Running with a another figlet command: ./ubuntu-figlet_v3.sif figlet 'Something else' Should give: ____ _ _ _ _ / ___| ___ _ __ ___ ___| |_| |__ (_)_ __ __ _ ___| |___ ___ \\___ \\ / _ \\| '_ ` _ \\ / _ \\ __| '_ \\| | '_ \\ / _` | / _ \\ / __|/ _ \\ ___) | (_) | | | | | | __/ |_| | | | | | | | (_| | | __/ \\__ \\ __/ |____/ \\___/|_| |_| |_|\\___|\\__|_| |_|_|_| |_|\\__, | \\___|_|___/\\___| |___/ R Python Pulling the search_biomart_datasets image: apptainer pull docker:// [ USER NAME ] /search_biomart_datasets:v1 Running it without command: ./search_biomart_datasets_v1.sif Running with a command: ./search_biomart_datasets_v1.sif --pattern sapiens To overwrite both entrypoint and the command: apptainer exec search_biomart_datasets_v1.sif search_biomart_datasets.R --pattern \"(R|r)at\" Pulling the daterange.py image: apptainer pull docker:// [ USER NAME ] /daterange:v1 Running it without command: ./daterange_v1.sif Running with a command: ./daterange_v1.sif --date 20221005 To overwrite both entrypoint and the command: apptainer exec daterange_v1.sif daterange.py --date 20221005","title":"Executing an image"},{"location":"tutorials/containers/apptainer/#mounting-with-apptainer","text":"Apptainer is also different from Docker in the way it handles mounting. By default, Apptainer binds your home directory and a number of paths in the root directory to the container. This results in behaviour that is almost like if you are working on the directory structure of the host. If your directory is not mounted by default It depends on the apptainer settings whether most directories are mounted by default to the container. If your directory is not mounted, you can do that with the --bind option of apptainer exec : apptainer exec --bind /my/dir/to/mount/ [ IMAGE NAME ] .sif [ COMMAND ] Running the command pwd (full name of current working directory) will therefore result in a path on the host machine: ./ubuntu-figlet_v3.sif pwd Exercise: Run the above command. What is the output? How would the output look like if you would run a similar command with Docker? Hint A similar Docker command would look like (run this on your local computer): docker run --rm ubuntu-figlet:v3 pwd Answer The output of ./ubuntu-figlet_v3.sif pwd is the current directory on the host: i.e. /home/username if you have it in your home directory. The output of docker run --rm ubuntu-figlet:v3 pwd (on the local host) would be / , which is the default workdir (root directory) of the container. As we did not mount any host directory, this directory exists only within the container (i.e. separated from the host).","title":"Mounting with Apptainer"},{"location":"tutorials/containers/apptainer/#interactive-shell","text":"If you want to debug or inspect an image, it can be helpful to have a shell inside the container. You can do that with apptainer shell : apptainer shell ubuntu-figlet_v3.sif Note To exit the shell type exit . Exercise: Can you run figlet inside this shell? Answer Yes: Apptainer> figlet test _ _ | |_ ___ ___| |_ | __/ _ \\/ __| __| | || __/\\__ \\ |_ \\__\\___||___/\\__| During the lecture you have learned that apptainer takes over the user privileges of the user on the host. You can get user information with command like whoami , id , groups etc. Exercise: Run the figlet container interactively. Do you have the same user privileges as if you were on the host? How is that with docker ? Answer A command like whoami will result in your username printed at stdout: Apptainer> whoami myusername Apptainer> id uid=1030(myusername) gid=1031(myusername) groups=1031(myusername),1001(condausers) Apptainer> groups myusername condausers With apptainer, you have the same privileges inside the apptainer container as on the host. If you do this in the docker container (based on the same image), you\u2019ll get output like this: root@a3d6e59dc19d:/# whoami root root@a3d6e59dc19d:/# groups root root@a3d6e59dc19d:/# id uid=0(root) gid=0(root) groups=0(root)","title":"Interactive shell"},{"location":"tutorials/containers/apptainer/#a-bioinformatics-example-extra","text":"All bioconda packages also have a pre-built container. Have a look at the bioconda website , and search for fastqc . In the search results, click on the appropriate record (i.e. package \u2018fastqc\u2019). Now, scroll down and find the namespace and tag for the latest fastqc image. Now we can pull it with apptainer like this: apptainer pull docker://quay.io/biocontainers/fastqc:0.11.9--hdfd78af_1 Let\u2019s test the image. Download some sample reads first: mkdir reads cd reads wget https://introduction-containers.s3.eu-central-1.amazonaws.com/ecoli_reads.tar.gz tar -xzvf ecoli_reads.tar.gz rm ecoli_reads.tar.gz Now you can simply run the image as an executable preceding the commands you would like to run within the container. E.g. running fastqc would look like: cd ./fastqc_0.11.9--hdfd78af_1.sif fastqc ./reads/ecoli_*.fastq.gz This will result in html files in the directory ./reads . These are quality reports for the sequence reads. If you\u2019d like to view them, you can download them with scp or e.g. FileZilla , and view them with your local browser.","title":"A bioinformatics example (extra)"},{"location":"tutorials/containers/dockerfiles/","text":"Learning outcomes After having completed this chapter you will be able to: Build an image based on a dockerfile Use the basic dockerfile syntax Change the default command of an image and validate the change Map ports to a container to display interactive content through a browser Material Official Dockerfile reference Ten simple rules for writing dockerfiles Exercises To make your images shareable and adjustable, it\u2019s good practice to work with a Dockerfile . This is a script with a set of instructions to build your image from an existing image. Basic Dockerfile You can generate an image from a Dockerfile using the command docker build . A Dockerfile has its own syntax for giving instructions. Luckily, they are rather simple. The script always contains a line starting with FROM that takes the image name from which the new image will be built. After that you usually want to run some commands to e.g. configure and/or install software. The instruction to run these commands during building starts with RUN . In our figlet example that would be: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet On writing reproducible Dockerfiles At the FROM statement in the the above Dockerfile you see that we have added a specific tag to the image (i.e. jammy-20230308 ). We could also have written: FROM ubuntu RUN apt-get update RUN apt-get install figlet This will automatically pull the image with the tag latest . However, if the maintainer of the ubuntu images decides to tag another ubuntu version as latest , rebuilding with the above Dockerfile will not give you the same result. Therefore it\u2019s always good practice to add the (stable) tag to the image in a Dockerfile . More rules on making your Dockerfiles more reproducible here . Exercise: Create a file on your computer called Dockerfile , and paste the above instruction lines in that file. Make the directory containing the Dockerfile your current directory. Build a new image based on that Dockerfile with: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build . docker build --platform amd64 . If using an Apple M1 chip (newer Macs) If you are using a computer with an Apple M1 chip, you have the less common ARM system architecture, which can limit transferability of images to (more common) x86_64/AMD64 machines. When building images on a Mac with an M1 chip (especially if you have sharing in mind), it\u2019s best to specify the --platform amd64 flag. The argument of docker build The command docker build takes a directory as input (providing . means the current directory). This directory should contain the Dockerfile , but it can also contain more of the build context, e.g. (python, R, shell) scripts that are required to build the image. What has happened? What is the name of the build image? Answer A new image was created based on the Dockerfile . You can check it with: docker image ls , which gives something like: REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> 92c980b09aad 7 seconds ago 101MB ubuntu-figlet latest e08b999c7978 About an hour ago 101MB ubuntu latest f63181f19b2f 30 hours ago 72.9MB It has created an image without a name or tag. That\u2019s a bit inconvenient. Exercise: Build a new image with a specific name. You can do that with adding the option -t to docker build . Before that, remove the nameless image. Hint An image without a name is usually a \u201cdangling image\u201d. You can remove those with docker image prune . Answer Remove the nameless image with docker image prune . After that, rebuild an image with a name: x86_64 / AMD64 ARM (MacOS M1 chip) docker build -t ubuntu-figlet:v2 . docker build --platform amd64 -t ubuntu-figlet:v2 . Using CMD As you might remember the second positional argument of docker run is a command (i.e. docker run IMAGE [CMD] ). If you leave it empty, it uses the default command. You can change the default command in the Dockerfile with an instruction starting with CMD . For example: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD figlet My image works! Exercise: Build a new image based on the above Dockerfile . Can you validate the change using docker image inspect ? Can you overwrite this default with docker run ? Answer Copy the new line to your Dockerfile , and build the new image like this: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t ubuntu-figlet:v3 . docker build --platform amd64 -t ubuntu-figlet:v3 . The command docker inspect ubuntu-figlet:v3 will give: \"Cmd\": [ \"/bin/sh\", \"-c\", \"figlet My image works!\" ] So the default command ( /bin/bash ) has changed to figlet My image works! Running the image (with clean-up ( --rm )): docker run --rm ubuntu-figlet:v3 Will result in: __ __ _ _ _ | \\/ |_ _ (_)_ __ ___ __ _ __ _ ___ __ _____ _ __| | _____| | | |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| | | | | | |_| | | | | | | | | (_| | (_| | __/ \\ V V / (_) | | | <\\__ \\_| |_| |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___| \\_/\\_/ \\___/|_| |_|\\_\\___(_) |___/ |___/ And of course you can overwrite the default command: docker run --rm ubuntu-figlet:v3 figlet another text Resulting in: _ _ _ _ __ _ _ __ ___ | |_| |__ ___ _ __ | |_ _____ _| |_ / _` | '_ \\ / _ \\| __| '_ \\ / _ \\ '__| | __/ _ \\ \\/ / __| | (_| | | | | (_) | |_| | | | __/ | | || __/> <| |_ \\__,_|_| |_|\\___/ \\__|_| |_|\\___|_| \\__\\___/_/\\_\\\\__| Two flavours of CMD You have seen in the output of docker inspect that docker translates the command (i.e. figlet \"my image works!\" ) into this: [\"/bin/sh\", \"-c\", \"figlet 'My image works!'\"] . The notation we used in the Dockerfile is the shell notation while the notation with the square brackets ( [] ) is the exec-notation . You can use both notations in your Dockerfile . Altough the shell notation is more readable, the exec notation is directly used by the image, and therefore less ambiguous. A Dockerfile with shell notation: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD figlet My image works! A Dockerfile with exec notation: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD [ \"/bin/sh\" , \"-c\" , \"figlet My image works!\" ] Exercise: Now push our created image (with a version tag) to docker hub. We will use it later for the apptainer exercises . Answer docker tag ubuntu-figlet:v3 [ USER NAME ] /ubuntu-figlet:v3 docker push [ USER NAME ] /ubuntu-figlet:v3 Build an image for your own script (extra) Often containers are built for a specific purpose. For example, you can use a container to ship all dependencies together with your developed set of scripts/programs. For that you will need to add your scripts to the container. That is quite easily done with the instruction COPY . However, in order to make your container more user-friendly, there are several additional instructions that can come in useful. We will treat the most frequently used ones below. Depending on your preference, either choose R or Python below. R Python In the exercises will use a simple script called search_biomart_datasets.R . You can download it here , or copy-paste it: search_biomart_datasets.R #!/usr/bin/env Rscript library ( biomaRt ) library ( optparse ) option_list <- list ( make_option ( c ( \"--pattern\" ), type = \"character\" , help = \"Search pattern [default = %default]\" , default = \"mouse\" ) ) opt_parser <- OptionParser ( option_list = option_list , description = \"Searches biomaRt ensembl datasets\" ) opt <- parse_args ( opt_parser ) ensembl <- useEnsembl ( biomart = \"ensembl\" ) searchDatasets ( mart = ensembl , pattern = opt $ pattern ) After you have downloaded it, make sure to set the permissions to executable: chmod +x search_biomart_datasets.R It is a relatively simple script that searches for datasets in ensembl with biomaRt based on a pattern that is specified by the user. An example for execution would be: ./search_biomart_datasets.R --pattern \"mouse\" Returning a list with all mouse datasets: dataset description 94 mcaroli_gene_ensembl Ryukyu mouse genes (CAROLI_EIJ_v1.1) 109 mpahari_gene_ensembl Shrew mouse genes (PAHARI_EIJ_v1.1) 111 mspicilegus_gene_ensembl Steppe mouse genes (MUSP714) 112 mspretus_gene_ensembl Algerian mouse genes (SPRET_EiJ_v1) 149 pmbairdii_gene_ensembl Northern American deer mouse genes (HU_Pman_2.1) version 94 CAROLI_EIJ_v1.1 109 PAHARI_EIJ_v1.1 111 MUSP714 112 SPRET_EiJ_v1 149 HU_Pman_2.1 From the script you can see it has two dependencies: biomaRt and optparse . If we want to run it inside a container, we would have to install these. We do this in the Dockerfile below. We give the the following instructions: use the R base container version 4.2.3 install the package optparse from CRAN ( r-cran-optparse ) and biomaRt from Bioconductor ( r-bioc-biomart ) with apt-get . copy the script search_biomart_datasets.R to /opt inside the container: FROM r-base:4.2.3 RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart COPY search_biomart_datasets.R /opt Note In order to use COPY , the file that needs to be copied needs to be in the same directory as the Dockerfile or one of its subdirectories. R image stack The most used R image stack is from the rocker project . It contains many different base images (e.g. with shiny, Rstudio, tidyverse etc.). It depends on the type of image whether installations with apt-get is possible. To understand more about how to install R packages in different containers, check it this cheat sheet , or visit rocker-project.org . Exercise: Download the search_biomart_datasets.R and build the image with docker build . After that, execute the script inside the container. Hint Make an interactive session with the options -i and -t and use /bin/bash as the command. Answer Build the container: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t search_biomart_datasets . docker build --platform amd64 -t search_biomart_datasets . Run the container: docker run -it --rm search_biomart_datasets /bin/bash Inside the container we look up the script: cd /opt ls This should return search_biomart_datasets.R . Now you can execute it from inside the container: ./search_biomart_datasets.R --pattern sapiens That\u2019s kind of nice. We can ship our R script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add /opt to the global $PATH variable with ENV . The $PATH variable The path variable is a special variable that consists of a list of path seperated by colons ( : ). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia . FROM r-base:4.2.3 RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart COPY search_biomart_datasets.R /opt ENV PATH = /opt: $PATH Note The ENV instruction can be used to set any variable. Exercise : Rebuild the image and start an interactive bash session inside the new image. Is the path variable updated? (i.e. can we execute search_biomart_datasets.R from anywhere?) Answer After re-building we start an interactive session: docker run -it --rm search_biomart_datasets /bin/bash The path is upated, /opt is appended to the beginning of the variable: echo $PATH returns: /opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin Now you can try to execute it from the root directory (or any other): search_biomart_datasets.R --pattern \"(C|c)ow\" Instead of starting an interactive session with /bin/bash we can now more easily run the script non-interactively: docker run --rm search_biomart_datasets search_biomart_datasets.R --pattern \"(R|r)at\" Now it will directly print the output of search_biomart_datasets.R to stdout. In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use ENTRYPOINT . ENTRYPOINT is similar to CMD , but has two important differences: ENTRYPOINT can not be overwritten by the positional arguments (i.e. docker run image [CMD] ), but has to be overwritten by --entrypoint . The positional arguments (or CMD ) are pasted to the ENTRYPOINT command. This means that you can use ENTRYPOINT as the executable and the positional arguments (or CMD ) as the options. Let\u2019s try it out: FROM r-base:4.2.3 RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart COPY search_biomart_datasets.R /opt ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"search_biomart_datasets.R\" ] # default option (if positional arguments are not specified) CMD [ \"--pattern\" , \"mouse\" ] Exercise : Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different pattern to --pattern . How do the commands look? Answer Just running the container non-interactively would be: docker run --rm search_biomart_datasets Passing a different argument (i.e. overwriting CMD ) would be: docker run --rm search_biomart_datasets --pattern \"sapiens\" Here, the container behaves as the executable itself to which you can pass arguments. Most containerized applications need multiple build steps. Often, you want to perform these steps and executions in a specific directory. Therefore, it can be in convenient to specify a working directory. You can do that with WORKDIR . This instruction will set the default directory for all other instructions (like RUN , COPY etc.). It will also change the directory in which you will land if you run the container interactively. FROM r-base:4.2.3 RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart WORKDIR /opt COPY search_biomart_datasets.R . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"search_biomart_datasets.R\" ] # default option (if positional arguments are not specified) CMD [ \"--pattern\" , \"mouse\" ] Exercise : build the image, and start the container interactively. Has the default directory changed? After that, push the image to dockerhub, so we can use it later with the apptainer exercises. Note You can overwrite ENTRYPOINT with --entrypoint as an argument to docker run . Answer Running the container interactively would be: docker run -it --rm --entrypoint /bin/bash search_biomart_datasets Which should result in a terminal looking something like this: root@9a27da455fb1:/opt# Meaning that indeed the default directory has changed to /opt Pushing it to dockerhub: docker tag search_biomart_datasets [ USER NAME ] /search_biomart_datasets:v1 docker push [ USER NAME ] /search_biomart_datasets:v1 Get information on your image with docker inspect (extra) We have used docker inspect already in the previous chapter to find the default Cmd of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the Config record from the output of docker inspect . For our image this looks like: \"Config\" : { \"Hostname\" : \"\" , \"Domainname\" : \"\" , \"User\" : \"\" , \"AttachStdin\" : false , \"AttachStdout\" : false , \"AttachStderr\" : false , \"Tty\" : false , \"OpenStdin\" : false , \"StdinOnce\" : false , \"Env\" : [ \"PATH=/opt:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" , \"LC_ALL=en_US.UTF-8\" , \"LANG=en_US.UTF-8\" , \"R_BASE_VERSION=4.2.3\" ], \"Cmd\" : [ \"--pattern\" , \"mouse\" ], \"ArgsEscaped\" : true , \"Image\" : \"\" , \"Volumes\" : null , \"WorkingDir\" : \"/opt\" , \"Entrypoint\" : [ \"search_biomart_datasets.R\" ], \"OnBuild\" : null , \"Labels\" : { \"org.opencontainers.image.authors\" : \"Dirk Eddelbuettel <edd@debian.org>\" , \"org.opencontainers.image.licenses\" : \"GPL-2.0-or-later\" , \"org.opencontainers.image.source\" : \"https://github.com/rocker-org/rocker\" , \"org.opencontainers.image.vendor\" : \"Rocker Project\" } } Adding metadata to your image (extra) You can annotate your Dockerfile and the image by using the instruction LABEL . You can give it any key and value with <key>=<value> . However, it is recommended to use the Open Container Initiative (OCI) keys . Exercise : Annotate our Dockerfile with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with docker inspect . Note You can type LABEL for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.: LABEL keyx = \"valuex\" keyy = \"valuey\" Answer The Dockerfile would look like: FROM r-base:4.2.3 LABEL org.opencontainers.image.created = \"2023-04-12\" \\ org.opencontainers.image.authors = \"Geert van Geest\" \\ org.opencontainers.image.description = \"Container to search ensembl datasets with biomart\" RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart WORKDIR /opt COPY search_biomart_datasets.R . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"search_biomart_datasets.R\" ] # default option (if positional arguments are not specified) CMD [ \"--pattern\" , \"mouse\" ] The Config record in the output of docker inspect was updated with: \"Labels\" : { \"org.opencontainers.image.authors\" : \"Geert van Geest\" , \"org.opencontainers.image.created\" : \"2023-04-12\" , \"org.opencontainers.image.description\" : \"Container to search ensembl datasets with biomart\" , \"org.opencontainers.image.licenses\" : \"GPL-2.0-or-later\" , \"org.opencontainers.image.source\" : \"https://github.com/rocker-org/rocker\" , \"org.opencontainers.image.vendor\" : \"Rocker Project\" } Building an image with a browser interface (extra) In this exercise, we will use a different base image ( rocker/rstudio:4 ), and we\u2019ll install the same packages. Rstudio server is a nice browser interface that you can use for a.o. programming in R. With the image we are creating we will be able to run Rstudio server inside a container. Check out the Dockerfile : FROM rocker/rstudio:4 RUN apt-get update && \\ apt-get install -y libz-dev RUN install2.r \\ optparse \\ BiocManager RUN R -q -e 'BiocManager::install(\"biomaRt\")' This will create an image from the existing rstudio image. It will also install libz-dev with apt-get , BiocManager with install2.r and biomaRt with an R command. Despiste we\u2019re installing the same packages, the installation steps need to be different from the r-base image. This is because in the rocker/rstudio images R is installed from source, and therefore you can\u2019t install packages with apt-get . More information on how to install R packages in R containers in this cheat sheet , or visit rocker-project.org . Exercise: Build an image based on this Dockerfile and give it a meaningful name. Answer x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t rstudio-server . docker build --platform amd64 -t rstudio-server . You can now run a container from the image. However, you will have to tell docker where to publish port 8787 from the docker container with -p [HOSTPORT:CONTAINERPORT] . We choose to publish it to the same port number: docker run --rm -it -p 8787 :8787 rstudio-server Networking More info on docker container networking here By running the above command, a container will be started exposing rstudio server at port 8787 at localhost. You can approach the instance of jupyterhub by typing localhost:8787 in your browser. You will be asked for a password. You can find this password in the terminal from which you have started the container. We can make this even more interesting by mounting a local directory to the container running the jupyter-lab image: docker run \\ -it \\ --rm \\ -p 8787 :8787 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /home/rstudio/working_dir \\ rstudio-server By doing this you have a completely isolated and shareable R environment running Rstudio server, but with your local files available to it. Pretty neat right? In the exercises will use a simple script called daterange.py . You can download it here . Or copy-paste it from here: daterange.py #!/usr/bin/env python3 import pandas as pd import argparse parser = argparse . ArgumentParser ( description = \"Get a daterange\" ) parser . add_argument ( '-d' , '--date' , type = str , required = True , help = 'Date. Format: [YYYYMMDD]' ) args = parser . parse_args () dates = pd . date_range ( args . date , periods = 7 ) for d in dates : print ( d ) After you have downloaded it, make sure to set the permissions to executable: chmod +x daterange.py Have a look at daterange.py . It is a simple script that uses pandas and argparse . It takes a date (in the format YYYYMMDD ) as provided by the option --date , and returns a list of all dates in the week starting from that date. An example for execution would be: ./daterange.py --date 20220226 Giving a list of dates starting from 26-FEB-2022: 2022-02-26 00:00:00 2022-02-27 00:00:00 2022-02-28 00:00:00 2022-03-01 00:00:00 2022-03-02 00:00:00 2022-03-03 00:00:00 2022-03-04 00:00:00 From the script, you can see it has the dependecy pandas , which is not a built-in module . In the Dockerfile below we give the instruction to install pandas with pip and copy daterange.py to /opt inside the container: FROM python:3.9.16 RUN pip install pandas COPY daterange.py /opt Note In order to use COPY , the file that needs to be copied needs to be in the same directory as the Dockerfile or one of its subdirectories. Exercise: Download the daterange.py and build the image with docker build . After that, execute the script inside the container. Hint Make an interactive session with the options -i and -t and use /bin/bash as the command. Answer Build the container: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t daterange . docker build --platform amd64 -t daterange . Run the container: docker run -it --rm daterange /bin/bash Inside the container we look up the script: cd /opt ls This should return daterange.py . Now you can execute it from inside the container: ./daterange.py --date 20220226 That\u2019s kind of nice. We can ship our python script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add /opt to the global $PATH variable with ENV . The $PATH variable The path variable is a special variable that consists of a list of path seperated by colons ( : ). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia . FROM python:3.9.16 RUN pip install pandas COPY daterange.py /opt ENV PATH = /opt: $PATH Note The ENV instruction can be used to set any variable. Exercise : Start an interactive bash session inside the new container. Is the path variable updated? (i.e. can we execute daterange.py from anywhere?) Answer After re-building we start an interactive session: docker run -it --rm daterange /bin/bash The path is upated, /opt is appended to the beginning of the variable: echo $PATH returns: /opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin Now you can try to execute it from the root directory (or any other): daterange.py --date 20220226 Instead of starting an interactive session with /bin/bash we can now more easily run the script non-interactively: docker run --rm daterange daterange.py --date 20220226 Now it will directly print the output of daterange.py to stdout. In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use ENTRYPOINT . ENTRYPOINT is similar to CMD , but has two important differences: ENTRYPOINT can not be overwritten by the positional arguments (i.e. docker run image [CMD] ), but has to be overwritten by --entrypoint . The positional arguments (or CMD ) are pasted to the ENTRYPOINT command. This means that you can use ENTRYPOINT as the executable and the positional arguments (or CMD ) as the options. Let\u2019s try it out: FROM python:3.9.16 RUN pip install pandas COPY daterange.py /opt ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"daterange.py\" ] # default option (if positional arguments are not specified) CMD [ \"--date\" , \"20220226\" ] Exercise : Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different date to --date . How do the commands look? Answer Just running the container non-interactively would be: docker run --rm daterange Passing a different argument (i.e. overwriting CMD ) would be: docker run --rm daterange --date 20210330 Here, the container behaves as the executable itself to which you can pass arguments. Most containerized applications need multiple build steps. Often, you want to perform these steps and executions in a specific directory. Therefore, it can be in convenient to specify a working directory. You can do that with WORKDIR . This instruction will set the default directory for all other instructions (like RUN , COPY etc.). It will also change the directory in which you will land if you run the container interactively. FROM python:3.9.16 RUN pip install pandas WORKDIR /opt # we don't have to specify /opt as target dir but the current dir COPY daterange.py . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"daterange.py\" ] # default option (if positional arguments are not specified) CMD [ \"--date\" , \"20220226\" ] Exercise : build the image, and start the container interactively. Has the default directory changed? After that, push the image to dockerhub, so we can use it later with the apptainer exercises. Note You can overwrite ENTRYPOINT with --entrypoint as an argument to docker run . Answer Running the container interactively would be: docker run -it --rm --entrypoint /bin/bash daterange Which should result in a terminal looking something like this: root@9a27da455fb1:/opt# Meaning that indeed the default directory has changed to /opt Pushing it to dockerhub: docker tag daterage [ USER NAME ] /daterange:v1 docker push [ USER NAME ] /daterange:v1 Get information on your image with docker inspect (extra) We have used docker inspect already in the previous chapter to find the default Cmd of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the Config record from the output of docker inspect . For our image this looks like: \"Config\" : { \"Hostname\" : \"\" , \"Domainname\" : \"\" , \"User\" : \"\" , \"AttachStdin\" : false , \"AttachStdout\" : false , \"AttachStderr\" : false , \"Tty\" : false , \"OpenStdin\" : false , \"StdinOnce\" : false , \"Env\" : [ \"PATH=/opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" , \"LANG=C.UTF-8\" , \"GPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568\" , \"PYTHON_VERSION=3.9.4\" , \"PYTHON_PIP_VERSION=21.1.1\" , \"PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/1954f15b3f102ace496a34a013ea76b061535bd2/public/get-pip.py\" , \"PYTHON_GET_PIP_SHA256=f499d76e0149a673fb8246d88e116db589afbd291739bd84f2cd9a7bca7b6993\" ], \"Cmd\" : [ \"--date\" , \"20220226\" ], \"ArgsEscaped\" : true , \"Image\" : \"\" , \"Volumes\" : null , \"WorkingDir\" : \"/opt\" , \"Entrypoint\" : [ \"daterange.py\" ], \"OnBuild\" : null , \"Labels\" : null } , Adding metadata to your image (extra) You can annotate your Dockerfile and the image by using the instruction LABEL . You can give it any key and value with <key>=<value> . However, it is recommended to use the Open Container Initiative (OCI) keys . Exercise : Annotate our Dockerfile with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with docker inspect . Note You can type LABEL for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.: LABEL keyx = \"valuex\" keyy = \"valuey\" Answer The Dockerfile would look like: FROM python:3.9.16 LABEL org.opencontainers.image.created = \"2022-04-12\" \\ org.opencontainers.image.authors = \"Geert van Geest\" \\ org.opencontainers.image.description = \"Great container for getting all dates in a week! \\ You will never use a calender again\" RUN pip install pandas WORKDIR /opt COPY daterange.py . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"daterange.py\" ] # default option (if positional arguments are not specified) CMD [ \"--date\" , \"20220226\" ] The Config record in the output of docker inspect was updated with: \"Labels\" : { \"org.opencontainers.image.authors\" : \"Geert van Geest\" , \"org.opencontainers.image.created\" : \"2022-04-12\" , \"org.opencontainers.image.description\" : \"Great container for getting all dates in a week! You will never use a calender again\" } Building an image with a browser interface (extra) In this exercise, we will use a different base image from the jupyter docker image stack . JupyterLab is a nice browser interface that you can use for a.o. programming in python. With the image we are creating we will be able to run jupyter lab inside a container. Check out the Dockerfile : FROM jupyter/base-notebook:python-3.9 RUN pip install pandas This will create an image from the existing python image. It will also install jupyterlab with pip . As a default command it starts a jupyter notebook at port 8888. Ports We have specified here that jupyter lab should use port 8888. However, this inside the container. We can not connect to it yet with our browser. Exercise: Build an image based on this Dockerfile and give it a meaningful name. Answer x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t jupyter-lab . docker build --platform amd64 -t jupyter-lab . You can now run a container from the image. However, you will have to tell docker where to publish port 8888 from the docker container with -p [HOSTPORT:CONTAINERPORT] . We choose to publish it to the same port number: docker run --rm -it -p 8888 :8888 jupyter-lab Networking More info on docker container networking here By running the above command, a container will be started exposing jupyterhub at port 8888 at localhost. You can approach the instance of jupyterhub by typing localhost:8888 in your browser. You will be asked for a token. You can find this token in the terminal from which you have started the container. We can make this even more interesting by mounting a local directory to the container running the jupyter-lab image: docker run \\ -it \\ --rm \\ -p 8888 :8888 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ jupyter-lab By doing this you have a completely isolated and shareable python environment running jupyter lab, but with your local files available to it. Pretty neat right? Note Jupyter has a wide range of pre-built images available here .","title":"Dockerfiles"},{"location":"tutorials/containers/dockerfiles/#learning-outcomes","text":"After having completed this chapter you will be able to: Build an image based on a dockerfile Use the basic dockerfile syntax Change the default command of an image and validate the change Map ports to a container to display interactive content through a browser","title":"Learning outcomes"},{"location":"tutorials/containers/dockerfiles/#material","text":"Official Dockerfile reference Ten simple rules for writing dockerfiles","title":"Material"},{"location":"tutorials/containers/dockerfiles/#exercises","text":"To make your images shareable and adjustable, it\u2019s good practice to work with a Dockerfile . This is a script with a set of instructions to build your image from an existing image.","title":"Exercises"},{"location":"tutorials/containers/dockerfiles/#basic-dockerfile","text":"You can generate an image from a Dockerfile using the command docker build . A Dockerfile has its own syntax for giving instructions. Luckily, they are rather simple. The script always contains a line starting with FROM that takes the image name from which the new image will be built. After that you usually want to run some commands to e.g. configure and/or install software. The instruction to run these commands during building starts with RUN . In our figlet example that would be: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet On writing reproducible Dockerfiles At the FROM statement in the the above Dockerfile you see that we have added a specific tag to the image (i.e. jammy-20230308 ). We could also have written: FROM ubuntu RUN apt-get update RUN apt-get install figlet This will automatically pull the image with the tag latest . However, if the maintainer of the ubuntu images decides to tag another ubuntu version as latest , rebuilding with the above Dockerfile will not give you the same result. Therefore it\u2019s always good practice to add the (stable) tag to the image in a Dockerfile . More rules on making your Dockerfiles more reproducible here . Exercise: Create a file on your computer called Dockerfile , and paste the above instruction lines in that file. Make the directory containing the Dockerfile your current directory. Build a new image based on that Dockerfile with: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build . docker build --platform amd64 . If using an Apple M1 chip (newer Macs) If you are using a computer with an Apple M1 chip, you have the less common ARM system architecture, which can limit transferability of images to (more common) x86_64/AMD64 machines. When building images on a Mac with an M1 chip (especially if you have sharing in mind), it\u2019s best to specify the --platform amd64 flag. The argument of docker build The command docker build takes a directory as input (providing . means the current directory). This directory should contain the Dockerfile , but it can also contain more of the build context, e.g. (python, R, shell) scripts that are required to build the image. What has happened? What is the name of the build image? Answer A new image was created based on the Dockerfile . You can check it with: docker image ls , which gives something like: REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> 92c980b09aad 7 seconds ago 101MB ubuntu-figlet latest e08b999c7978 About an hour ago 101MB ubuntu latest f63181f19b2f 30 hours ago 72.9MB It has created an image without a name or tag. That\u2019s a bit inconvenient. Exercise: Build a new image with a specific name. You can do that with adding the option -t to docker build . Before that, remove the nameless image. Hint An image without a name is usually a \u201cdangling image\u201d. You can remove those with docker image prune . Answer Remove the nameless image with docker image prune . After that, rebuild an image with a name: x86_64 / AMD64 ARM (MacOS M1 chip) docker build -t ubuntu-figlet:v2 . docker build --platform amd64 -t ubuntu-figlet:v2 .","title":"Basic Dockerfile"},{"location":"tutorials/containers/dockerfiles/#using-cmd","text":"As you might remember the second positional argument of docker run is a command (i.e. docker run IMAGE [CMD] ). If you leave it empty, it uses the default command. You can change the default command in the Dockerfile with an instruction starting with CMD . For example: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD figlet My image works! Exercise: Build a new image based on the above Dockerfile . Can you validate the change using docker image inspect ? Can you overwrite this default with docker run ? Answer Copy the new line to your Dockerfile , and build the new image like this: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t ubuntu-figlet:v3 . docker build --platform amd64 -t ubuntu-figlet:v3 . The command docker inspect ubuntu-figlet:v3 will give: \"Cmd\": [ \"/bin/sh\", \"-c\", \"figlet My image works!\" ] So the default command ( /bin/bash ) has changed to figlet My image works! Running the image (with clean-up ( --rm )): docker run --rm ubuntu-figlet:v3 Will result in: __ __ _ _ _ | \\/ |_ _ (_)_ __ ___ __ _ __ _ ___ __ _____ _ __| | _____| | | |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| | | | | | |_| | | | | | | | | (_| | (_| | __/ \\ V V / (_) | | | <\\__ \\_| |_| |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___| \\_/\\_/ \\___/|_| |_|\\_\\___(_) |___/ |___/ And of course you can overwrite the default command: docker run --rm ubuntu-figlet:v3 figlet another text Resulting in: _ _ _ _ __ _ _ __ ___ | |_| |__ ___ _ __ | |_ _____ _| |_ / _` | '_ \\ / _ \\| __| '_ \\ / _ \\ '__| | __/ _ \\ \\/ / __| | (_| | | | | (_) | |_| | | | __/ | | || __/> <| |_ \\__,_|_| |_|\\___/ \\__|_| |_|\\___|_| \\__\\___/_/\\_\\\\__| Two flavours of CMD You have seen in the output of docker inspect that docker translates the command (i.e. figlet \"my image works!\" ) into this: [\"/bin/sh\", \"-c\", \"figlet 'My image works!'\"] . The notation we used in the Dockerfile is the shell notation while the notation with the square brackets ( [] ) is the exec-notation . You can use both notations in your Dockerfile . Altough the shell notation is more readable, the exec notation is directly used by the image, and therefore less ambiguous. A Dockerfile with shell notation: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD figlet My image works! A Dockerfile with exec notation: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD [ \"/bin/sh\" , \"-c\" , \"figlet My image works!\" ] Exercise: Now push our created image (with a version tag) to docker hub. We will use it later for the apptainer exercises . Answer docker tag ubuntu-figlet:v3 [ USER NAME ] /ubuntu-figlet:v3 docker push [ USER NAME ] /ubuntu-figlet:v3","title":"Using CMD"},{"location":"tutorials/containers/dockerfiles/#build-an-image-for-your-own-script-extra","text":"Often containers are built for a specific purpose. For example, you can use a container to ship all dependencies together with your developed set of scripts/programs. For that you will need to add your scripts to the container. That is quite easily done with the instruction COPY . However, in order to make your container more user-friendly, there are several additional instructions that can come in useful. We will treat the most frequently used ones below. Depending on your preference, either choose R or Python below. R Python In the exercises will use a simple script called search_biomart_datasets.R . You can download it here , or copy-paste it: search_biomart_datasets.R #!/usr/bin/env Rscript library ( biomaRt ) library ( optparse ) option_list <- list ( make_option ( c ( \"--pattern\" ), type = \"character\" , help = \"Search pattern [default = %default]\" , default = \"mouse\" ) ) opt_parser <- OptionParser ( option_list = option_list , description = \"Searches biomaRt ensembl datasets\" ) opt <- parse_args ( opt_parser ) ensembl <- useEnsembl ( biomart = \"ensembl\" ) searchDatasets ( mart = ensembl , pattern = opt $ pattern ) After you have downloaded it, make sure to set the permissions to executable: chmod +x search_biomart_datasets.R It is a relatively simple script that searches for datasets in ensembl with biomaRt based on a pattern that is specified by the user. An example for execution would be: ./search_biomart_datasets.R --pattern \"mouse\" Returning a list with all mouse datasets: dataset description 94 mcaroli_gene_ensembl Ryukyu mouse genes (CAROLI_EIJ_v1.1) 109 mpahari_gene_ensembl Shrew mouse genes (PAHARI_EIJ_v1.1) 111 mspicilegus_gene_ensembl Steppe mouse genes (MUSP714) 112 mspretus_gene_ensembl Algerian mouse genes (SPRET_EiJ_v1) 149 pmbairdii_gene_ensembl Northern American deer mouse genes (HU_Pman_2.1) version 94 CAROLI_EIJ_v1.1 109 PAHARI_EIJ_v1.1 111 MUSP714 112 SPRET_EiJ_v1 149 HU_Pman_2.1 From the script you can see it has two dependencies: biomaRt and optparse . If we want to run it inside a container, we would have to install these. We do this in the Dockerfile below. We give the the following instructions: use the R base container version 4.2.3 install the package optparse from CRAN ( r-cran-optparse ) and biomaRt from Bioconductor ( r-bioc-biomart ) with apt-get . copy the script search_biomart_datasets.R to /opt inside the container: FROM r-base:4.2.3 RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart COPY search_biomart_datasets.R /opt Note In order to use COPY , the file that needs to be copied needs to be in the same directory as the Dockerfile or one of its subdirectories. R image stack The most used R image stack is from the rocker project . It contains many different base images (e.g. with shiny, Rstudio, tidyverse etc.). It depends on the type of image whether installations with apt-get is possible. To understand more about how to install R packages in different containers, check it this cheat sheet , or visit rocker-project.org . Exercise: Download the search_biomart_datasets.R and build the image with docker build . After that, execute the script inside the container. Hint Make an interactive session with the options -i and -t and use /bin/bash as the command. Answer Build the container: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t search_biomart_datasets . docker build --platform amd64 -t search_biomart_datasets . Run the container: docker run -it --rm search_biomart_datasets /bin/bash Inside the container we look up the script: cd /opt ls This should return search_biomart_datasets.R . Now you can execute it from inside the container: ./search_biomart_datasets.R --pattern sapiens That\u2019s kind of nice. We can ship our R script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add /opt to the global $PATH variable with ENV . The $PATH variable The path variable is a special variable that consists of a list of path seperated by colons ( : ). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia . FROM r-base:4.2.3 RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart COPY search_biomart_datasets.R /opt ENV PATH = /opt: $PATH Note The ENV instruction can be used to set any variable. Exercise : Rebuild the image and start an interactive bash session inside the new image. Is the path variable updated? (i.e. can we execute search_biomart_datasets.R from anywhere?) Answer After re-building we start an interactive session: docker run -it --rm search_biomart_datasets /bin/bash The path is upated, /opt is appended to the beginning of the variable: echo $PATH returns: /opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin Now you can try to execute it from the root directory (or any other): search_biomart_datasets.R --pattern \"(C|c)ow\" Instead of starting an interactive session with /bin/bash we can now more easily run the script non-interactively: docker run --rm search_biomart_datasets search_biomart_datasets.R --pattern \"(R|r)at\" Now it will directly print the output of search_biomart_datasets.R to stdout. In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use ENTRYPOINT . ENTRYPOINT is similar to CMD , but has two important differences: ENTRYPOINT can not be overwritten by the positional arguments (i.e. docker run image [CMD] ), but has to be overwritten by --entrypoint . The positional arguments (or CMD ) are pasted to the ENTRYPOINT command. This means that you can use ENTRYPOINT as the executable and the positional arguments (or CMD ) as the options. Let\u2019s try it out: FROM r-base:4.2.3 RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart COPY search_biomart_datasets.R /opt ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"search_biomart_datasets.R\" ] # default option (if positional arguments are not specified) CMD [ \"--pattern\" , \"mouse\" ] Exercise : Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different pattern to --pattern . How do the commands look? Answer Just running the container non-interactively would be: docker run --rm search_biomart_datasets Passing a different argument (i.e. overwriting CMD ) would be: docker run --rm search_biomart_datasets --pattern \"sapiens\" Here, the container behaves as the executable itself to which you can pass arguments. Most containerized applications need multiple build steps. Often, you want to perform these steps and executions in a specific directory. Therefore, it can be in convenient to specify a working directory. You can do that with WORKDIR . This instruction will set the default directory for all other instructions (like RUN , COPY etc.). It will also change the directory in which you will land if you run the container interactively. FROM r-base:4.2.3 RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart WORKDIR /opt COPY search_biomart_datasets.R . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"search_biomart_datasets.R\" ] # default option (if positional arguments are not specified) CMD [ \"--pattern\" , \"mouse\" ] Exercise : build the image, and start the container interactively. Has the default directory changed? After that, push the image to dockerhub, so we can use it later with the apptainer exercises. Note You can overwrite ENTRYPOINT with --entrypoint as an argument to docker run . Answer Running the container interactively would be: docker run -it --rm --entrypoint /bin/bash search_biomart_datasets Which should result in a terminal looking something like this: root@9a27da455fb1:/opt# Meaning that indeed the default directory has changed to /opt Pushing it to dockerhub: docker tag search_biomart_datasets [ USER NAME ] /search_biomart_datasets:v1 docker push [ USER NAME ] /search_biomart_datasets:v1","title":"Build an image for your own script (extra)"},{"location":"tutorials/containers/dockerfiles/#get-information-on-your-image-with-docker-inspect-extra","text":"We have used docker inspect already in the previous chapter to find the default Cmd of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the Config record from the output of docker inspect . For our image this looks like: \"Config\" : { \"Hostname\" : \"\" , \"Domainname\" : \"\" , \"User\" : \"\" , \"AttachStdin\" : false , \"AttachStdout\" : false , \"AttachStderr\" : false , \"Tty\" : false , \"OpenStdin\" : false , \"StdinOnce\" : false , \"Env\" : [ \"PATH=/opt:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" , \"LC_ALL=en_US.UTF-8\" , \"LANG=en_US.UTF-8\" , \"R_BASE_VERSION=4.2.3\" ], \"Cmd\" : [ \"--pattern\" , \"mouse\" ], \"ArgsEscaped\" : true , \"Image\" : \"\" , \"Volumes\" : null , \"WorkingDir\" : \"/opt\" , \"Entrypoint\" : [ \"search_biomart_datasets.R\" ], \"OnBuild\" : null , \"Labels\" : { \"org.opencontainers.image.authors\" : \"Dirk Eddelbuettel <edd@debian.org>\" , \"org.opencontainers.image.licenses\" : \"GPL-2.0-or-later\" , \"org.opencontainers.image.source\" : \"https://github.com/rocker-org/rocker\" , \"org.opencontainers.image.vendor\" : \"Rocker Project\" } }","title":"Get information on your image with docker inspect (extra)"},{"location":"tutorials/containers/dockerfiles/#adding-metadata-to-your-image-extra","text":"You can annotate your Dockerfile and the image by using the instruction LABEL . You can give it any key and value with <key>=<value> . However, it is recommended to use the Open Container Initiative (OCI) keys . Exercise : Annotate our Dockerfile with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with docker inspect . Note You can type LABEL for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.: LABEL keyx = \"valuex\" keyy = \"valuey\" Answer The Dockerfile would look like: FROM r-base:4.2.3 LABEL org.opencontainers.image.created = \"2023-04-12\" \\ org.opencontainers.image.authors = \"Geert van Geest\" \\ org.opencontainers.image.description = \"Container to search ensembl datasets with biomart\" RUN apt-get update RUN apt-get install -y \\ r-cran-optparse \\ r-bioc-biomart WORKDIR /opt COPY search_biomart_datasets.R . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"search_biomart_datasets.R\" ] # default option (if positional arguments are not specified) CMD [ \"--pattern\" , \"mouse\" ] The Config record in the output of docker inspect was updated with: \"Labels\" : { \"org.opencontainers.image.authors\" : \"Geert van Geest\" , \"org.opencontainers.image.created\" : \"2023-04-12\" , \"org.opencontainers.image.description\" : \"Container to search ensembl datasets with biomart\" , \"org.opencontainers.image.licenses\" : \"GPL-2.0-or-later\" , \"org.opencontainers.image.source\" : \"https://github.com/rocker-org/rocker\" , \"org.opencontainers.image.vendor\" : \"Rocker Project\" }","title":"Adding metadata to your image (extra)"},{"location":"tutorials/containers/dockerfiles/#building-an-image-with-a-browser-interface-extra","text":"In this exercise, we will use a different base image ( rocker/rstudio:4 ), and we\u2019ll install the same packages. Rstudio server is a nice browser interface that you can use for a.o. programming in R. With the image we are creating we will be able to run Rstudio server inside a container. Check out the Dockerfile : FROM rocker/rstudio:4 RUN apt-get update && \\ apt-get install -y libz-dev RUN install2.r \\ optparse \\ BiocManager RUN R -q -e 'BiocManager::install(\"biomaRt\")' This will create an image from the existing rstudio image. It will also install libz-dev with apt-get , BiocManager with install2.r and biomaRt with an R command. Despiste we\u2019re installing the same packages, the installation steps need to be different from the r-base image. This is because in the rocker/rstudio images R is installed from source, and therefore you can\u2019t install packages with apt-get . More information on how to install R packages in R containers in this cheat sheet , or visit rocker-project.org . Exercise: Build an image based on this Dockerfile and give it a meaningful name. Answer x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t rstudio-server . docker build --platform amd64 -t rstudio-server . You can now run a container from the image. However, you will have to tell docker where to publish port 8787 from the docker container with -p [HOSTPORT:CONTAINERPORT] . We choose to publish it to the same port number: docker run --rm -it -p 8787 :8787 rstudio-server Networking More info on docker container networking here By running the above command, a container will be started exposing rstudio server at port 8787 at localhost. You can approach the instance of jupyterhub by typing localhost:8787 in your browser. You will be asked for a password. You can find this password in the terminal from which you have started the container. We can make this even more interesting by mounting a local directory to the container running the jupyter-lab image: docker run \\ -it \\ --rm \\ -p 8787 :8787 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /home/rstudio/working_dir \\ rstudio-server By doing this you have a completely isolated and shareable R environment running Rstudio server, but with your local files available to it. Pretty neat right? In the exercises will use a simple script called daterange.py . You can download it here . Or copy-paste it from here: daterange.py #!/usr/bin/env python3 import pandas as pd import argparse parser = argparse . ArgumentParser ( description = \"Get a daterange\" ) parser . add_argument ( '-d' , '--date' , type = str , required = True , help = 'Date. Format: [YYYYMMDD]' ) args = parser . parse_args () dates = pd . date_range ( args . date , periods = 7 ) for d in dates : print ( d ) After you have downloaded it, make sure to set the permissions to executable: chmod +x daterange.py Have a look at daterange.py . It is a simple script that uses pandas and argparse . It takes a date (in the format YYYYMMDD ) as provided by the option --date , and returns a list of all dates in the week starting from that date. An example for execution would be: ./daterange.py --date 20220226 Giving a list of dates starting from 26-FEB-2022: 2022-02-26 00:00:00 2022-02-27 00:00:00 2022-02-28 00:00:00 2022-03-01 00:00:00 2022-03-02 00:00:00 2022-03-03 00:00:00 2022-03-04 00:00:00 From the script, you can see it has the dependecy pandas , which is not a built-in module . In the Dockerfile below we give the instruction to install pandas with pip and copy daterange.py to /opt inside the container: FROM python:3.9.16 RUN pip install pandas COPY daterange.py /opt Note In order to use COPY , the file that needs to be copied needs to be in the same directory as the Dockerfile or one of its subdirectories. Exercise: Download the daterange.py and build the image with docker build . After that, execute the script inside the container. Hint Make an interactive session with the options -i and -t and use /bin/bash as the command. Answer Build the container: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t daterange . docker build --platform amd64 -t daterange . Run the container: docker run -it --rm daterange /bin/bash Inside the container we look up the script: cd /opt ls This should return daterange.py . Now you can execute it from inside the container: ./daterange.py --date 20220226 That\u2019s kind of nice. We can ship our python script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add /opt to the global $PATH variable with ENV . The $PATH variable The path variable is a special variable that consists of a list of path seperated by colons ( : ). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia . FROM python:3.9.16 RUN pip install pandas COPY daterange.py /opt ENV PATH = /opt: $PATH Note The ENV instruction can be used to set any variable. Exercise : Start an interactive bash session inside the new container. Is the path variable updated? (i.e. can we execute daterange.py from anywhere?) Answer After re-building we start an interactive session: docker run -it --rm daterange /bin/bash The path is upated, /opt is appended to the beginning of the variable: echo $PATH returns: /opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin Now you can try to execute it from the root directory (or any other): daterange.py --date 20220226 Instead of starting an interactive session with /bin/bash we can now more easily run the script non-interactively: docker run --rm daterange daterange.py --date 20220226 Now it will directly print the output of daterange.py to stdout. In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use ENTRYPOINT . ENTRYPOINT is similar to CMD , but has two important differences: ENTRYPOINT can not be overwritten by the positional arguments (i.e. docker run image [CMD] ), but has to be overwritten by --entrypoint . The positional arguments (or CMD ) are pasted to the ENTRYPOINT command. This means that you can use ENTRYPOINT as the executable and the positional arguments (or CMD ) as the options. Let\u2019s try it out: FROM python:3.9.16 RUN pip install pandas COPY daterange.py /opt ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"daterange.py\" ] # default option (if positional arguments are not specified) CMD [ \"--date\" , \"20220226\" ] Exercise : Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different date to --date . How do the commands look? Answer Just running the container non-interactively would be: docker run --rm daterange Passing a different argument (i.e. overwriting CMD ) would be: docker run --rm daterange --date 20210330 Here, the container behaves as the executable itself to which you can pass arguments. Most containerized applications need multiple build steps. Often, you want to perform these steps and executions in a specific directory. Therefore, it can be in convenient to specify a working directory. You can do that with WORKDIR . This instruction will set the default directory for all other instructions (like RUN , COPY etc.). It will also change the directory in which you will land if you run the container interactively. FROM python:3.9.16 RUN pip install pandas WORKDIR /opt # we don't have to specify /opt as target dir but the current dir COPY daterange.py . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"daterange.py\" ] # default option (if positional arguments are not specified) CMD [ \"--date\" , \"20220226\" ] Exercise : build the image, and start the container interactively. Has the default directory changed? After that, push the image to dockerhub, so we can use it later with the apptainer exercises. Note You can overwrite ENTRYPOINT with --entrypoint as an argument to docker run . Answer Running the container interactively would be: docker run -it --rm --entrypoint /bin/bash daterange Which should result in a terminal looking something like this: root@9a27da455fb1:/opt# Meaning that indeed the default directory has changed to /opt Pushing it to dockerhub: docker tag daterage [ USER NAME ] /daterange:v1 docker push [ USER NAME ] /daterange:v1","title":"Building an image with a browser interface (extra)"},{"location":"tutorials/containers/dockerfiles/#get-information-on-your-image-with-docker-inspect-extra_1","text":"We have used docker inspect already in the previous chapter to find the default Cmd of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the Config record from the output of docker inspect . For our image this looks like: \"Config\" : { \"Hostname\" : \"\" , \"Domainname\" : \"\" , \"User\" : \"\" , \"AttachStdin\" : false , \"AttachStdout\" : false , \"AttachStderr\" : false , \"Tty\" : false , \"OpenStdin\" : false , \"StdinOnce\" : false , \"Env\" : [ \"PATH=/opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" , \"LANG=C.UTF-8\" , \"GPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568\" , \"PYTHON_VERSION=3.9.4\" , \"PYTHON_PIP_VERSION=21.1.1\" , \"PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/1954f15b3f102ace496a34a013ea76b061535bd2/public/get-pip.py\" , \"PYTHON_GET_PIP_SHA256=f499d76e0149a673fb8246d88e116db589afbd291739bd84f2cd9a7bca7b6993\" ], \"Cmd\" : [ \"--date\" , \"20220226\" ], \"ArgsEscaped\" : true , \"Image\" : \"\" , \"Volumes\" : null , \"WorkingDir\" : \"/opt\" , \"Entrypoint\" : [ \"daterange.py\" ], \"OnBuild\" : null , \"Labels\" : null } ,","title":"Get information on your image with docker inspect (extra)"},{"location":"tutorials/containers/dockerfiles/#adding-metadata-to-your-image-extra_1","text":"You can annotate your Dockerfile and the image by using the instruction LABEL . You can give it any key and value with <key>=<value> . However, it is recommended to use the Open Container Initiative (OCI) keys . Exercise : Annotate our Dockerfile with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with docker inspect . Note You can type LABEL for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.: LABEL keyx = \"valuex\" keyy = \"valuey\" Answer The Dockerfile would look like: FROM python:3.9.16 LABEL org.opencontainers.image.created = \"2022-04-12\" \\ org.opencontainers.image.authors = \"Geert van Geest\" \\ org.opencontainers.image.description = \"Great container for getting all dates in a week! \\ You will never use a calender again\" RUN pip install pandas WORKDIR /opt COPY daterange.py . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"daterange.py\" ] # default option (if positional arguments are not specified) CMD [ \"--date\" , \"20220226\" ] The Config record in the output of docker inspect was updated with: \"Labels\" : { \"org.opencontainers.image.authors\" : \"Geert van Geest\" , \"org.opencontainers.image.created\" : \"2022-04-12\" , \"org.opencontainers.image.description\" : \"Great container for getting all dates in a week! You will never use a calender again\" }","title":"Adding metadata to your image (extra)"},{"location":"tutorials/containers/dockerfiles/#building-an-image-with-a-browser-interface-extra_1","text":"In this exercise, we will use a different base image from the jupyter docker image stack . JupyterLab is a nice browser interface that you can use for a.o. programming in python. With the image we are creating we will be able to run jupyter lab inside a container. Check out the Dockerfile : FROM jupyter/base-notebook:python-3.9 RUN pip install pandas This will create an image from the existing python image. It will also install jupyterlab with pip . As a default command it starts a jupyter notebook at port 8888. Ports We have specified here that jupyter lab should use port 8888. However, this inside the container. We can not connect to it yet with our browser. Exercise: Build an image based on this Dockerfile and give it a meaningful name. Answer x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t jupyter-lab . docker build --platform amd64 -t jupyter-lab . You can now run a container from the image. However, you will have to tell docker where to publish port 8888 from the docker container with -p [HOSTPORT:CONTAINERPORT] . We choose to publish it to the same port number: docker run --rm -it -p 8888 :8888 jupyter-lab Networking More info on docker container networking here By running the above command, a container will be started exposing jupyterhub at port 8888 at localhost. You can approach the instance of jupyterhub by typing localhost:8888 in your browser. You will be asked for a token. You can find this token in the terminal from which you have started the container. We can make this even more interesting by mounting a local directory to the container running the jupyter-lab image: docker run \\ -it \\ --rm \\ -p 8888 :8888 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ jupyter-lab By doing this you have a completely isolated and shareable python environment running jupyter lab, but with your local files available to it. Pretty neat right? Note Jupyter has a wide range of pre-built images available here .","title":"Building an image with a browser interface (extra)"},{"location":"tutorials/containers/introduction_containers/","text":"Learning outcomes After having completed this chapter you will be able to: Discriminate between an image and a container Run a docker container from dockerhub interactively Validate the available containers and their status Material Introduction to containers: Download the presentation Exercises If working on Windows If you are working on Windows, you can either use WSL2 or the local terminal in MobaXterm . Make sure you install the latest versions before you install docker. In principle, you can also use a native shell like PowerShell, but this might result into some issues with bind mounting directories. Let\u2019s create our first container from an existing image. We do this with the image ubuntu , generating an environment with a minimal installation of ubuntu. docker run -it ubuntu This will give you an interactive shell into the created container (this interactivity was invoked by the options -i and -t ) . Exercise: Check out the operating system of the container by typing cat /etc/os-release in the container\u2019s shell. Are we really in an ubuntu environment? Answer Yes: root@27f7d11608de:/# cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"20.04.1 LTS (Focal Fossa)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 20.04.1 LTS\" VERSION_ID=\"20.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=focal UBUNTU_CODENAME=focal Where does the image come from? If the image ubuntu was not on your computer yet, docker will search and try to get them from dockerhub , and download it. Exercise: Run the command whoami in the docker container. Who are you? Answer The command whoami returns the current user. In the container whoami will return root . This means you are the root user i.e. within the container you are admin and can basically change anything. Check out the container panel at the Docker dashboard (the Docker gui) or open another host terminal and type: docker container ls -a Exercise: What is the container status? Answer In Docker dashboard you can see that the shell is running: The output of docker container ls -a is: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 27f7d11608de ubuntu \"/bin/bash\" 7 minutes ago Up 6 minutes great_moser Also showing you that the STATUS is Up . Now let\u2019s install some software in our ubuntu environment. We\u2019ll install some simple software called figlet . Type into the container shell: apt-get update apt-get install figlet This will give some warnings This installation will give some warnings. It\u2019s safe to ignore them. Now let\u2019s try it out. Type into the container shell: figlet 'IBU does awesome bioinfo!' Now you have installed and used software figlet in an ubuntu environment (almost) completely separated from your host computer. This already gives you an idea of the power of containerization. Exit the shell by typing exit . Check out the container panel of Docker dashboard or type: docker container ls -a Exercise: What is the container status? Answer docker container ls -a gives: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 27f7d11608de ubuntu \"/bin/bash\" 15 minutes ago Exited (0) 8 seconds ago great_moser Showing that the container has exited, meaning it\u2019s not running.","title":"Introduction to containers"},{"location":"tutorials/containers/introduction_containers/#learning-outcomes","text":"After having completed this chapter you will be able to: Discriminate between an image and a container Run a docker container from dockerhub interactively Validate the available containers and their status","title":"Learning outcomes"},{"location":"tutorials/containers/introduction_containers/#material","text":"Introduction to containers: Download the presentation","title":"Material"},{"location":"tutorials/containers/introduction_containers/#exercises","text":"If working on Windows If you are working on Windows, you can either use WSL2 or the local terminal in MobaXterm . Make sure you install the latest versions before you install docker. In principle, you can also use a native shell like PowerShell, but this might result into some issues with bind mounting directories. Let\u2019s create our first container from an existing image. We do this with the image ubuntu , generating an environment with a minimal installation of ubuntu. docker run -it ubuntu This will give you an interactive shell into the created container (this interactivity was invoked by the options -i and -t ) . Exercise: Check out the operating system of the container by typing cat /etc/os-release in the container\u2019s shell. Are we really in an ubuntu environment? Answer Yes: root@27f7d11608de:/# cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"20.04.1 LTS (Focal Fossa)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 20.04.1 LTS\" VERSION_ID=\"20.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=focal UBUNTU_CODENAME=focal Where does the image come from? If the image ubuntu was not on your computer yet, docker will search and try to get them from dockerhub , and download it. Exercise: Run the command whoami in the docker container. Who are you? Answer The command whoami returns the current user. In the container whoami will return root . This means you are the root user i.e. within the container you are admin and can basically change anything. Check out the container panel at the Docker dashboard (the Docker gui) or open another host terminal and type: docker container ls -a Exercise: What is the container status? Answer In Docker dashboard you can see that the shell is running: The output of docker container ls -a is: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 27f7d11608de ubuntu \"/bin/bash\" 7 minutes ago Up 6 minutes great_moser Also showing you that the STATUS is Up . Now let\u2019s install some software in our ubuntu environment. We\u2019ll install some simple software called figlet . Type into the container shell: apt-get update apt-get install figlet This will give some warnings This installation will give some warnings. It\u2019s safe to ignore them. Now let\u2019s try it out. Type into the container shell: figlet 'IBU does awesome bioinfo!' Now you have installed and used software figlet in an ubuntu environment (almost) completely separated from your host computer. This already gives you an idea of the power of containerization. Exit the shell by typing exit . Check out the container panel of Docker dashboard or type: docker container ls -a Exercise: What is the container status? Answer docker container ls -a gives: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 27f7d11608de ubuntu \"/bin/bash\" 15 minutes ago Exited (0) 8 seconds ago great_moser Showing that the container has exited, meaning it\u2019s not running.","title":"Exercises"},{"location":"tutorials/containers/managing_docker/","text":"Learning outcomes After having completed this chapter you will be able to: Explain the concept of layers in the context of docker containers and images Use the command line to restart and re-attach to an exited container Create a new image with docker commit List locally available images with docker image ls Run a command inside a container non-interactively Use docker image inspect to get more information on an image Use the command line to prune dangling images and stopped containers Rename and tag a docker image Push a newly created image to dockerhub Use the option --mount to bind mount a host directory to a container Material Download the presentation Overview of how docker works More on bind mounts Docker volumes in general Exercises Restarting an exited container If you would like to go back to your container with the figlet installation, you could try to run again: docker run -it ubuntu Exercise: Run the above command. Is your figlet installation still there? Why? Hint Check the status of your containers: docker container ls -a Answer No, the installation is gone. Another container was created from the same ubuntu image, without the figlet installation. Running the command docker container ls -a results in: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8d7c4c611b70 ubuntu \"/bin/bash\" About a minute ago Up About a minute kind_mendel 27f7d11608de ubuntu \"/bin/bash\" 27 minutes ago Exited (0) 2 minutes ago great_moser In this case the container great_moser contains the figlet installation. But we have exited that container. We created a new container ( kind_mendel in this case) with a fresh environment created from the original ubuntu image. To restart your first created container, you\u2019ll have to look up its name. You can find it in the Docker dashboard, or with docker container ls -a . Container names The container name is the funny combination of two words separated by _ , e.g.: nifty_sinoussi . Alternatively you can use the container ID (the first column of the output of docker container ls ) To restart a container you can use: docker start [ CONTAINER NAME ] And after that to re-attach to the shell: docker attach [ CONTAINER NAME ] And you\u2019re back in the container shell. Exercise: Run the docker start and docker attach commands for the container that is supposed to contain the figlet installation. Is the installation of figlet still there? Answer yes: figlet 'try some more text!' Should give you output. docker attach and docker exec In addition to docker attach , you can also \u201cre-attach\u201d a container with docker exec . However, these two are quite different. While docker attach gets you back to your stopped shell process, docker exec creates a new one (more information on stackoverflow ). The command docker exec enables you therefore to have multiple shells open in the same container. That can be convenient if you have one shell open with a program running in the foreground, and another one for e.g. monitoring. An example for using docker exec on a running container: docker exec -it [ CONTAINER NAME ] /bin/bash Note that docker exec requires a CMD, it doesn\u2019t use the default. Creating a new image You can store your changes and create a new image based on the ubuntu image like this: docker commit [ CONTAINER NAME ] ubuntu-figlet Exercise: Run the above command with the name of the container containing the figlet installation. Check out docker image ls . What have we just created? Answer A new image called ubuntu-figlet based on the status of the container. The output of docker image ls should look like: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu-figlet latest e08b999c7978 4 seconds ago 101MB ubuntu latest f63181f19b2f 29 hours ago 72.9MB Now you can generate a new container based on the new image: docker run -it ubuntu-figlet Exercise: Run the above command. Is the figlet installation in the created container? Answer yes Commands The second positional argument of docker run can be a command followed by its arguments. So, we could run a container non-interactively (without -it ), and just let it run a single command: docker run ubuntu-figlet figlet 'non-interactive run' Resulting in just the output of the figlet command. In the previous exercises we have run containers without a command as positional argument. This doesn\u2019t mean that no command has been run, because the container would do nothing without a command. The default command is stored in the image, and you can find it by docker image inspect [IMAGE NAME] . Exercise: Have a look at the output of docker image inspect , particularly at \"Config\" (ignore \"ContainerConfig\" for now). What is the default command ( CMD ) of the ubuntu image? Answer Running docker image inspect ubuntu gives (amongst other information): \"Cmd\" : [ \"/bin/bash\" ] , In the case of the ubuntu the default command is bash , returning a shell in bash (i.e. Bourne again shell ). Adding the options -i and -t ( -it ) to your docker run command will therefore result in an interactive bash shell. You can modify this default behaviour. More on that later, when we will work on Dockerfiles . The difference between Config and ContainerConfig The configuration at Config represents the image, the configuration at ContainerConfig the last step during the build of the image, i.e. the last layer. More info e.g. at this post at stackoverflow . Removing containers In the meantime, with every call of docker run we have created a new container (check your containers with docker container ls -a ). You probably don\u2019t want to remove those one-by-one. These two commands are very useful to clean up your Docker cache: docker container prune : removes stopped containers docker image prune : removes dangling images (i.e. images without a name) So, remove your stopped containers with: docker container prune Unless you\u2019re developing further on a container, or you\u2019re using it for an analysis, you probably want to get rid of it once you have exited the container. You can do this with adding --rm to your docker run command, e.g.: docker run --rm ubuntu-figlet figlet 'non-interactive run' Pushing to dockerhub Now that we have created our first own docker image, we can store it and share it with the world on docker hub. Before we get there, we first have to (re)name and tag it. Before pushing an image to dockerhub, docker has to know to which user and which repository the image should be added. That information should be in the name of the image, like this: user/imagename . We can rename an image with docker tag (which is a bit of misleading name for the command). So we could push to dockerhub like this: docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet docker push [USER NAME]/ubuntu-figlet If on Linux If you are on Linux and haven\u2019t connected to docker hub before, you will have login first. To do that, run: docker login How docker makes money All images pushed to dockerhub are open to the world. With a free account you can have one image on dockerhub that is private. Paid accounts can have more private images, and are therefore popular for commercial organisations. As an alternative to dockerhub, you can store images locally with docker save . We didn\u2019t specify the tag for our new image. That\u2019s why docker tag gave it the default tag called latest . Pushing an image without a tag will overwrite the current image with the tag latest (more on (not) using latest here ). If you want to maintain multiple versions of your image, you will have to add a tag, and push the image with that tag to dockerhub: docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet:v1 docker push [USER NAME]/ubuntu-figlet:v1 Mounting a directory For many analyses you do calculations with files or scripts that are on your host (local) computer. But how do you make them available to a docker container? You can do that in several ways, but here we will use bind-mount. You can bind-mount a directory with -v ( --volume ) or --mount . Most old-school docker users will use -v , but --mount syntax is easier to understand and now recommended, so we will use the latter here: docker run \\ --mount type = bind,source = /host/source/path,target = /path/in/container \\ [ IMAGE ] The target directory will be created if it does not yet exist. The source directory should exist. MobaXterm users You can specify your local path with the Windows syntax (e.g. C:\\Users\\myusername ). However, you will have to use forward slashes ( / ) instead of backward slashes ( \\ ). Therefore, mounting a directory would look like: docker run \\ --mount type = bind,source = C:/Users/myusername,target = /path/in/container \\ [ IMAGE ] Do not use autocompletion or variable substitution (e.g. $PWD ) in MobaXterm, since these point to \u2018emulated\u2019 paths, and are not passed properly to the docker command. Using docker from Windows PowerShell Most of the syntax for docker is the same for both PowerShell and UNIX-based systems. However, there are some differences, e.g. in Windows, directories in file paths are separated by \\ instead of / . Also, line breaks are not escaped by \\ but by `. Exercise: Mount a host (local) directory to a target directory /working_dir in a container created from the ubuntu-figlet image and run it interactively. Check whether the target directory has been created. Answer e.g. on Mac OS this would be: docker run \\ -it \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ ubuntu-figlet This creates a directory called working_dir in the root directory ( / ): root@8d80a8698865:/# ls bin dev home lib32 libx32 mnt proc run srv tmp var boot etc lib lib64 media opt root sbin sys usr working_dir This mounted directory is both available for the host (locally) and for the container. You can therefore e.g. copy files in there, and write output generated by the container. Exercise: Write the output of figlet \"testing mounted dir\" to a file in /working_dir . Check whether it is available on the host (locally) in the source directory. Hint You can write the output of figlet to a file like this: figlet 'some string' > file.txt Answer root@8d80a8698865:/# figlet 'testing mounted dir' > /working_dir/figlet_output.txt This should create a file in both your host (local) source directory and the target directory in the container called figlet_output.txt . Using files on the host This of course also works the other way around. If you would have a file on the host with e.g. a text, you can copy it into your mounted directory, and it will be available to the container. Managing permissions (extra) Depending on your system, the user ID and group ID will be taken over from the user inside the container. If the user inside the container is root, this will be root. That\u2019s a bit inconvenient if you just want to run the container as a regular user (for example in certain circumstances your container could write in / ). To do that, use the -u option, and specify the group ID and user ID like this: docker run -u [ uid ] : [ gid ] So, e.g.: docker run \\ -it \\ -u 1000 :1000 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ ubuntu-figlet If you want docker to take over your current uid and gid, you can use: docker run -u \"$(id -u):$(id -g)\" This behaviour is different on MacOS and MobaXterm On MacOS and in the local shell of MobaXterm the uid and gid are taken over from the user running the container (even if you set -u as 0:0), i.e. your current ID. More info on stackoverflow . Exercise: Start an interactive container based on the ubuntu-figlet image, bind-mount a local directory and take over your current uid and gid . Write the output of a figlet command to a file in the mounted directory. Who and which group owns the file inside the container? And outside the container? Answer the same question but now run the container without setting -u . Answer Linux MacOS MobaXterm Running ubuntu-figlet interactively while taking over uid and gid and mounting my current directory: docker run -it --mount type = bind,source = $PWD ,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=1000 gid=1000 groups=1000 So, I have taken over uid 1000 and gid 1000. I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 1000 1000 0 Mar 400 13:37 uid_set.txt So the file belongs to user 1000, and group 1000. Outside container: ubuntu@ip-172-31-33-21:~$ ls -lh -rw-r--r-- 1 ubuntu ubuntu 400 Mar 5 13:37 uid_set.txt Which makes sense: ubuntu@ip-172-31-33-21:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu) Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = $PWD ,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, uid and gid are root . root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 1000 1000 400 Mar 5 13:37 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:40 uid_unset.txt Outside container: ubuntu@ip-172-31-33-21:~$ ls -lh -rw-r--r-- 1 ubuntu ubuntu 0 Mar 5 13:37 uid_set.txt -rw-r--r-- 1 root root 0 Mar 5 13:40 uid_unset.txt So, the uid and gid 0 (root:root) are taken over. Running ubuntu-figlet interactively while taking over uid and gid and mounting my current directory: docker run -it --mount type = bind,source = $PWD ,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=503 gid=20(dialout) groups=20(dialout) So, the container has taken over uid 503 and group 20 I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 503 dialout 400 Mar 5 13:11 uid_set.txt So the file belongs to user 503, and the group dialout . Outside container: mac-34392:~ geertvangeest$ ls -lh -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:11 uid_set.txt Which are the same as inside the container: mac-34392:~ geertvangeest$ echo \"$(id -u):$(id -g)\" 503:20 The uid 503 was nameless in the docker container. However the group 20 already existed in the ubuntu container, and was named dialout . Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = $PWD ,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, inside the container I am root . Creating new files will lead to ownership of root inside the container: root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 503 dialout 400 Mar 5 13:11 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:25 uid_unset.txt Outside container: mac-34392:~ geertvangeest$ ls -lh -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:11 uid_set.txt -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:15 uid_unset.txt So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used. Running ubuntu-figlet interactively while taking over uid and gid and mounting to a specfied directory: docker run -it --mount type = bind,source = C:/Users/geert/data,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=1003 gid=513 groups=513 So, the container has taken over uid 1003 and group 513 I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 1003 513 400 Mar 5 13:11 uid_set.txt So the file belongs to user 1003, and the group 513. Outside container: /home/mobaxterm/data$ ls -lh -rwx------ 1 geert UserGrp 400 Mar 5 14:11 uid_set.txt Which are the same as inside the container: /home/mobaxterm/data$ echo \"$(id -u):$(id -g)\" 1003:513 Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = C:/Users/geert/data,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, inside the container I am root . Creating new files will lead to ownership of root inside the container: root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 1003 503 400 Mar 5 13:11 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:25 uid_unset.txt Outside container: /home/mobaxterm/data$ ls -lh -rwx------ 1 geert UserGrp 400 Mar 5 14:11 uid_set.txt -rwx------ 1 geert UserGrp 400 Mar 5 14:15 uid_unset.txt So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used.","title":"Managing docker"},{"location":"tutorials/containers/managing_docker/#learning-outcomes","text":"After having completed this chapter you will be able to: Explain the concept of layers in the context of docker containers and images Use the command line to restart and re-attach to an exited container Create a new image with docker commit List locally available images with docker image ls Run a command inside a container non-interactively Use docker image inspect to get more information on an image Use the command line to prune dangling images and stopped containers Rename and tag a docker image Push a newly created image to dockerhub Use the option --mount to bind mount a host directory to a container","title":"Learning outcomes"},{"location":"tutorials/containers/managing_docker/#material","text":"Download the presentation Overview of how docker works More on bind mounts Docker volumes in general","title":"Material"},{"location":"tutorials/containers/managing_docker/#exercises","text":"","title":"Exercises"},{"location":"tutorials/containers/managing_docker/#restarting-an-exited-container","text":"If you would like to go back to your container with the figlet installation, you could try to run again: docker run -it ubuntu Exercise: Run the above command. Is your figlet installation still there? Why? Hint Check the status of your containers: docker container ls -a Answer No, the installation is gone. Another container was created from the same ubuntu image, without the figlet installation. Running the command docker container ls -a results in: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8d7c4c611b70 ubuntu \"/bin/bash\" About a minute ago Up About a minute kind_mendel 27f7d11608de ubuntu \"/bin/bash\" 27 minutes ago Exited (0) 2 minutes ago great_moser In this case the container great_moser contains the figlet installation. But we have exited that container. We created a new container ( kind_mendel in this case) with a fresh environment created from the original ubuntu image. To restart your first created container, you\u2019ll have to look up its name. You can find it in the Docker dashboard, or with docker container ls -a . Container names The container name is the funny combination of two words separated by _ , e.g.: nifty_sinoussi . Alternatively you can use the container ID (the first column of the output of docker container ls ) To restart a container you can use: docker start [ CONTAINER NAME ] And after that to re-attach to the shell: docker attach [ CONTAINER NAME ] And you\u2019re back in the container shell. Exercise: Run the docker start and docker attach commands for the container that is supposed to contain the figlet installation. Is the installation of figlet still there? Answer yes: figlet 'try some more text!' Should give you output. docker attach and docker exec In addition to docker attach , you can also \u201cre-attach\u201d a container with docker exec . However, these two are quite different. While docker attach gets you back to your stopped shell process, docker exec creates a new one (more information on stackoverflow ). The command docker exec enables you therefore to have multiple shells open in the same container. That can be convenient if you have one shell open with a program running in the foreground, and another one for e.g. monitoring. An example for using docker exec on a running container: docker exec -it [ CONTAINER NAME ] /bin/bash Note that docker exec requires a CMD, it doesn\u2019t use the default.","title":"Restarting an exited container"},{"location":"tutorials/containers/managing_docker/#creating-a-new-image","text":"You can store your changes and create a new image based on the ubuntu image like this: docker commit [ CONTAINER NAME ] ubuntu-figlet Exercise: Run the above command with the name of the container containing the figlet installation. Check out docker image ls . What have we just created? Answer A new image called ubuntu-figlet based on the status of the container. The output of docker image ls should look like: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu-figlet latest e08b999c7978 4 seconds ago 101MB ubuntu latest f63181f19b2f 29 hours ago 72.9MB Now you can generate a new container based on the new image: docker run -it ubuntu-figlet Exercise: Run the above command. Is the figlet installation in the created container? Answer yes","title":"Creating a new image"},{"location":"tutorials/containers/managing_docker/#commands","text":"The second positional argument of docker run can be a command followed by its arguments. So, we could run a container non-interactively (without -it ), and just let it run a single command: docker run ubuntu-figlet figlet 'non-interactive run' Resulting in just the output of the figlet command. In the previous exercises we have run containers without a command as positional argument. This doesn\u2019t mean that no command has been run, because the container would do nothing without a command. The default command is stored in the image, and you can find it by docker image inspect [IMAGE NAME] . Exercise: Have a look at the output of docker image inspect , particularly at \"Config\" (ignore \"ContainerConfig\" for now). What is the default command ( CMD ) of the ubuntu image? Answer Running docker image inspect ubuntu gives (amongst other information): \"Cmd\" : [ \"/bin/bash\" ] , In the case of the ubuntu the default command is bash , returning a shell in bash (i.e. Bourne again shell ). Adding the options -i and -t ( -it ) to your docker run command will therefore result in an interactive bash shell. You can modify this default behaviour. More on that later, when we will work on Dockerfiles . The difference between Config and ContainerConfig The configuration at Config represents the image, the configuration at ContainerConfig the last step during the build of the image, i.e. the last layer. More info e.g. at this post at stackoverflow .","title":"Commands"},{"location":"tutorials/containers/managing_docker/#removing-containers","text":"In the meantime, with every call of docker run we have created a new container (check your containers with docker container ls -a ). You probably don\u2019t want to remove those one-by-one. These two commands are very useful to clean up your Docker cache: docker container prune : removes stopped containers docker image prune : removes dangling images (i.e. images without a name) So, remove your stopped containers with: docker container prune Unless you\u2019re developing further on a container, or you\u2019re using it for an analysis, you probably want to get rid of it once you have exited the container. You can do this with adding --rm to your docker run command, e.g.: docker run --rm ubuntu-figlet figlet 'non-interactive run'","title":"Removing containers"},{"location":"tutorials/containers/managing_docker/#pushing-to-dockerhub","text":"Now that we have created our first own docker image, we can store it and share it with the world on docker hub. Before we get there, we first have to (re)name and tag it. Before pushing an image to dockerhub, docker has to know to which user and which repository the image should be added. That information should be in the name of the image, like this: user/imagename . We can rename an image with docker tag (which is a bit of misleading name for the command). So we could push to dockerhub like this: docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet docker push [USER NAME]/ubuntu-figlet If on Linux If you are on Linux and haven\u2019t connected to docker hub before, you will have login first. To do that, run: docker login How docker makes money All images pushed to dockerhub are open to the world. With a free account you can have one image on dockerhub that is private. Paid accounts can have more private images, and are therefore popular for commercial organisations. As an alternative to dockerhub, you can store images locally with docker save . We didn\u2019t specify the tag for our new image. That\u2019s why docker tag gave it the default tag called latest . Pushing an image without a tag will overwrite the current image with the tag latest (more on (not) using latest here ). If you want to maintain multiple versions of your image, you will have to add a tag, and push the image with that tag to dockerhub: docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet:v1 docker push [USER NAME]/ubuntu-figlet:v1","title":"Pushing to dockerhub"},{"location":"tutorials/containers/managing_docker/#mounting-a-directory","text":"For many analyses you do calculations with files or scripts that are on your host (local) computer. But how do you make them available to a docker container? You can do that in several ways, but here we will use bind-mount. You can bind-mount a directory with -v ( --volume ) or --mount . Most old-school docker users will use -v , but --mount syntax is easier to understand and now recommended, so we will use the latter here: docker run \\ --mount type = bind,source = /host/source/path,target = /path/in/container \\ [ IMAGE ] The target directory will be created if it does not yet exist. The source directory should exist. MobaXterm users You can specify your local path with the Windows syntax (e.g. C:\\Users\\myusername ). However, you will have to use forward slashes ( / ) instead of backward slashes ( \\ ). Therefore, mounting a directory would look like: docker run \\ --mount type = bind,source = C:/Users/myusername,target = /path/in/container \\ [ IMAGE ] Do not use autocompletion or variable substitution (e.g. $PWD ) in MobaXterm, since these point to \u2018emulated\u2019 paths, and are not passed properly to the docker command. Using docker from Windows PowerShell Most of the syntax for docker is the same for both PowerShell and UNIX-based systems. However, there are some differences, e.g. in Windows, directories in file paths are separated by \\ instead of / . Also, line breaks are not escaped by \\ but by `. Exercise: Mount a host (local) directory to a target directory /working_dir in a container created from the ubuntu-figlet image and run it interactively. Check whether the target directory has been created. Answer e.g. on Mac OS this would be: docker run \\ -it \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ ubuntu-figlet This creates a directory called working_dir in the root directory ( / ): root@8d80a8698865:/# ls bin dev home lib32 libx32 mnt proc run srv tmp var boot etc lib lib64 media opt root sbin sys usr working_dir This mounted directory is both available for the host (locally) and for the container. You can therefore e.g. copy files in there, and write output generated by the container. Exercise: Write the output of figlet \"testing mounted dir\" to a file in /working_dir . Check whether it is available on the host (locally) in the source directory. Hint You can write the output of figlet to a file like this: figlet 'some string' > file.txt Answer root@8d80a8698865:/# figlet 'testing mounted dir' > /working_dir/figlet_output.txt This should create a file in both your host (local) source directory and the target directory in the container called figlet_output.txt . Using files on the host This of course also works the other way around. If you would have a file on the host with e.g. a text, you can copy it into your mounted directory, and it will be available to the container.","title":"Mounting a directory"},{"location":"tutorials/containers/managing_docker/#managing-permissions-extra","text":"Depending on your system, the user ID and group ID will be taken over from the user inside the container. If the user inside the container is root, this will be root. That\u2019s a bit inconvenient if you just want to run the container as a regular user (for example in certain circumstances your container could write in / ). To do that, use the -u option, and specify the group ID and user ID like this: docker run -u [ uid ] : [ gid ] So, e.g.: docker run \\ -it \\ -u 1000 :1000 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ ubuntu-figlet If you want docker to take over your current uid and gid, you can use: docker run -u \"$(id -u):$(id -g)\" This behaviour is different on MacOS and MobaXterm On MacOS and in the local shell of MobaXterm the uid and gid are taken over from the user running the container (even if you set -u as 0:0), i.e. your current ID. More info on stackoverflow . Exercise: Start an interactive container based on the ubuntu-figlet image, bind-mount a local directory and take over your current uid and gid . Write the output of a figlet command to a file in the mounted directory. Who and which group owns the file inside the container? And outside the container? Answer the same question but now run the container without setting -u . Answer Linux MacOS MobaXterm Running ubuntu-figlet interactively while taking over uid and gid and mounting my current directory: docker run -it --mount type = bind,source = $PWD ,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=1000 gid=1000 groups=1000 So, I have taken over uid 1000 and gid 1000. I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 1000 1000 0 Mar 400 13:37 uid_set.txt So the file belongs to user 1000, and group 1000. Outside container: ubuntu@ip-172-31-33-21:~$ ls -lh -rw-r--r-- 1 ubuntu ubuntu 400 Mar 5 13:37 uid_set.txt Which makes sense: ubuntu@ip-172-31-33-21:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu) Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = $PWD ,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, uid and gid are root . root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 1000 1000 400 Mar 5 13:37 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:40 uid_unset.txt Outside container: ubuntu@ip-172-31-33-21:~$ ls -lh -rw-r--r-- 1 ubuntu ubuntu 0 Mar 5 13:37 uid_set.txt -rw-r--r-- 1 root root 0 Mar 5 13:40 uid_unset.txt So, the uid and gid 0 (root:root) are taken over. Running ubuntu-figlet interactively while taking over uid and gid and mounting my current directory: docker run -it --mount type = bind,source = $PWD ,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=503 gid=20(dialout) groups=20(dialout) So, the container has taken over uid 503 and group 20 I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 503 dialout 400 Mar 5 13:11 uid_set.txt So the file belongs to user 503, and the group dialout . Outside container: mac-34392:~ geertvangeest$ ls -lh -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:11 uid_set.txt Which are the same as inside the container: mac-34392:~ geertvangeest$ echo \"$(id -u):$(id -g)\" 503:20 The uid 503 was nameless in the docker container. However the group 20 already existed in the ubuntu container, and was named dialout . Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = $PWD ,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, inside the container I am root . Creating new files will lead to ownership of root inside the container: root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 503 dialout 400 Mar 5 13:11 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:25 uid_unset.txt Outside container: mac-34392:~ geertvangeest$ ls -lh -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:11 uid_set.txt -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:15 uid_unset.txt So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used. Running ubuntu-figlet interactively while taking over uid and gid and mounting to a specfied directory: docker run -it --mount type = bind,source = C:/Users/geert/data,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=1003 gid=513 groups=513 So, the container has taken over uid 1003 and group 513 I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 1003 513 400 Mar 5 13:11 uid_set.txt So the file belongs to user 1003, and the group 513. Outside container: /home/mobaxterm/data$ ls -lh -rwx------ 1 geert UserGrp 400 Mar 5 14:11 uid_set.txt Which are the same as inside the container: /home/mobaxterm/data$ echo \"$(id -u):$(id -g)\" 1003:513 Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = C:/Users/geert/data,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, inside the container I am root . Creating new files will lead to ownership of root inside the container: root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 1003 503 400 Mar 5 13:11 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:25 uid_unset.txt Outside container: /home/mobaxterm/data$ ls -lh -rwx------ 1 geert UserGrp 400 Mar 5 14:11 uid_set.txt -rwx------ 1 geert UserGrp 400 Mar 5 14:15 uid_unset.txt So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used.","title":"Managing permissions (extra)"},{"location":"tutorials/gitlab/git_cheatsheet/","text":"Git cheatsheet Status and logs Get status of the repository. E.g. staged or non-tracked files. git status Get log of different commits git log Get difference between working directory and HEAD (latest commit) git diff Get difference between staged changed and HEAD git diff --staged Get difference but color by words (not lines) git diff --color-words See changes between different commits (or HEAD) git diff fa4f3deac9a...HEAD Show changes (diff) of a specific commit git show --color-words fa4f Show changes of latest commit: git show HEAD Restore or unstage changes Move HEAD to a specific commit (or tree-ish ): git checkout fa4f3deac Discard unstaged changes in working directory (here in resources.html ): git restore resources.html # or git checkout -- resources.html Unstage a file: git restore --staged resources.html Retrieve back a file version at the point after a commit. Here fa4f3deac is the sha for the commit before index.html was changed git restore --source fa4f3deac index.html # or git checkout fa4f3deac -- index.html Or, take the version before the \u2018bad\u2019 ( 813c98e ) commit: git restore --source 813c98e~1 index.html Or, revert the entire commit: git revert 813c98e Remove untracked files: git clean -f # use git clean -n for dry-run Finding commits Search commits with git log : git log --since = \"2021-01-01\" git log --author = \"Kevin\" git log --grep = \"bug\" git log <SHA>...<SHA> git log filename.html Format log: git log --oneline # oneline quick overview git log --graph --all --oneline --decorate Branches Create a branch: git branch new_feature Check out branches: git branch Changing branches: git checkout new_feature Combine creating and changing: git checkout -b shorten_title Check which branches are contained in branch: git branch --merged Or not merged (i.e. unmerged changes): git branch --no-merged Change branch name: git branch --move seo_title Delete a branch (can only be done from another branch): git branch -d branch_to_delete Resetting repository Only in private repository. A soft reset moves HEAD to a tree-ish and puts all changes from that tree-ish to current HEAD in the staging index. This is e.g. convenient for merging multiple commits together. git reset --soft <tree-ish> Moving back to a commit can be done with the \u2018old\u2019 SHA as tree-ish . A mixed reset does the same as soft but does not leave the changes staged, but as changes in the working directory. Can be convenient for splitting commits. git reset --mixed <tree-ish> A hard reset returns to an old state and discards all changes. Useful to permanently undo commits. However, you can go back while pointing to the SHA (on the short term) git reset --hard <tree-ish> Because the argument is a tree-ish , you can also make your branch look like another branch: git reset --hard seo_title Merging Merge branch with current branch git merge other_branch Abort merge: git merge --abort Before resolving conflicts, it can be conveniet to check word changes with: git show --color-words View branches and merges: git log --graph --all --oneline --decorate To minimize merge conflicts, merge often. Either from develop branch to master (if possible) or merge master into development branch, to stay up to date. Stashing Stashing is used to e.g. switch branches while leaving changes un-committed. To stash changes: git stash save \"a descriptive stash message\" List stashed changes: git stash list See actual changes in a specific stash: git stash show -p stash@ { 0 } Retrieve a stash and delete it: git stash pop [ stash@ { 0 }] Retrieve a stash and leave it: git stash apply [ stash@ { 0 }] Remove a stash: git stash drop stash@ { 0 } Or to clear all stashes: git stash clear Remotes Get the remote connections to the repo: git remote git remote -v A naming convention is to give the remote connection the name origin . Therefore setting up a remote connection looks like: git remote add origin https://github.com/GeertvanGeest/explore_california.git Delete a remote: git remote rm origin Pushing can be done on a branch-by-branch basis. The -u option makes sure the branch is tracked. git push -u origin master If branch is already tracked, you will need only: git push View remote branches: git branch -r # all branches: git branch -a Start tracking a remote branch: git branch -u origin/branch_name branch_name Fetch remote changes. Only tracked branches are fetched: git fetch Merge current branch with a tracked branch: git merge origin/master A shortcut for git fetch and git merge : git pull If merge is expected, best to use the two-step way (fetch & merge). Removing a branch: git push origin --delete non_tracking Use remote branch and start tracking it locally: git checkout -b branch_name origin/branch_name","title":"Git cheatsheet"},{"location":"tutorials/gitlab/git_cheatsheet/#git-cheatsheet","text":"","title":"Git cheatsheet"},{"location":"tutorials/gitlab/git_cheatsheet/#status-and-logs","text":"Get status of the repository. E.g. staged or non-tracked files. git status Get log of different commits git log Get difference between working directory and HEAD (latest commit) git diff Get difference between staged changed and HEAD git diff --staged Get difference but color by words (not lines) git diff --color-words See changes between different commits (or HEAD) git diff fa4f3deac9a...HEAD Show changes (diff) of a specific commit git show --color-words fa4f Show changes of latest commit: git show HEAD","title":"Status and logs"},{"location":"tutorials/gitlab/git_cheatsheet/#restore-or-unstage-changes","text":"Move HEAD to a specific commit (or tree-ish ): git checkout fa4f3deac Discard unstaged changes in working directory (here in resources.html ): git restore resources.html # or git checkout -- resources.html Unstage a file: git restore --staged resources.html Retrieve back a file version at the point after a commit. Here fa4f3deac is the sha for the commit before index.html was changed git restore --source fa4f3deac index.html # or git checkout fa4f3deac -- index.html Or, take the version before the \u2018bad\u2019 ( 813c98e ) commit: git restore --source 813c98e~1 index.html Or, revert the entire commit: git revert 813c98e Remove untracked files: git clean -f # use git clean -n for dry-run","title":"Restore or unstage changes"},{"location":"tutorials/gitlab/git_cheatsheet/#finding-commits","text":"Search commits with git log : git log --since = \"2021-01-01\" git log --author = \"Kevin\" git log --grep = \"bug\" git log <SHA>...<SHA> git log filename.html Format log: git log --oneline # oneline quick overview git log --graph --all --oneline --decorate","title":"Finding commits"},{"location":"tutorials/gitlab/git_cheatsheet/#branches","text":"Create a branch: git branch new_feature Check out branches: git branch Changing branches: git checkout new_feature Combine creating and changing: git checkout -b shorten_title Check which branches are contained in branch: git branch --merged Or not merged (i.e. unmerged changes): git branch --no-merged Change branch name: git branch --move seo_title Delete a branch (can only be done from another branch): git branch -d branch_to_delete","title":"Branches"},{"location":"tutorials/gitlab/git_cheatsheet/#resetting-repository","text":"Only in private repository. A soft reset moves HEAD to a tree-ish and puts all changes from that tree-ish to current HEAD in the staging index. This is e.g. convenient for merging multiple commits together. git reset --soft <tree-ish> Moving back to a commit can be done with the \u2018old\u2019 SHA as tree-ish . A mixed reset does the same as soft but does not leave the changes staged, but as changes in the working directory. Can be convenient for splitting commits. git reset --mixed <tree-ish> A hard reset returns to an old state and discards all changes. Useful to permanently undo commits. However, you can go back while pointing to the SHA (on the short term) git reset --hard <tree-ish> Because the argument is a tree-ish , you can also make your branch look like another branch: git reset --hard seo_title","title":"Resetting repository"},{"location":"tutorials/gitlab/git_cheatsheet/#merging","text":"Merge branch with current branch git merge other_branch Abort merge: git merge --abort Before resolving conflicts, it can be conveniet to check word changes with: git show --color-words View branches and merges: git log --graph --all --oneline --decorate To minimize merge conflicts, merge often. Either from develop branch to master (if possible) or merge master into development branch, to stay up to date.","title":"Merging"},{"location":"tutorials/gitlab/git_cheatsheet/#stashing","text":"Stashing is used to e.g. switch branches while leaving changes un-committed. To stash changes: git stash save \"a descriptive stash message\" List stashed changes: git stash list See actual changes in a specific stash: git stash show -p stash@ { 0 } Retrieve a stash and delete it: git stash pop [ stash@ { 0 }] Retrieve a stash and leave it: git stash apply [ stash@ { 0 }] Remove a stash: git stash drop stash@ { 0 } Or to clear all stashes: git stash clear","title":"Stashing"},{"location":"tutorials/gitlab/git_cheatsheet/#remotes","text":"Get the remote connections to the repo: git remote git remote -v A naming convention is to give the remote connection the name origin . Therefore setting up a remote connection looks like: git remote add origin https://github.com/GeertvanGeest/explore_california.git Delete a remote: git remote rm origin Pushing can be done on a branch-by-branch basis. The -u option makes sure the branch is tracked. git push -u origin master If branch is already tracked, you will need only: git push View remote branches: git branch -r # all branches: git branch -a Start tracking a remote branch: git branch -u origin/branch_name branch_name Fetch remote changes. Only tracked branches are fetched: git fetch Merge current branch with a tracked branch: git merge origin/master A shortcut for git fetch and git merge : git pull If merge is expected, best to use the two-step way (fetch & merge). Removing a branch: git push origin --delete non_tracking Use remote branch and start tracking it locally: git checkout -b branch_name origin/branch_name","title":"Remotes"},{"location":"tutorials/gitlab/gitlab/","text":"Based on this youtube video: Log in https://gitlab.bioinformatics.unibe.ch User name and password are the same as for the cluster. The older https://binfgitlab.unibe.ch still exists for historical/compatibility reasons. Create a new project in GitLab This is self-explanatory. In GitLab \u2014> New Project Note: It is easier NOT to initialize with a readme otherwise you will run into the issue described in the last step Connect to GitLab from binfservms01 We need to set up SSH keys so that binfservms01 and gitlab can talk to each other. You can do this following the instructions here to learn how to generate SSH keys. Once you generated your keys, you can follow the instructions here to copy your private key to gitlab. To test if it worked: From the cluster, type: ssh -T git@gitlab.bioinformatics.unibe.ch If everything is fine, you should get a Welcome to GitLab message. Specify user name and email On the cluster, check your current git user name and email address: git config --list It is probably a good idea for the user name to be the same as for your cluster account (but not sure if this is strictly necessary). You can change the current values like this: git config --global user.name <myname> git config --global user.email <myemail@example.ch> Set up new repository on the cluster mkdir <myscripts> cd <myscripts> git init Add files to this directory. When you now run git status , you see that there are untracked files in the folder. To start tracking the files git add * git commit * Push to GitLab Go to the project you created in GitLab and copy the path to the repository. This will look similar to this: git@binfgitlab.unibe.ch:/. On the cluster: git remote add origin git@gitlab.bioinformatics.unibe.ch:<username>/<myproject> Now, everything is set up so that you can push to the GitLab repository: git push -u origin master If you go back to GitLab and refresh the page, you should now see the new files. If GitLab automatically created a readme file when you first created your project, you may get this error Updates were rejected because the remote contains work that you do not have locally . In this case, run git pull origin master git push origin master","title":"Set up"},{"location":"tutorials/gitlab/gitlab/#log-in","text":"https://gitlab.bioinformatics.unibe.ch User name and password are the same as for the cluster. The older https://binfgitlab.unibe.ch still exists for historical/compatibility reasons.","title":"Log in"},{"location":"tutorials/gitlab/gitlab/#create-a-new-project-in-gitlab","text":"This is self-explanatory. In GitLab \u2014> New Project Note: It is easier NOT to initialize with a readme otherwise you will run into the issue described in the last step","title":"Create a new project in GitLab"},{"location":"tutorials/gitlab/gitlab/#connect-to-gitlab-from-binfservms01","text":"We need to set up SSH keys so that binfservms01 and gitlab can talk to each other. You can do this following the instructions here to learn how to generate SSH keys. Once you generated your keys, you can follow the instructions here to copy your private key to gitlab. To test if it worked: From the cluster, type: ssh -T git@gitlab.bioinformatics.unibe.ch If everything is fine, you should get a Welcome to GitLab message.","title":"Connect to GitLab from binfservms01"},{"location":"tutorials/gitlab/gitlab/#specify-user-name-and-email","text":"On the cluster, check your current git user name and email address: git config --list It is probably a good idea for the user name to be the same as for your cluster account (but not sure if this is strictly necessary). You can change the current values like this: git config --global user.name <myname> git config --global user.email <myemail@example.ch>","title":"Specify user name and email"},{"location":"tutorials/gitlab/gitlab/#set-up-new-repository-on-the-cluster","text":"mkdir <myscripts> cd <myscripts> git init Add files to this directory. When you now run git status , you see that there are untracked files in the folder. To start tracking the files git add * git commit *","title":"Set up new repository on the cluster"},{"location":"tutorials/gitlab/gitlab/#push-to-gitlab","text":"Go to the project you created in GitLab and copy the path to the repository. This will look similar to this: git@binfgitlab.unibe.ch:/. On the cluster: git remote add origin git@gitlab.bioinformatics.unibe.ch:<username>/<myproject> Now, everything is set up so that you can push to the GitLab repository: git push -u origin master If you go back to GitLab and refresh the page, you should now see the new files. If GitLab automatically created a readme file when you first created your project, you may get this error Updates were rejected because the remote contains work that you do not have locally . In this case, run git pull origin master git push origin master","title":"Push to GitLab"},{"location":"tutorials/gitlab/gitlab_ci/","text":"Gitlab and GitHub CI are extremely useful for running processes that are related to your software development project, like testing, building container images, deploying webpages etc. In gitlab, all you need to do is generate a file called .gitlab-ci.yml , and specify there what kind of processes need to run. In such a file you specify: When someting needs to be run (e.g. after a push to the main branch, or after a release) In what kind of environment it needs to run (often an image on dockerhub) What needs to be run (e.g. running a test script) In order to get started with gitlab ci, have a look here . Build and push image example This example of .gitlab-ci.yml does the following: Pull and use docker-in-docker (dind) container Generate an image tag based on the CI container registry name and a repository tag (i.e. release). CI container registry By default, the CI container registry is the registry associated to the repo; find it at Packages & Registries > Container Registry ) Login to the CI container registry Build the image using the repository as a context (i.e. using a Dockerfile that is in the base directory of the repo) Push the built image to the container registry Run the CI pipeline only when a tag (i.e. release) is pushed. By default it will with each push. Predefined variables Many of the variables used in this yml are predefined. Have a look at the reference here: https://docs.gitlab.com/ee/ci/variables/predefined_variables.html . variables : DOCKER_DRIVER : overlay2 DOCKER_TLS_CERTDIR : \"\" stages : - build build : image : docker:20.10.16 stage : build services : - docker:20.10.16-dind variables : IMAGE_TAG : $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG script : - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY - docker build -t $IMAGE_TAG . - docker push $IMAGE_TAG only : - tags If everything goes well, the image will be pushed to the container registry. In order to pull the image: Docker apptainer docker login gitlab.bioinformatics.unibe.ch:5050 docker pull \\ gitlab.bioinformatics.unibe.ch:5050/ [ namespace ] / [ repo name ] / [ image name ] : [ tag name ] # use login only the first time apptainer remote login --username foo docker://gitlab.bioinformatics.unibe.ch:5050 apptainer pull \\ docker://gitlab.bioinformatics.unibe.ch:5050/ [ namespace ] / [ repo name ] / [ image name ] : [ tag name ] \ud83e\udd73 R-package build example This pipeline installs a package inside a container, and deploys a documentation website. It assumes the project uses a.o. renv , and pkgdown . While building pages, it makes efficient use of the cache. Dockerfile .gitlab-ci.yml FROM rocker/tidyverse:4.2.1 COPY . /IBURNApipe WORKDIR /IBURNApipe RUN apt-get -y update -qq \\ && apt-get install -y --no-install-recommends \\ pandoc libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev \\ libtiff5-dev libjpeg-dev libcurl4-openssl-dev libfontconfig1-dev \\ libxml2-dev libssl-dev default-jre default-jdk RUN R -e \"if (!requireNamespace('renv', quietly = TRUE)) install.packages('renv')\" RUN R -e \"renv::restore()\" RUN R -e \"if (!requireNamespace('devtools', quietly = TRUE)) install.packages('devtools')\" RUN R -e \"devtools::install()\" RUN Rscript additional_packages_container.R image : rocker/tidyverse:latest variables : DOCKER_DRIVER : overlay2 PKGNAME : IBURNApipe CHECK_DIR : ${CI_PROJECT_DIR}/ci/logs BUILD_LOGS_DIR : ${CI_PROJECT_DIR}/ci/logs/${PKGNAME}.Rcheck RENV_PATHS_CACHE : ${CI_PROJECT_DIR}/cache RENV_PATHS_LIBRARY : ${CI_PROJECT_DIR}/renv/library DOCKER_TLS_CERTDIR : \"\" cache : key : ${CI_JOB_NAME} paths : - ${RENV_PATHS_CACHE} - ${RENV_PATHS_LIBRARY} stages : - build - deploy build : image : docker:20.10.16 stage : build services : - docker:20.10.16-dind variables : IMAGE_TAG : $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG script : - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY - docker build -t $IMAGE_TAG . - docker push $IMAGE_TAG only : - tags pages : stage : deploy script : - apt-get update - apt-get -y install pandoc libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev libcurl4-openssl-dev libfontconfig1-dev libxml2-dev libssl-dev - Rscript -e \"if (!requireNamespace('renv', quietly = TRUE)) install.packages('renv')\" - Rscript -e \"renv::restore()\" - R -e \"pkgdown::build_site()\" artifacts : paths : - public expire_in : 1 day rules : - if : $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH","title":"CI & automatic image builds"},{"location":"tutorials/gitlab/gitlab_ci/#build-and-push-image-example","text":"This example of .gitlab-ci.yml does the following: Pull and use docker-in-docker (dind) container Generate an image tag based on the CI container registry name and a repository tag (i.e. release). CI container registry By default, the CI container registry is the registry associated to the repo; find it at Packages & Registries > Container Registry ) Login to the CI container registry Build the image using the repository as a context (i.e. using a Dockerfile that is in the base directory of the repo) Push the built image to the container registry Run the CI pipeline only when a tag (i.e. release) is pushed. By default it will with each push. Predefined variables Many of the variables used in this yml are predefined. Have a look at the reference here: https://docs.gitlab.com/ee/ci/variables/predefined_variables.html . variables : DOCKER_DRIVER : overlay2 DOCKER_TLS_CERTDIR : \"\" stages : - build build : image : docker:20.10.16 stage : build services : - docker:20.10.16-dind variables : IMAGE_TAG : $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG script : - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY - docker build -t $IMAGE_TAG . - docker push $IMAGE_TAG only : - tags If everything goes well, the image will be pushed to the container registry. In order to pull the image: Docker apptainer docker login gitlab.bioinformatics.unibe.ch:5050 docker pull \\ gitlab.bioinformatics.unibe.ch:5050/ [ namespace ] / [ repo name ] / [ image name ] : [ tag name ] # use login only the first time apptainer remote login --username foo docker://gitlab.bioinformatics.unibe.ch:5050 apptainer pull \\ docker://gitlab.bioinformatics.unibe.ch:5050/ [ namespace ] / [ repo name ] / [ image name ] : [ tag name ] \ud83e\udd73","title":"Build and push image example"},{"location":"tutorials/gitlab/gitlab_ci/#r-package-build-example","text":"This pipeline installs a package inside a container, and deploys a documentation website. It assumes the project uses a.o. renv , and pkgdown . While building pages, it makes efficient use of the cache. Dockerfile .gitlab-ci.yml FROM rocker/tidyverse:4.2.1 COPY . /IBURNApipe WORKDIR /IBURNApipe RUN apt-get -y update -qq \\ && apt-get install -y --no-install-recommends \\ pandoc libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev \\ libtiff5-dev libjpeg-dev libcurl4-openssl-dev libfontconfig1-dev \\ libxml2-dev libssl-dev default-jre default-jdk RUN R -e \"if (!requireNamespace('renv', quietly = TRUE)) install.packages('renv')\" RUN R -e \"renv::restore()\" RUN R -e \"if (!requireNamespace('devtools', quietly = TRUE)) install.packages('devtools')\" RUN R -e \"devtools::install()\" RUN Rscript additional_packages_container.R image : rocker/tidyverse:latest variables : DOCKER_DRIVER : overlay2 PKGNAME : IBURNApipe CHECK_DIR : ${CI_PROJECT_DIR}/ci/logs BUILD_LOGS_DIR : ${CI_PROJECT_DIR}/ci/logs/${PKGNAME}.Rcheck RENV_PATHS_CACHE : ${CI_PROJECT_DIR}/cache RENV_PATHS_LIBRARY : ${CI_PROJECT_DIR}/renv/library DOCKER_TLS_CERTDIR : \"\" cache : key : ${CI_JOB_NAME} paths : - ${RENV_PATHS_CACHE} - ${RENV_PATHS_LIBRARY} stages : - build - deploy build : image : docker:20.10.16 stage : build services : - docker:20.10.16-dind variables : IMAGE_TAG : $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG script : - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY - docker build -t $IMAGE_TAG . - docker push $IMAGE_TAG only : - tags pages : stage : deploy script : - apt-get update - apt-get -y install pandoc libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev libcurl4-openssl-dev libfontconfig1-dev libxml2-dev libssl-dev - Rscript -e \"if (!requireNamespace('renv', quietly = TRUE)) install.packages('renv')\" - Rscript -e \"renv::restore()\" - R -e \"pkgdown::build_site()\" artifacts : paths : - public expire_in : 1 day rules : - if : $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH","title":"R-package build example"},{"location":"tutorials/short/conda_env_container/","text":"Let\u2019s say you are creating a conda environment named hisat2 : conda create -y -n hisat2-env hisat2 = 2 .2.1 If you require a single program If you want to use only a single program inside a container, it\u2019s best to use the readily available containers at bioconda.github.io/ . E.g. for hisat2 the singularity command would be: singularity pull docker://quay.io/biocontainers/hisat2:2.2.1--h87f3376_4 If using snakemake If you are using conda with snakemake, it\u2019s most efficient to convert your environment ymls into a Dockerfile with snakemake --containerize . For more information, have a look at the snakemake docs . Now you can create an environment.yml file that describes the environment and can be used to create the docker container: conda activate hisat2-env conda env export > environment.yml This yaml file will look like this: name: hisat2-env channels: - bioconda - conda-forge - defaults dependencies: - _libgcc_mutex=0.1=conda_forge - _openmp_mutex=4.5=2_gnu - bzip2=1.0.8=h7f98852_4 - ca-certificates=2022.12.7=ha878542_0 - hisat2=2.2.1=h87f3376_4 - ld_impl_linux-64=2.40=h41732ed_0 - libffi=3.4.2=h7f98852_5 - libgcc-ng=12.2.0=h65d4601_19 - libgomp=12.2.0=h65d4601_19 - libnsl=2.0.0=h7f98852_0 - libsqlite=3.40.0=h753d276_0 - libstdcxx-ng=12.2.0=h46fd767_19 - libuuid=2.32.1=h7f98852_1000 - libzlib=1.2.13=h166bdaf_4 - ncurses=6.3=h27087fc_1 - openssl=3.0.8=h0b41bf4_0 - perl=5.32.1=2_h7f98852_perl5 - pip=23.0=pyhd8ed1ab_0 - python=3.11.0=he550d4f_1_cpython - readline=8.1.2=h0f457ee_0 - setuptools=67.1.0=pyhd8ed1ab_0 - tk=8.6.12=h27826a3_0 - tzdata=2022g=h191b570_0 - wheel=0.38.4=pyhd8ed1ab_0 - xz=5.2.6=h166bdaf_0 prefix: /home/gvangeest/miniconda3/envs/hisat2 Now, we can prepare the Dockerfile : FROM continuumio/miniconda3 COPY environment.yml /opt RUN conda env create -f /opt/environment.yml ENTRYPOINT [ \"conda\" , \"run\" , \"-n\" , \"hisat2-env\" ] Note The name of the environment at ENTRYPOINT ( hisat2-env ) should be the same as specified in the environment.yml after name: . Building images without entrypoint If you do not want to set an entrypoint (e.g. to use singularity exec ), you can\u2019t use an environment.yml , as this will always create a separate environment that is not activated at container start up. An alternative can be to not use environment.yml , but add the conda install command directly to the Dockerfile , and install it in base environment: FROM continuumio/miniconda3 RUN conda install -y -c bioconda hisat2 = 2 .2.1 In order to create the container image, both the Dockerfile and environment.yml need to be in the same directory. After that, cd into that directory and run: docker build -t [ docker hub namespace ] /conda-hisat2 . Note Replace [docker hub namespace] with you username on docker hub. E.g. for me this would be: docker build -t geertvangeest/conda-hisat2 . If you don\u2019t have an account yet on dockerhub, and you want to use dockerhub as a container image repository, make one first. Now we can check whether the container does what it should: docker run --rm conda-hisat2 hisat2 --help Which should return the hisat2 helper. In order to share the container, upload it to a container repository (e.g. docker hub): docker push [ docker hub namespace ] /conda-hisat2 To use it with singularity you can run: singularity pull docker:// [ docker hub namespace ] /conda-hisat2 And check wheter it does what is should with: singularity run conda-hisat2_latest.sif hisat2 --help Warning This doens\u2019t work with singularity exec , as this command replaces the the entrypoint. If you want/need to run singularity exec , use: singularity exec conda-hisat2_latest.sif conda run -n hisat2-env hisat2 --help Use gitlab CI! Of course, you can also set up automatic builds with gitlab, which enables you to let gitlab re-create your container at every push/release to your repo. More information at CI & automatic image builds","title":"Conda env in a container"},{"location":"tutorials/short/download_s3/","text":"To download files with s3 (e.g. for igenomes), use the docker container with aws installed. Download the latest image like so: singularity pull docker://amazon/aws-cli Now specify the bucket and the target dir in some variables, and run the container: S3_DIR = \"s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/\" TARGET_DIR = \"./reference/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/\" singularity run \\ --bind \"/data\" \\ aws-cli_latest.sif \\ s3 \\ --no-sign-request \\ --region eu-west-1 \\ sync \\ \" $S3_DIR \" \\ \" $TARGET_DIR \"","title":"Download from Amazon S3"},{"location":"tutorials/short/mount_cluster/","text":"Linux Install sshfs : sudo apt install sshfs or sudo dnf install sshfs Create a mount directory: mkdir ~/cluster Connect: sshfs username@binfservms01.unibe.ch:/ ~/cluster Unmount: fusermount -zu ~/cluster Note Add these options if you experience performance problems: -oauto_cache,reconnect Mac Install sshfs and FUSE Create a mount directory mkdir myMountDir Connect: sshfs -o defer_permissions username@binfservms01.unibe.ch:/ myMountDir/ Note -o defer_permissions might be needed to get access to folders outside of your home dir, see this post: https://github.com/fabiokr/vagrant-sshfs/issues/33 Windows Download the latest version of winfsp and install it. (Tested: WinFsp 2018.2 B4 ) Download the latest version of sshfs-win and install it. (Tested: SSHFS-Win 3.2 BETA ) In PowerShell, type net use X: \\\\sshfs\\username@binfservms01.unibe.ch\\..\\.. this will mount the root of binfservms01 as a network drive under Windows. Deleting files on Windows Be careful when deleting files with sshfs ! Let\u2019s say you have these files and folders: . |-- test | `-- file.txt -> ../test2/file.txt `-- test2 `-- file.txt After rm -rf test , you get the expected result: . `-- test2 `-- file.txt However, if you delete the folder test via sshfs on windows 10, you get: . |-- test | `-- file.txt -> ../test2/file.txt `-- test2 The folder test and its link were not deleted, sshfs deleted the target file only ! (WTF?!)","title":"Mount cluster as network drive"},{"location":"tutorials/short/mount_cluster/#linux","text":"Install sshfs : sudo apt install sshfs or sudo dnf install sshfs Create a mount directory: mkdir ~/cluster Connect: sshfs username@binfservms01.unibe.ch:/ ~/cluster Unmount: fusermount -zu ~/cluster Note Add these options if you experience performance problems: -oauto_cache,reconnect","title":"Linux"},{"location":"tutorials/short/mount_cluster/#mac","text":"Install sshfs and FUSE Create a mount directory mkdir myMountDir Connect: sshfs -o defer_permissions username@binfservms01.unibe.ch:/ myMountDir/ Note -o defer_permissions might be needed to get access to folders outside of your home dir, see this post: https://github.com/fabiokr/vagrant-sshfs/issues/33","title":"Mac"},{"location":"tutorials/short/mount_cluster/#windows","text":"Download the latest version of winfsp and install it. (Tested: WinFsp 2018.2 B4 ) Download the latest version of sshfs-win and install it. (Tested: SSHFS-Win 3.2 BETA ) In PowerShell, type net use X: \\\\sshfs\\username@binfservms01.unibe.ch\\..\\.. this will mount the root of binfservms01 as a network drive under Windows. Deleting files on Windows Be careful when deleting files with sshfs ! Let\u2019s say you have these files and folders: . |-- test | `-- file.txt -> ../test2/file.txt `-- test2 `-- file.txt After rm -rf test , you get the expected result: . `-- test2 `-- file.txt However, if you delete the folder test via sshfs on windows 10, you get: . |-- test | `-- file.txt -> ../test2/file.txt `-- test2 The folder test and its link were not deleted, sshfs deleted the target file only ! (WTF?!)","title":"Windows"},{"location":"tutorials/short/port_forwarding_apptainer/","text":"With port forwarding you can access a port on a remote machine as if it were on your local machine. This is useful for accessing web interfaces running on a remote machine. Note You can find a wrapper script for these steps here Start an interactive session with SLURM srun --pty --mem = 4G --time = 4 :0:0 /bin/bash As an example, we will use an rstudio container. First, we need to pull the container: apptainer pull docker://rocker/rstudio:4.2 Now, we run the container with apptainer based on the rocker documentation : mkdir -p run var-lib-rstudio-server printf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' > database.conf PASSWORD = mypass apptainer exec \\ --bind run:/run,var-lib-rstudio-server:/var/lib/rstudio-server,database.conf:/etc/rstudio/database.conf \\ rstudio_4.2.sif \\ /usr/lib/rstudio-server/bin/rserver --www-address = 0 .0.0.0 --auth-none = 0 \\ --auth-pam-helper-path = pam-helper --server-user = $( whoami ) This will create an rstudio instance on port 8787 of the compute node. We can\u2019t approach this port directly, but we can create an ssh tunnel to it. So, let\u2019s create an ssh tunnel to the compute node on your local computer: ssh -N -f -L 8787 :binfservasXX:8787 gvangeest@binfservms01.unibe.ch Note Replace binfservasXX with the name of the compute node you are on. You can find this with hostname . Of course, also replace gvangeest with your username. Now, you can open a browser on your local computer and go to localhost:8787 . You should see the rstudio login screen. Login with your cluster username and the password specified at PASSWORD in the apptainer exec command above. I did not test the jupyter stack yet, but it definitely works with jupyter installed with conda, e.g. like this: srun --pty --mem = 4G --time = 4 :0:0 /bin/bash conda create -n jupyterlab jupyterlab conda activate jupyterlab jupyter lab --no-browser --ip = 0 .0.0.0 --port = 8879 Locally, you can now create an ssh tunnel to the compute node on your local computer: ssh -N -f -L 8879 :binfservasXX:8879 gvangeest@binfservms01.unibe.ch You should be able to access jupyterlab on localhost:8879 . To login enter the token that is printed in the terminal.","title":"Port forwarding for apptainer"}]}